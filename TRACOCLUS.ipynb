{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from timeit import default_timer as timer\n",
    "import sys, os, shutil, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Version of the packages in this work.'''\n",
    "# # print(\"Pandas version: \",pd.__version__)\n",
    "# print(\"Pandas version: 0.25.1\")\n",
    "# # print(\"Numpy version: \",np.__version__)\n",
    "# print(\"Numpy version: 1.16.4\")\n",
    "# # print(\"Sys python version: \",sys.version)\n",
    "# print(\"Sys python version: 3.7.1 | package by conda-forge [MSC v.1900 64 bit (AMD64)]\")\n",
    "# print(\"IPython: 7.8.0\")\n",
    "# print(\"IPython genutils: 0.2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--->Verbose mode ON.<---\n",
      "Executing TRACOCLUS method\n",
      "Toy example\n",
      "######################################\n",
      "Number of trajectories: 22\n",
      "Number of unique check-ins: 11\n",
      "########################################\n",
      "Map_attribute_to_id:{'hotel': '0', 'trabalho': '1', 'academia': '2', 'restaurante': '3', 'casa': '4', 'farmacia': '5', 'parque': '6', 'estadio': '7', 'padaria': '8', 'festa': '9', 'aeroporto': '10'}\n",
      "\n",
      "Map_id_to_attribute:{'0': 'hotel', '1': 'trabalho', '2': 'academia', '3': 'restaurante', '4': 'casa', '5': 'farmacia', '6': 'parque', '7': 'estadio', '8': 'padaria', '9': 'festa', '10': 'aeroporto'}\n",
      "\n",
      "Frequence_per_poi:{'0': 13, '1': 30, '2': 13, '3': 10, '4': 23, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1, '10': 4}\n",
      "\n",
      "Trajectories: {'0': ['0', '1', '2', '3', '1', '0'], '1': ['4', '1', '2', '3', '4'], '2': ['5', '1', '2', '4'], '3': ['6', '2', '3', '4'], '4': ['5', '2', '3', '7'], '5': ['4', '2', '1', '4'], '6': ['4', '8', '4', '1', '2', '1', '4'], '7': ['4', '1', '2', '1', '4'], '8': ['0', '1', '2', '1', '0'], '9': ['4', '1', '4'], '10': ['4', '1', '9'], '11': ['4', '1', '3', '1', '4'], '12': ['1', '3', '1', '4'], '13': ['0', '1', '0'], '14': ['0', '1', '2', '3', '0'], '15': ['0', '2', '1', '0'], '16': ['10', '1', '0', '10'], '17': ['6', '1', '0'], '18': ['0', '1', '3', '1', '10'], '19': ['4', '1', '3', '1', '10'], '20': ['4', '1', '2', '4', '1', '4'], '21': ['4', '1', '2', '3', '1', '4']}\n",
      "\n",
      "POI occurring at trajectories: {'0': {'8', '15', '14', '0', '18', '16', '17', '13'}, '1': {'7', '5', '18', '19', '12', '13', '9', '6', '1', '15', '14', '2', '17', '8', '0', '20', '21', '11', '16', '10'}, '2': {'7', '21', '6', '5', '1', '15', '14', '0', '4', '2', '3', '8', '20'}, '3': {'21', '1', '14', '0', '18', '4', '11', '19', '12', '3'}, '4': {'9', '7', '21', '6', '5', '1', '10', '19', '2', '11', '12', '3', '20'}, '5': {'4', '2'}, '6': {'3', '17'}, '7': {'4'}, '8': {'6'}, '9': {'10'}, '10': {'18', '19', '16'}}\n",
      "Get data is DONE!\n",
      "\n",
      "Searching co-cluster:  1\n",
      "\n",
      "S:  {'0': 13, '1': 30, '2': 13, '3': 10, '4': 23, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1, '10': 4}\n",
      "Sorted att:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "<class 'collections.deque'>\n",
      "s*:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "['1', 30]\n",
      "<class 'list'>\n",
      "\n",
      "Head sequence:  1-1\n",
      "Tested head sequence \"1-1\" does NOT exist!\n",
      "Tail sequence:  1-1\n",
      "Tested tail sequence \"1-1\" does NOT exist!\n",
      "Current co-cluster cost:  9223372036854775807\n",
      "Queue s* BEFORE to upadate:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  4-1\n",
      "Trajectories with this sequence: {'9', '7', '21', '6', '1', '19', '11', '10', '20'}\n",
      "Num. objs:  9, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Head cost: -7\n",
      "Tail sequence:  1-4\n",
      "Trajectories with this sequence: {'9', '7', '21', '6', '5', '11', '12', '20'}\n",
      "Num. objs:  8, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -6\n",
      "Current co-cluster cost:  9223372036854775807\n",
      "Queue s* BEFORE to upadate:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Co-cluster improved with HEAD sequence.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  0-4-1\n",
      "Tested head sequence \"0-4-1\" does NOT exist!\n",
      "Tail sequence:  4-1-0\n",
      "Tested tail sequence \"4-1-0\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  2-4-1\n",
      "Trajectories with this sequence: {'20'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Head cost: 1\n",
      "Tail sequence:  4-1-2\n",
      "Trajectories with this sequence: {'7', '21', '6', '1', '20'}\n",
      "Num. objs:  5, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -7\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Co-cluster improved with TAIL sequence.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  3-4-1-2\n",
      "Tested head sequence \"3-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-3\n",
      "Trajectories with this sequence: {'21', '1'}\n",
      "Num. objs:  2, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -2\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  10-4-1-2\n",
      "Tested head sequence \"10-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-10\n",
      "Tested tail sequence \"4-1-2-10\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  5-4-1-2\n",
      "Tested head sequence \"5-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-5\n",
      "Tested tail sequence \"4-1-2-5\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  6-4-1-2\n",
      "Tested head sequence \"6-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-6\n",
      "Tested tail sequence \"4-1-2-6\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  7-4-1-2\n",
      "Tested head sequence \"7-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-7\n",
      "Tested tail sequence \"4-1-2-7\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  8-4-1-2\n",
      "Trajectories with this sequence: {'6'}\n",
      "Num. objs:  1, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "Head cost: 1\n",
      "Tail sequence:  4-1-2-8\n",
      "Tested tail sequence \"4-1-2-8\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Head sequence:  9-4-1-2\n",
      "Tested head sequence \"9-4-1-2\" does NOT exist!\n",
      "Tail sequence:  4-1-2-9\n",
      "Tested tail sequence \"4-1-2-9\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Co-cluster identified. Go to the next searching.\n",
      "Main list S BEFORE to update:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Main list S AFTER to update:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Cluster \"1\" finished at time \"0.012228200000009792\".\n",
      "Total clustering time:  0.012252899999992906\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#toy example dataset\n",
    "# input_test2.dat and input_test2.dat\n",
    "\n",
    "#synthetic datasets\n",
    "# synthetic-1_fimi.dat, synthetic-2_fimi.dat and synthetic-3_fimi.dat\n",
    "\n",
    "experiment = 'Toy' # Syn: run the synthetic data analysis; Real: run the real data analysis; \n",
    "                   # Toy: run the toy example.\n",
    "toy_number = \"2\" # sufix of a given toy dataset [1,1-1,2]\n",
    "\n",
    "find_overlap = False # True: find overlapped co-clusters; False: Find no overlapped co-clusters\n",
    "VERBOSE = True # True: print results as a verbose mode; False: print the main results\n",
    "num_of_sim = 1 # number of simulations to perform\n",
    "k = -1 #max number of co-cluster that could be found. -1: default [driven by cost function]\n",
    "e_obj = 0.4 # maximum error tolerance for object. -1: default [accept the maximum error]\n",
    "e_att = .8 # maximum error tolerance for attribute. -1: default [accept the maximum error]\n",
    "\n",
    "if VERBOSE: print('--->Verbose mode ON.<---')\n",
    "\n",
    "print(\"Executing TRACOCLUS method\")\n",
    "if experiment == 'Syn':\n",
    "    path = \"./data/synthetic/fimi/\"\n",
    "    syn_datasets = [path+\"synthetic-1_fimi.dat\",path+\"synthetic-2_fimi.dat\",path+\"synthetic-3_fimi.dat\"]\n",
    "    path_method = \"OutputAnalysis\\ococlus\"\n",
    "    check_path(path_method)\n",
    "\n",
    "    for ds in range(len(syn_datasets)):\n",
    "        ds_name = \"Syn-\"+str(ds+1)\n",
    "        print(\"\\nDataset: \"+ds_name)\n",
    "        res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "        for run in range(num_of_sim):\n",
    "            print(\"Run-\"+str(run+1))\n",
    "\n",
    "            df_fimi = pd.read_csv(syn_datasets[ds], header=None, names=[\"transation\"])\n",
    "            D,co_clusters = OCoClus(df_fimi,k,e_obj,e_att) # Calling OCoClus main method\n",
    "\n",
    "            print(\"\")\n",
    "            Rec_error(D,co_clusters)\n",
    "#             print(co_clusters)\n",
    "            \n",
    "            omega_format = build_clustering_output_omega(co_clusters)\n",
    "            OCoClus_clustering_xm = xmeasures_format(omega_format)# save to XMEASURES format C++ version\n",
    "            df_gt = pd.DataFrame(OCoClus_clustering_xm)\n",
    "            path = path_method+\"/\"+ds_name\n",
    "            df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_ococlus_\"+ds_name+\"_co.cnl\", \n",
    "                         header= False,index=False, encoding='utf8')\n",
    "            del omega_format, df_gt, OCoClus_clustering_xm\n",
    "            gc.collect()\n",
    "elif experiment == 'Real':\n",
    "    k = 10\n",
    "    print('Real data clustering.')\n",
    "    path = \"./data/real_application/\"\n",
    "    real_datasets = [path+\"cal500_fimi.dat\",path+\"covid19_fimi.dat\"]\n",
    "    path_method = \"OutputAnalysis\\ococlus\"\n",
    "    check_path(path_method)\n",
    "    \n",
    "    for ds in range(len(real_datasets)):\n",
    "        ds_name = real_datasets[ds].replace(\"/\",\"_\").split(\"_\")[4].capitalize()\n",
    "        print(\"\\nDataset: \"+ds_name)\n",
    "        res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "        \n",
    "        df_fimi = pd.read_csv(real_datasets[ds], header=None, names=[\"transation\"])\n",
    "        D,co_clusters = OCoClus(df_fimi,k,e_obj,e_att) # Calling OCoClus main method\n",
    "        writeFileOutput(co_clusters,ds_name,method='OCoClus',fileName='OCoClusResult_'+ds_name)\n",
    "        \n",
    "    print('DONE!')\n",
    "elif experiment == 'Toy':\n",
    "    print('Toy example')\n",
    "#     input_data_pd = pd.read_csv(\"./data/toy_example/toy\"+toy_number+\"_traj.dat\", header=None, names=[\"transation\"])\n",
    "    input_data_pd = pd.read_csv(\"./data/toy_example/toy\"+toy_number+\"_traj.dat\", header=None, names=[\"transation\"])\n",
    "#     print(input_data_pd)\n",
    "    D,co_clusters = TRACOCLUS(input_data_pd,k,e_obj,e_att)\n",
    "    print(co_clusters)\n",
    "    #Compute the measures\n",
    "#     print(\"\")\n",
    "#     Rec_error(D,co_clusters)\n",
    "else:\n",
    "    print('ERROR! Choose a valid option for the experiment analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Omega index, overlapped F1, ONMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "### omega index and f-score\n",
    "### ./xmeasures -o -fp -ku -O gt.txt cls2.txt\n",
    "# pwd\n",
    "# ls\n",
    "\n",
    "if [ -d \"xmeasures/OutputAnalysis/ococlus\" ] \n",
    "then\n",
    "#     echo \"Directory exists.\"\n",
    "    rm -R xmeasures/OutputAnalysis/ococlus\n",
    "else\n",
    "    echo \"Error: Directory does not exists.\"\n",
    "fi\n",
    "\n",
    "cp -R OutputAnalysis xmeasures\n",
    "cd xmeasures/\n",
    "\n",
    "#Method: [Ococlus]\n",
    "#ground-truth: [gt_xm_s1_co.cnl,gt_xm_s2_co.cnl,gt_xm_s3_co.cnl]\n",
    "for i in 1 2 3 # index of synthetic datasets\n",
    "# for i in 1\n",
    "do\n",
    "#     for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    do\n",
    "    #     echo \" $(grep -c '' ${file})\"\n",
    "        res=$(grep -c '' ${file})\n",
    "    #     echo \"${file} lines equal to: ${res}\"\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "    #         echo \"Empty file\"\n",
    "            ./xmeasures -o -fp -ku -O ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "#### ONMI\n",
    "#### ./onmi file1 file2\n",
    "\n",
    "cd xmeasures\n",
    "\n",
    "#Method: [Ococlus]\n",
    "#ground-truth: [gt_xm_s1_trad.cnl,gt_xm_s2_trad.cnl,gt_xm_s3_trad.cnl]\n",
    "for i in 1 2 3 # index of synthetic datasets\n",
    "# for i in 1\n",
    "do\n",
    "    for file in OutputAnalysis/ococlus/Syn-${i}/*\n",
    "    do\n",
    "    #     echo \" $(grep -c '' ${file})\"\n",
    "        res=$(grep -c '' ${file})\n",
    "    #     echo \"${file} lines equal to: ${res}\"\n",
    "\n",
    "        if [ ${res} != 0 ]\n",
    "        then \n",
    "            echo \" \"\n",
    "            echo \"File ${file} is not empty. It has ${res} lines.\"\n",
    "    #         echo \"Empty file\"\n",
    "            ./onmi ./gts/gt_xm_s${i}_trad.cnl ${file} &\n",
    "            echo \" \"\n",
    "        else\n",
    "              echo \"File ${file}\"\n",
    "              echo \"Empty file. SKIPPED!\"\n",
    "        fi\n",
    "#     # wait until all child processes are done\n",
    "#     wait\n",
    "    done\n",
    "    echo \" \"\n",
    "    # wait until all child processes are done\n",
    "    wait\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TraCoClus algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main algorithm of TraCoClus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '3', '1', '4', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "er = '0-3-1-4-0-1'\n",
    "er2 = er.split('-')\n",
    "print(er2)\n",
    "# print(er2.nunique())\n",
    "def change_vect_cont(queue):\n",
    "    queue[0] = 'Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q', 2, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "my_vect = [1,2,8,4]\n",
    "change_vect_cont(my_vect)\n",
    "print(my_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24d1fc20630>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_tested_atts = np.log2(10)\n",
    "vec_log_num = [1,10,20,30,40,50,60,70,80,90,100]#,250,500]#,750,1000]#,1500,2000,3000,5000,7500,10000]\n",
    "base = [2,10]\n",
    "df_log_base_diff = pd.DataFrame(columns = ['Input x','Log Result','Log Base'])\n",
    "popupalte_df_dict = {'Input x':0,'Log Result':0,'Log Base':''}\n",
    "\n",
    "for base_i in base:\n",
    "    popupalte_df_dict['Log Base'] = 'Log_'+str(base_i)\n",
    "    for num in vec_log_num:\n",
    "        popupalte_df_dict['Input x'] = num\n",
    "#         if base_i == 2:\n",
    "#         popupalte_df_dict['Log Result'] = np.log2(num)    \n",
    "        popupalte_df_dict['Log Result'] = np.log(num)/np.log(base_i)\n",
    "#         else:\n",
    "#             popupalte_df_dict['Log Result'] = np.log10(num)\n",
    "        df_log_base_diff = df_log_base_diff.append(popupalte_df_dict, ignore_index=True)\n",
    "df_log_base_diff.head()\n",
    "sns.lineplot(data=df_log_base_diff, x=\"Input x\", y=\"Log Result\", hue=\"Log Base\", style=\"Log Base\",\n",
    "             markers=True, dashes=False)\n",
    "# plt.show()\n",
    "# print(max_tested_atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################\n",
      "Number of trajectories: 22\n",
      "Number of unique check-ins: 11\n",
      "########################################\n",
      "Map_attribute_to_id:{'hotel': '0', 'trabalho': '1', 'academia': '2', 'restaurante': '3', 'casa': '4', 'farmacia': '5', 'parque': '6', 'estadio': '7', 'padaria': '8', 'festa': '9', 'aeroporto': '10'}\n",
      "\n",
      "Map_id_to_attribute:{'0': 'hotel', '1': 'trabalho', '2': 'academia', '3': 'restaurante', '4': 'casa', '5': 'farmacia', '6': 'parque', '7': 'estadio', '8': 'padaria', '9': 'festa', '10': 'aeroporto'}\n",
      "\n",
      "Frequence_per_poi:{'0': 13, '1': 30, '2': 13, '3': 10, '4': 23, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1, '10': 4}\n",
      "\n",
      "Trajectories: {'0': ['0', '1', '2', '3', '1', '0'], '1': ['4', '1', '2', '3', '4'], '2': ['5', '1', '2', '4'], '3': ['6', '2', '3', '4'], '4': ['5', '2', '3', '7'], '5': ['4', '2', '1', '4'], '6': ['4', '8', '4', '1', '2', '1', '4'], '7': ['4', '1', '2', '1', '4'], '8': ['0', '1', '2', '1', '0'], '9': ['4', '1', '4'], '10': ['4', '1', '9'], '11': ['4', '1', '3', '1', '4'], '12': ['1', '3', '1', '4'], '13': ['0', '1', '0'], '14': ['0', '1', '2', '3', '0'], '15': ['0', '2', '1', '0'], '16': ['10', '1', '0', '10'], '17': ['6', '1', '0'], '18': ['0', '1', '3', '1', '10'], '19': ['4', '1', '3', '1', '10'], '20': ['4', '1', '2', '4', '1', '4'], '21': ['4', '1', '2', '3', '1', '4']}\n",
      "\n",
      "POI occurring at trajectories: {'0': {'8', '15', '14', '0', '18', '16', '17', '13'}, '1': {'7', '5', '18', '19', '12', '13', '9', '6', '1', '15', '14', '2', '17', '8', '0', '20', '21', '11', '16', '10'}, '2': {'7', '21', '6', '5', '1', '15', '14', '0', '4', '2', '3', '8', '20'}, '3': {'21', '1', '14', '0', '18', '4', '11', '19', '12', '3'}, '4': {'9', '7', '21', '6', '5', '1', '10', '19', '2', '11', '12', '3', '20'}, '5': {'4', '2'}, '6': {'3', '17'}, '7': {'4'}, '8': {'6'}, '9': {'10'}, '10': {'18', '19', '16'}}\n",
      "Get data is DONE!\n",
      "\n",
      "Searching co-cluster:  1\n",
      "\n",
      "S:  {'0': 13, '1': 30, '2': 13, '3': 10, '4': 23, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1, '10': 4}\n",
      "Sorted att:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "<class 'collections.deque'>\n",
      "s*:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "['1', 30]\n",
      "<class 'list'>\n",
      "\n",
      "-> Head sequence:  1-1\n",
      "Tested head sequence \"1-1\" does NOT exist!\n",
      "-> Tail sequence:  1-1\n",
      "Tested tail sequence \"1-1\" does NOT exist!\n",
      "Current co-cluster cost:  9223372036854775807\n",
      "Queue s* BEFORE to upadate:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  4-1\n",
      "Trajectories with this sequence: {'9', '7', '21', '6', '1', '19', '11', '10', '20'}\n",
      "Num. objs:  9, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Head cost: -7 and overlap_coef: 0.\n",
      "-> Tail sequence:  1-4\n",
      "Trajectories with this sequence: {'9', '7', '21', '6', '5', '11', '12', '20'}\n",
      "Num. objs:  8, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -6 and overlap_coef: 0.\n",
      "Current co-cluster cost:  9223372036854775807\n",
      "Queue s* BEFORE to upadate:  deque([['1', 30], ['4', 23], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Co-cluster improved with HEAD sequence.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  0-4-1\n",
      "Tested head sequence \"0-4-1\" does NOT exist!\n",
      "-> Tail sequence:  4-1-0\n",
      "Tested tail sequence \"4-1-0\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  2-4-1\n",
      "Trajectories with this sequence: {'20'}\n",
      "Num. objs:  1, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Head cost: 1 and overlap_coef: 0.\n",
      "-> Tail sequence:  4-1-2\n",
      "Trajectories with this sequence: {'7', '21', '6', '1', '20'}\n",
      "Num. objs:  5, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -7 and overlap_coef: 0.\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 13], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Co-cluster improved with TAIL sequence.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  3-4-1-2\n",
      "Tested head sequence \"3-4-1-2\" does NOT exist!\n",
      "-> Tail sequence:  4-1-2-3\n",
      "Trajectories with this sequence: {'21', '1'}\n",
      "Num. objs:  2, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -2 and overlap_coef: 0.\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  10-4-1-2\n",
      "Tested head sequence \"10-4-1-2\" does NOT exist!\n",
      "-> Tail sequence:  4-1-2-10\n",
      "Tested tail sequence \"4-1-2-10\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  5-4-1-2\n",
      "Tested head sequence \"5-4-1-2\" does NOT exist!\n",
      "-> Tail sequence:  4-1-2-5\n",
      "Tested tail sequence \"4-1-2-5\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  6-4-1-2\n",
      "Tested head sequence \"6-4-1-2\" does NOT exist!\n",
      "-> Tail sequence:  4-1-2-6\n",
      "Tested tail sequence \"4-1-2-6\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  7-4-1-2\n",
      "Tested head sequence \"7-4-1-2\" does NOT exist!\n",
      "-> Tail sequence:  4-1-2-7\n",
      "Tested tail sequence \"4-1-2-7\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  8-4-1-2\n",
      "Trajectories with this sequence: {'6'}\n",
      "Num. objs:  1, Num. att:  4, Num. covered:  0, Num. noise:  0\n",
      "Head cost: 1 and overlap_coef: 0.\n",
      "-> Tail sequence:  4-1-2-8\n",
      "Tested tail sequence \"4-1-2-8\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  9-4-1-2\n",
      "Tested head sequence \"9-4-1-2\" does NOT exist!\n",
      "-> Tail sequence:  4-1-2-9\n",
      "Tested tail sequence \"4-1-2-9\" does NOT exist!\n",
      "Current co-cluster cost:  -7\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Co-cluster identified. Go to the next searching.\n",
      "Main list S BEFORE to update:  {'1': 30, '4': 23, '0': 13, '2': 13, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Main list S AFTER to update:  {'1': 29, '4': 22, '0': 13, '2': 12, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Cluster \"1\" finished at time \"0.005863900001713773\".\n",
      "\n",
      "Searching co-cluster:  2\n",
      "\n",
      "S:  {'1': 29, '4': 22, '0': 13, '2': 12, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Sorted att:  {'1': 29, '4': 22, '0': 13, '2': 12, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "<class 'collections.deque'>\n",
      "s*:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "['1', 29]\n",
      "<class 'list'>\n",
      "\n",
      "-> Head sequence:  1-1\n",
      "Tested head sequence \"1-1\" does NOT exist!\n",
      "-> Tail sequence:  1-1\n",
      "Tested tail sequence \"1-1\" does NOT exist!\n",
      "Current co-cluster cost:  9223372036854775807\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  4-1\n",
      "Trajectories with this sequence: {'9', '7', '21', '6', '1', '19', '11', '10', '20'}\n",
      "Num. objs:  9, Num. att:  2, Num. covered: 10, Num. noise:  0\n",
      "Head cost: 3 and overlap_coef: 0.6666666666666666.\n",
      "-> Tail sequence:  1-4\n",
      "Trajectories with this sequence: {'9', '7', '21', '6', '5', '11', '12', '20'}\n",
      "Num. objs:  8, Num. att:  2, Num. covered:  0, Num. noise:  0\n",
      "Tail cost: -6 and overlap_coef: 0.\n",
      "Current co-cluster cost:  9223372036854775807\n",
      "Queue s* BEFORE to upadate:  deque([['1', 29], ['4', 22], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Co-cluster improved with TAIL sequence.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  0-1-4\n",
      "Tested head sequence \"0-1-4\" does NOT exist!\n",
      "-> Tail sequence:  1-4-0\n",
      "Tested tail sequence \"1-4-0\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  2-1-4\n",
      "Trajectories with this sequence: {'5', '7', '6'}\n",
      "Num. objs:  3, Num. att:  3, Num. covered:  2, Num. noise:  0\n",
      "Head cost: -1 and overlap_coef: 0.2222222222222222.\n",
      "-> Tail sequence:  1-4-2\n",
      "Tested tail sequence \"1-4-2\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  3-1-4\n",
      "Trajectories with this sequence: {'12', '21', '11'}\n",
      "Num. objs:  3, Num. att:  3, Num. covered:  0, Num. noise:  0\n",
      "Head cost: -3 and overlap_coef: 0.\n",
      "-> Tail sequence:  1-4-3\n",
      "Tested tail sequence \"1-4-3\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  10-1-4\n",
      "Tested head sequence \"10-1-4\" does NOT exist!\n",
      "-> Tail sequence:  1-4-10\n",
      "Tested tail sequence \"1-4-10\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  5-1-4\n",
      "Tested head sequence \"5-1-4\" does NOT exist!\n",
      "-> Tail sequence:  1-4-5\n",
      "Tested tail sequence \"1-4-5\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  6-1-4\n",
      "Tested head sequence \"6-1-4\" does NOT exist!\n",
      "-> Tail sequence:  1-4-6\n",
      "Tested tail sequence \"1-4-6\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  7-1-4\n",
      "Tested head sequence \"7-1-4\" does NOT exist!\n",
      "-> Tail sequence:  1-4-7\n",
      "Tested tail sequence \"1-4-7\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  8-1-4\n",
      "Tested head sequence \"8-1-4\" does NOT exist!\n",
      "-> Tail sequence:  1-4-8\n",
      "Tested tail sequence \"1-4-8\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "-> Head sequence:  9-1-4\n",
      "Tested head sequence \"9-1-4\" does NOT exist!\n",
      "-> Tail sequence:  1-4-9\n",
      "Tested tail sequence \"1-4-9\" does NOT exist!\n",
      "Current co-cluster cost:  -6\n",
      "Queue s* BEFORE to upadate:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "Tested sequences do not improved the current co-cluster.\n",
      "Queue s* AFTER to update:  deque([['1', 28], ['4', 21], ['0', 13], ['2', 12], ['3', 10], ['10', 4], ['5', 2], ['6', 2], ['7', 1], ['8', 1], ['9', 1]])\n",
      "\n",
      "Co-cluster identified. Go to the next searching.\n",
      "Main list S BEFORE to update:  {'1': 29, '4': 22, '0': 13, '2': 12, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Main list S AFTER to update:  {'1': 28, '4': 21, '0': 13, '2': 12, '3': 10, '10': 4, '5': 2, '6': 2, '7': 1, '8': 1, '9': 1}\n",
      "Cluster \"2\" finished at time \"0.015793000002304325\".\n",
      "Total clustering time:  0.015805400002136594\n",
      "\n",
      "Final co-clusters:  {'0': {'cc_objs': {'7', '21', '6', '20', '1'}, 'cc_atts': '4-1-2', 'cc_elements': {'111', '122', '2140', '2011', '722', '2122', '711', '2040', '2022', '613', '642', '624', '140', '740', '2111'}}, '1': {'cc_objs': {'9', '7', '21', '6', '5', '11', '12', '20'}, 'cc_atts': '1-4', 'cc_elements': {'543', '615', '512', '1144', '1212', '1243', '911', '942', '2045', '2014', '2145', '744', '713', '1113', '646', '2114'}}}\n"
     ]
    }
   ],
   "source": [
    "D,co_clusters = TRACOCLUS(input_data_pd,k,e_obj,e_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_queue(poi_freq_dict):\n",
    "#     print([{k:v} for k,v in poi_freq_dict.items()])\n",
    "    queue = deque()\n",
    "    [queue.append([k,v]) for k,v in poi_freq_dict.items()]\n",
    "    return queue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequence(trajectory_dataset_dict_list, candidate_trajectories_sequence_set, test_traj_sequence):\n",
    "    '''This method receive 3 parameters: \n",
    "       1) trajectory dataset as a dict->list | x['key']:[...];\n",
    "       2) trajectories indeces that contains the tested check-ins as a set;\n",
    "       3) the given tested sequence of check-ins as a string\n",
    "    '''\n",
    "    new_set_trajectories = set()\n",
    "    position_pois_per_traj_list = {}\n",
    "#     test_traj_sequence = test_traj_sequence.strip()\n",
    "    for traj_id in candidate_trajectories_sequence_set:\n",
    "\n",
    "        try:\n",
    "#             traj_dataset = '-'.join(trajectory_dataset_dict_list[traj_id]).strip()\n",
    "            test_subsequence, positions_at_traj = is_subsequence(trajectory_dataset_dict_list[traj_id],test_traj_sequence.split('-'))\n",
    "            if test_subsequence:\n",
    "#                 print('OK->',end=' ')\n",
    "#                 print(traj_id,positions_at_traj)\n",
    "                new_set_trajectories.add(traj_id)\n",
    "                position_pois_per_traj_list[traj_id] = positions_at_traj\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "#         print('{}-> {}'.format(traj_id,trajectory_dataset_dict_list[traj_id]))\n",
    "#     print('Sequence \"{}\" is present in trajectories: {}'.format(test_traj_sequence,new_set_trajectories))\n",
    "    return new_set_trajectories, position_pois_per_traj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subsequence(sequence, subsequence):\n",
    "    '''This sub method receive two arrays: \n",
    "       first one is the sequence and second one is the tested subsequence.'''\n",
    "    n = len(sequence)\n",
    "    m = len(subsequence)\n",
    "    position_poi_sequence = []\n",
    "    \n",
    "    # Two pointers to traverse the arrays\n",
    "    i = 0; j = 0;\n",
    " \n",
    "    # Traverse both arrays simultaneously\n",
    "    while (i < n and j < m):\n",
    " \n",
    "        # If element matches\n",
    "        # increment both pointers\n",
    "        if (sequence[i] == subsequence[j]):\n",
    "            position_poi_sequence.append(str(i))\n",
    "            i += 1\n",
    "            j += 1\n",
    " \n",
    "            # If array B is completely\n",
    "            # traversed\n",
    "            if (j == m):\n",
    "                return True, position_poi_sequence\n",
    "         \n",
    "        # If not,\n",
    "        # increment i and reset j\n",
    "        else:\n",
    "            position_poi_sequence = []\n",
    "            i = i - j + 1\n",
    "            j = 0\n",
    "         \n",
    "    return False,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ['2', '3']\n",
      "0 ['3', '4']\n",
      "['013', '0104', '312', '3103']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0104', '013', '3103', '312'}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = {'0':['1','4','6','1','10'],'1':['3','6','7'],'2':['7','9','5'],'3':['4','6','1','10']\n",
    "                ,'4':['9','5']}\n",
    "test_candidate = set(['0','1','2','3','4'])\n",
    "test_sequence = '1-10'\n",
    "mySet, myPos = check_sequence(test_dataset,test_candidate,test_sequence)\n",
    "print([rows+test_sequence.split('-')[poi_id]+poi_pos for rows in mySet \n",
    " for poi_id in range(len(test_sequence.split('-'))) for poi_pos in myPos[rows][poi_id]])\n",
    "form_elements(mySet,test_sequence,myPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_elements(trajs_index_set, tested_sequence, poi_positions_trajectories_dict_list):\n",
    "    '''\n",
    "    This method returns a set of elements.\n",
    "    Each element is formed by the traj ID, poi ID in the sequence and its respective position at traj ID.\n",
    "    Ex: set(['013', '0104', '312', '3103'])\n",
    "        '013': 0-> traj ID, 1-> poi ID, and 3-> position of poi ID at traj ID\n",
    "    '''\n",
    "    tested_sequence = tested_sequence.split('-')\n",
    "    return set([trajID+tested_sequence[poi_id]+poi_pos for trajID in trajs_index_set \n",
    "                for poi_id in range(len(tested_sequence)) \n",
    "                for poi_pos in poi_positions_trajectories_dict_list[trajID][poi_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusA = set([1,2,3,4,5])\n",
    "dict_cc = {'0': {'cc_elements':set([5,6,7,8,9])},'1': {'cc_elements':set([4,5,6,7,8])},\n",
    "           '2': {'cc_elements':set([5,4,3,2,1])}}\n",
    "overlap_coefficient(clusA,dict_cc)\n",
    "# print(overlap_coefficient(clusA,dict_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_coefficient(clusterA, discovered_cc):\n",
    "    max_overlap = 0\n",
    "    \n",
    "    for key, value in discovered_cc.items():\n",
    "#         print(key,value)\n",
    "        elements_intersection = len(clusterA.intersection(discovered_cc[key]['cc_elements']))\n",
    "#         print(elements_intersection)\n",
    "        curr_overlap = elements_intersection/np.min([len(clusterA),len(discovered_cc[key]['cc_elements'])])\n",
    "#         print(curr_overlap)\n",
    "        \n",
    "        if curr_overlap > max_overlap:\n",
    "            max_overlap = curr_overlap\n",
    "            \n",
    "    return max_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def TRACOCLUS(input_D, k=-1, e_obj=-1, e_att=-1):\n",
    "    ### variable declaration\n",
    "    if k == -1:\n",
    "        k=sys.maxsize\n",
    "    if e_obj == -1:\n",
    "        e_obj = 1\n",
    "    if e_att == -1:\n",
    "        e_att = 1\n",
    "    \n",
    "    cost_model = sys.float_info.max # initial cost function of the model\n",
    "    num_of_coclusters = 0\n",
    "    D = []\n",
    "    final_coclusters = [] # store the attribute and objects clusters. final_coclusters[[C1_att,C1_obj],[Ck_att,Ck_obj]]\n",
    "    pattern_model = [set(),set()]# Union between the found co-clusters [list of obj,list of att]\n",
    "    cost_per_cocluster = []# stores the cost to build the cocluster\n",
    "    history_cost_model = []\n",
    "    ###\n",
    "    \n",
    "#     D,N,data_dict,data_res_dict,map_id_to_attribute = get_data(input_D)\n",
    "    map_id_to_attribute_dict, S_poi_freq_dict, poi_at_trajs_dict_set, trajs_data_dict_list = get_data(input_D)\n",
    "    \n",
    "    overlap_coef_threshold = 0.5\n",
    "    final_coclusters = {}\n",
    "    final_clustered_elements = set()\n",
    "    \n",
    "    start = timer()\n",
    "    for iter_k in range(2):\n",
    "        print('\\nSearching co-cluster: ',iter_k+1)\n",
    "        print('')\n",
    "        print('S: ',S_poi_freq_dict)\n",
    "        S_poi_freq_dict = sort_attributes(S_poi_freq_dict)\n",
    "        s_poi_freq_queue_list = populate_queue(S_poi_freq_dict)\n",
    "        print(type(s_poi_freq_queue_list))\n",
    "        print('s*: ',s_poi_freq_queue_list)\n",
    "        print(s_poi_freq_queue_list[0])\n",
    "        print(type(s_poi_freq_queue_list[0]))\n",
    "        print('')\n",
    "        \n",
    "        cocluster_sequence_str = ''\n",
    "        cocluster_attributes_list = ''\n",
    "        cocluster_index_rows_set = set()\n",
    "        cocluster_elements_set = set()\n",
    "        cocluster_cost_function = sys.maxsize\n",
    "        \n",
    "        for poi_name, poi_frequence in S_poi_freq_dict.items():\n",
    "#             print(poi_name,poi_frequence)\n",
    "            if cocluster_sequence_str == '':\n",
    "                head_sequence_str = poi_name\n",
    "                trajectories_head_sequence_set = poi_at_trajs_dict_set[poi_name]\n",
    "                tail_sequence_str = poi_name\n",
    "                trajectories_tail_sequence_set = poi_at_trajs_dict_set[poi_name]\n",
    "\n",
    "                for poi_node_queue in s_poi_freq_queue_list:\n",
    "    #                 print(poi_node_queue)\n",
    "    #                 print(poi_node_queue[0],poi_node_queue[1])\n",
    "\n",
    "                    #### test POI at the HEAD ###\n",
    "                    tmp_head_sequence_str = head_sequence_str\n",
    "                    head_sequence_str = poi_node_queue[0]+'-'+head_sequence_str\n",
    "                    print('-> Head sequence: ',head_sequence_str)\n",
    "                    tmp_traj_set = trajectories_head_sequence_set\n",
    "                    trajectories_head_sequence_set = trajectories_head_sequence_set.intersection(poi_at_trajs_dict_set[poi_node_queue[0]])\n",
    "                    trajectories_head_sequence_set, position_poi_per_traj_head = check_sequence(trajs_data_dict_list,\n",
    "                                                                                                trajectories_head_sequence_set,\n",
    "                                                                                                head_sequence_str)\n",
    "                    \n",
    "                    if len(trajectories_head_sequence_set) > 0:\n",
    "                        print('Trajectories with this sequence: {}'.format(trajectories_head_sequence_set))\n",
    "                        elements_head_sequence = form_elements(trajectories_head_sequence_set,\n",
    "                                                               head_sequence_str,\n",
    "                                                               position_poi_per_traj_head)    \n",
    "                        overlapped_elements = elements_head_sequence.intersection(final_clustered_elements)\n",
    "                        cost_head_sequence = cost_function(len(trajectories_head_sequence_set),\n",
    "                                                           len(head_sequence_str.split('-')),\n",
    "                                                           len(overlapped_elements))\n",
    "                        overlap_coef_head = overlap_coefficient(elements_head_sequence,final_coclusters)\n",
    "                        print('Head cost: {} and overlap_coef: {}.'.format(cost_head_sequence,\n",
    "                                                                           overlap_coef_head))\n",
    "                    else:\n",
    "                        print('Tested head sequence \"{}\" does NOT exist!'.format(head_sequence_str))\n",
    "                        trajectories_head_sequence_set = tmp_traj_set\n",
    "                        head_sequence_str = tmp_head_sequence_str\n",
    "                        cost_head_sequence = sys.maxsize\n",
    "                        overlap_coef_head = 1\n",
    "                    #### END test HEAD sequence ####\n",
    "\n",
    "                    #### test POI at the TAIL ####\n",
    "                    tmp_tail_sequence_str = tail_sequence_str\n",
    "                    tail_sequence_str = tail_sequence_str+'-'+poi_node_queue[0]\n",
    "                    print('-> Tail sequence: ',tail_sequence_str)\n",
    "                    tmp_traj_set = trajectories_tail_sequence_set\n",
    "                    trajectories_tail_sequence_set = trajectories_tail_sequence_set.intersection(poi_at_trajs_dict_set[poi_node_queue[0]])\n",
    "                    trajectories_tail_sequence_set, position_poi_per_traj_tail = check_sequence(trajs_data_dict_list,\n",
    "                                                                                                trajectories_tail_sequence_set,\n",
    "                                                                                                tail_sequence_str)\n",
    "                    \n",
    "                    if (len(trajectories_tail_sequence_set) > 0):\n",
    "                        print('Trajectories with this sequence: {}'.format(trajectories_tail_sequence_set))\n",
    "                        elements_tail_sequence = form_elements(trajectories_tail_sequence_set,\n",
    "                                                               tail_sequence_str,\n",
    "                                                               position_poi_per_traj_tail)\n",
    "                        overlapped_elements = elements_tail_sequence.intersection(final_clustered_elements)\n",
    "                        cost_tail_sequence = cost_function(len(trajectories_tail_sequence_set),\n",
    "                                                           len(tail_sequence_str.split('-')),\n",
    "                                                           len(overlapped_elements))\n",
    "                        overlap_coef_tail = overlap_coefficient(elements_tail_sequence,final_coclusters)\n",
    "                        print('Tail cost: {} and overlap_coef: {}.'.format(cost_tail_sequence,\n",
    "                                                                           overlap_coef_tail))\n",
    "                    else:\n",
    "                        print('Tested tail sequence \"{}\" does NOT exist!'.format(tail_sequence_str))\n",
    "                        trajectories_tail_sequence_set = tmp_traj_set\n",
    "                        tail_sequence_str = tmp_tail_sequence_str\n",
    "                        cost_tail_sequence = sys.maxsize\n",
    "                        overlap_coef_tail = 1\n",
    "                    #### END test TAIL sequence ####\n",
    "\n",
    "\n",
    "                    print('Current co-cluster cost: ',cocluster_cost_function)\n",
    "                    print('Queue s* BEFORE to upadate: ',s_poi_freq_queue_list)\n",
    "                    \n",
    "                    ### test if some sequence can improve the current co-cluster\n",
    "                    improved_cocluster = False\n",
    "                    if ((cost_head_sequence < cost_tail_sequence) and \n",
    "                        (cost_head_sequence <= cocluster_cost_function) and \n",
    "                        (overlap_coef_head <= overlap_coef_threshold)):\n",
    "                        print('Co-cluster improved with HEAD sequence.')\n",
    "                        \n",
    "                        # update the nodes of queue s.\n",
    "                        update_queue_s(cocluster_sequence_str,head_sequence_str,\n",
    "                                       s_poi_freq_queue_list,poi_node_queue)\n",
    "                        \n",
    "                        cocluster_sequence_str = head_sequence_str\n",
    "                        cocluster_attributes_list = head_sequence_str.split('-')\n",
    "                        cocluster_index_rows_set = trajectories_head_sequence_set.copy()\n",
    "                        cocluster_elements_set = elements_head_sequence.copy()\n",
    "                        cocluster_cost_function = cost_head_sequence\n",
    "                        improved_cocluster = True\n",
    "                    elif ((cost_tail_sequence < cost_head_sequence) and \n",
    "                          (cost_tail_sequence <= cocluster_cost_function)):\n",
    "                        print('Co-cluster improved with TAIL sequence.')\n",
    "                        \n",
    "                        # update the nodes of queue s.\n",
    "                        update_queue_s(cocluster_sequence_str,tail_sequence_str,\n",
    "                                       s_poi_freq_queue_list,poi_node_queue)\n",
    "                        \n",
    "                        cocluster_sequence_str = tail_sequence_str\n",
    "                        cocluster_attributes_list = tail_sequence_str.split('-')\n",
    "                        cocluster_index_rows_set = trajectories_tail_sequence_set.copy()\n",
    "                        cocluster_elements_set = elements_tail_sequence.copy()\n",
    "                        cocluster_cost_function = cost_tail_sequence\n",
    "                        improved_cocluster = True\n",
    "                    \n",
    "                    if improved_cocluster:\n",
    "                        trajectories_head_sequence_set = cocluster_index_rows_set\n",
    "                        head_sequence_str = cocluster_sequence_str\n",
    "                        trajectories_tail_sequence_set = cocluster_index_rows_set\n",
    "                        tail_sequence_str = cocluster_sequence_str\n",
    "#                         ## decrementar attributo em s\n",
    "#                         print('Poi node: ', poi_node_queue)\n",
    "#                         poi_node_queue[1] -= 1\n",
    "#                         print('Poi node: ', poi_node_queue)\n",
    "                    else:\n",
    "                        print('Tested sequences do not improved the current co-cluster.')\n",
    "                        trajectories_head_sequence_set = tmp_traj_set\n",
    "                        head_sequence_str = tmp_head_sequence_str\n",
    "                        trajectories_tail_sequence_set = tmp_traj_set\n",
    "                        tail_sequence_str = tmp_tail_sequence_str\n",
    "                    \n",
    "                    print('Queue s* AFTER to update: ',s_poi_freq_queue_list)\n",
    "\n",
    "                    print('')\n",
    "    #                 tail_sequence_str = tail_sequence_str+'-'+poi_node_queue[0]\n",
    "    #                 print('Tail: ',tail_sequence_str)\n",
    "            \n",
    "            else:\n",
    "                print('Co-cluster identified. Go to the next searching.')\n",
    "                final_coclusters.update({str(iter_k):{'cc_objs':cocluster_index_rows_set,\n",
    "                                                      'cc_atts':cocluster_sequence_str,\n",
    "                                                      'cc_elements':cocluster_elements_set}})\n",
    "                final_clustered_elements = final_clustered_elements.union(cocluster_elements_set)\n",
    "                print('Main list S BEFORE to update: ',S_poi_freq_dict)\n",
    "                for attribute in cocluster_attributes_list:\n",
    "                    S_poi_freq_dict[attribute] -= 1\n",
    "                print('Main list S AFTER to update: ',S_poi_freq_dict)\n",
    "#                 partial = timer()\n",
    "#                 print('Cluster \"{}\" finished at time \"{}\".'.format((iter_k+1),(partial-start))\n",
    "                break\n",
    "            \n",
    "            \n",
    "        ## into loop of iteration k\n",
    "        \n",
    "        partial = timer()\n",
    "        print('Cluster \"{}\" finished at time \"{}\".'.format(iter_k+1,partial-start))\n",
    "    ## out of loop iteraton k\n",
    "    end = timer()\n",
    "    print('Total clustering time: ',str(end-start))\n",
    "    print('\\nFinal co-clusters: ',final_coclusters)\n",
    "        # candidate_cocluster = find_base_cocluster()\n",
    "        # max_cocluster = expand_cocluster()\n",
    "        # testa se max_cocluster != VAZIO e ainda não descoberto\n",
    "            # OK: salva max_cocluster no conjunto de co-clusters\n",
    "            #     atualiza a lista de frequencia S\n",
    "            # FAIL: terminar busca por novos co-clusters\n",
    "    \n",
    "    # return conjunto de co-clusters\n",
    "            \n",
    "    \n",
    "    \n",
    "#     print(\"Data dict (in main ococlus):\"+str(data_dict))\n",
    "#     for itertator in range(1,k+1):\n",
    "#         print(\"Iterator: \",itertator)\n",
    "#         if VERBOSE:\n",
    "#             print(\"Pattern model: \",pattern_model)\n",
    "#         C,E,new_cost_dense = find_dense_cocluster(data_dict, data_res_dict)\n",
    "#         if new_cost_dense >= 0:\n",
    "#             print(\"No relevant co-cluster can be found anymore.\")\n",
    "#             break    \n",
    "#         C_expanded = expand_dense_cocluster(C, E, data_dict,e_obj,e_att)\n",
    "#         print(\"Expanded: \"+str(C_expanded))\n",
    "\n",
    "#         print(\"\")\n",
    "#         new_cost = cost_function(len(pattern_model[0].union(C[1])),\n",
    "#                                               len(pattern_model[1].union(set(C[0]))))\n",
    "#         if VERBOSE:\n",
    "#             print(\"Dense co-cluster cost: \",new_cost_dense)\n",
    "#             print(\"Current model cost: \",cost_model)\n",
    "#             print(\"New model cost with the found co-cluster: \",new_cost)\n",
    "#             print(\"Attribute cluster:\"+str(C[0])+\", Object cluster: \"+str(C[1]))\n",
    "#             print(\" \")\n",
    "        \n",
    "#         if new_cost < cost_model:\n",
    "#             final_coclusters.append(C)\n",
    "#             cost_model = new_cost\n",
    "#             num_of_coclusters += 1\n",
    "#             data_res_dict = update_residual_dataset(data_res_dict,C[0],C[1])\n",
    "#             pattern_model[0] = pattern_model[0].union(C[1])\n",
    "#             pattern_model[1] = pattern_model[1].union(set(C[0]))\n",
    "#         else:\n",
    "#             print(\"No co-cluster can be found anymore.\")\n",
    "#             break\n",
    "    \n",
    "#     print(\"Find global co-cluster is done.\")\n",
    "    \n",
    "#     if find_overlap:\n",
    "#         attributes_per_cluster = []\n",
    "#         objects_per_cluster = []\n",
    "#         for i in range(len(final_coclusters)):\n",
    "#             attributes_per_cluster.append(set(list(map(int,final_coclusters[i][0]))))\n",
    "#             objects_per_cluster.append(final_coclusters[i][1])\n",
    "\n",
    "#         print(\"\\nFind derived overlapped co-clusters method\")\n",
    "#         attribute_clusters, objects_clusters = findOverlap(attributes_per_cluster,objects_per_cluster)\n",
    "\n",
    "#         final_coclusters = []\n",
    "#         for i in range(len(attribute_clusters)):\n",
    "#             final_coclusters.append([attribute_clusters[i],objects_clusters[i]])\n",
    "\n",
    "#         print(\"\\nNon-overlapped and overlapped co-clusters.\")\n",
    "#         print(\"Number of co-clusters: \",len(final_coclusters))\n",
    "#         if VERBOSE:\n",
    "#             print(\"Final co-clusters: \",final_coclusters)\n",
    "#     else:\n",
    "#         print(\"\\nNon-overlapped co-clusters.\")\n",
    "#         print(\"Number of co-clusters: \",num_of_coclusters)\n",
    "# #         for i in range(len(final_coclusters)):\n",
    "# #             tmp_data = final_coclusters[i][1]\n",
    "# #             final_coclusters[i][1] = list(tmp_data)\n",
    "#         if VERBOSE:\n",
    "#             print(\"Final co-clusters: \",final_coclusters)\n",
    "    \n",
    "#     print(\"Final co-clusters:\"+str(final_coclusters))\n",
    "    \n",
    "#     print(\"Remap the attributes to its original label (final co-clusters):\")\n",
    "#     # go back to the original att map\n",
    "#     for i in range(len(final_coclusters)):\n",
    "#         cluster_att_id = final_coclusters[i][0]\n",
    "#         for j in range(len(cluster_att_id)):\n",
    "#             final_coclusters[i][0][j] = map_id_to_attribute[str(cluster_att_id[j])]\n",
    "            \n",
    "    return D,final_coclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'cc_objs': [], 'cc_atts': [], 'cc_elements': []}}\n",
      "[]\n",
      "{'1': {'cc_objs': [], 'cc_atts': [], 'cc_elements': []}, '2': {}}\n"
     ]
    }
   ],
   "source": [
    "f = {}\n",
    "f['1'] = {}\n",
    "f['1'].update({'cc_objs':[]})\n",
    "f['1'].update({'cc_atts':[]})\n",
    "f['1'].update({'cc_elements':[]})\n",
    "print(f)\n",
    "print(f['1']['cc_objs'])\n",
    "f.update({'2':{}})\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_queue_s(cocluster_sequence_str,tested_sequence_str,s_poi_freq_queue_list,poi_node_queue):\n",
    "    '''\n",
    "    Method to update the nodes in queue s. It decrements the value of a given node in s.\n",
    "    The input are:\n",
    "        1. The current string sequence of a cocluster;\n",
    "        2. The tested string sequence to improve a cocluster;\n",
    "        3. The queue s;\n",
    "        4. A single node of queue s.\n",
    "    '''\n",
    "    # update list s when the first sequence is identified\n",
    "    if cocluster_sequence_str == '':\n",
    "        tmp_split = tested_sequence_str.split('-')\n",
    "        for attribute in tmp_split:\n",
    "            for node_s in s_poi_freq_queue_list:\n",
    "                if attribute == node_s[0]:\n",
    "                    node_s[1] -= 1\n",
    "                    break\n",
    "    else: # update a single node in case a sequence is already discovered\n",
    "        poi_node_queue[1] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1',\n",
       " 'a2',\n",
       " 'a3',\n",
       " 'a4',\n",
       " 'b1',\n",
       " 'b2',\n",
       " 'b3',\n",
       " 'b4',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'c3',\n",
       " 'c4',\n",
       " 'd1',\n",
       " 'd2',\n",
       " 'd3',\n",
       " 'd4']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = ['a','b','c','d']\n",
    "a2 = [1,2,3,4]\n",
    "[str(i)+str(j) for i in a1 for j in a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 3 0 0 0 0]\n",
      " [1 1 3 0 0 0 0]\n",
      " [1 1 3 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[[1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]]\n",
      "[[2 2 4 1 1 1 1]\n",
      " [2 2 4 1 1 1 1]\n",
      " [2 2 4 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]]\n",
      "[[4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]\n",
      " [4 4 4 4 4 4 4]]\n",
      "[[-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]\n",
      " [-3 -3 -3 -3 -3 -3 -3]]\n",
      "-147\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,1,1,0,0,0,0])\n",
    "b = np.array([1,1,3,0,0,0,0])\n",
    "c = np.outer(a,b)\n",
    "print(c)\n",
    "d = (c*0)+1\n",
    "print(d)\n",
    "print(c+d)\n",
    "e = d*4\n",
    "print(e)\n",
    "f = d-e\n",
    "print(f)\n",
    "print(sum(sum(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['f', 2], ['h', 5], ['t', 1]]\n",
      "[['f', 2], ['h', 5], ['t', 1]]\n",
      "[['h', 5], ['f', 2], ['t', 1]]\n",
      "[['h', 4], ['f', 2], ['t', 1]]\n"
     ]
    }
   ],
   "source": [
    "def myFunc(e):\n",
    "    return e[:][1]\n",
    "\n",
    "er = [['f',2],['h',5],['t',1]]\n",
    "print(er)\n",
    "er.sort()\n",
    "print(er)\n",
    "er.sort(reverse=True, key=myFunc)\n",
    "print(er)\n",
    "er[0][1] -= 1\n",
    "print(er)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dense co-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dense_cocluster(input_dataset, residual_dataset):\n",
    "    '''\n",
    "    Input\n",
    "        input_dataset: a dictionary with the list of objects per attribute. input_dataset[att] -> [objects_in_att]\n",
    "        residual_dataset: it is a copy of input_dataset used to sort the attributes\n",
    "    \n",
    "    Output\n",
    "        C: A dense co-cluster. It is a array with a list of attributes and a set of objects. C[att_list,obj_set]\n",
    "    '''\n",
    "    print(\"Find dense cocluster method.\")\n",
    "    att_dense_cocluster_list = []\n",
    "#     att_dense_cocluster_set = set([])\n",
    "    obj_dense_cocluster_list = []\n",
    "    att_extension_list = []\n",
    "    cost_dense_cocluster = sys.float_info.max\n",
    "    \n",
    "    sads = sort_att_ds(residual_dataset)\n",
    "#     print(\"Sorted att: \",sads)\n",
    "    cc_att = sads[0]\n",
    "#     att_dense_cocluster_list.append(cc_att)\n",
    "    att_dense_cocluster_list.append(cc_att)\n",
    "    obj_dense_cocluter_set = set(residual_dataset[cc_att]) # get the objs forthe given attribute cc_att\n",
    "    count_group_att = 1\n",
    "#     cost_dense_cocluster = cost_function(len(pattern_model[0].union(obj_dense_cocluter_set)),\n",
    "#                                          len(pattern_model[1].union(set(cc_att))))\n",
    "    new_cost_function = cost_function(len(obj_dense_cocluter_set), count_group_att)\n",
    "    if VERBOSE:\n",
    "        print(\"New cost: \"+str(new_cost_function)+\", Cost dense: \"+str(cost_dense_cocluster))\n",
    "    cost_dense_cocluster = new_cost_function\n",
    "    \n",
    "    for next_att in range(1,len(sads)):\n",
    "        cc_att_test = sads[next_att]\n",
    "#         print(\"Attribute: \",cc_att_test)\n",
    "        curr_cc_att = set(residual_dataset[cc_att_test])\n",
    "        intersection_objs = obj_dense_cocluter_set.intersection(curr_cc_att)\n",
    "#         print(\"Intersection test: \",intersection_objs)\n",
    "        tmp = att_dense_cocluster_list.copy()\n",
    "        tmp.append(cc_att_test)\n",
    "#         new_cost_function = cost_function(len(pattern_model[0].union(intersection_objs)),\n",
    "#                                           len(pattern_model[1].union(set(tmp))))\n",
    "        new_cost_function = cost_function(len(intersection_objs),count_group_att+1)\n",
    "#         print(intersection_objs,curr_cc_att)\n",
    "#         print(len(intersection_objs),count_group_att+1)\n",
    "        if VERBOSE:\n",
    "            print(\"New cost: \"+str(new_cost_function)+\", Cost dense: \"+str(cost_dense_cocluster))\n",
    "            \n",
    "        if  new_cost_function <= cost_dense_cocluster:\n",
    "            att_dense_cocluster_list.append(cc_att_test)\n",
    "            obj_dense_cocluter_set = intersection_objs\n",
    "            cost_dense_cocluster = new_cost_function\n",
    "            count_group_att += 1\n",
    "        else:\n",
    "            att_extension_list.append(cc_att_test)\n",
    "    \n",
    "    C = [att_dense_cocluster_list, obj_dense_cocluter_set]\n",
    "    print(att_dense_cocluster_list)\n",
    "    \n",
    "    # no good rectangle was found\n",
    "    if cost_dense_cocluster >= 0:\n",
    "        att_extension_list = []\n",
    "        C = [[],set()]\n",
    "    else:\n",
    "        C = [att_dense_cocluster_list, obj_dense_cocluter_set]\n",
    "    \n",
    "    return C, att_extension_list, cost_dense_cocluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= [1,2,3,4,5,6,7,8]\n",
    "print(t)\n",
    "push_to_end = 3\n",
    "complete_cicle = False\n",
    "reload = True\n",
    "print(t.pop(2))\n",
    "print(t)\n",
    "t= [1,2,3,4,5,6,7,8]\n",
    "print(t)\n",
    "# while(reload and complete_cicle != True):\n",
    "#     for i in range(len(t)):\n",
    "#         if t[i] == push_to_end:\n",
    "#             tmp = t.pop(i)\n",
    "#             t.append(tmp)\n",
    "#             complete_cicle == True\n",
    "#         else:\n",
    "#             print(t[i])\n",
    "#         if push_to_end == t[i] and complete_cicle == True:\n",
    "#             reload = False\n",
    "#             break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand dense co-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dense_cocluster(C,E,att_data_dict,e_obj,e_att):\n",
    "    '''\n",
    "    INPUT\n",
    "        C = a tuple(list_atts, set_objs); list_atts has the list of attributes and set_objs is a set of objects\n",
    "        E = list of attributes not present in C_a to be tested\n",
    "        att_data_dict = a dict with list of objects per att; att_data_dict[att] -> [objects]\n",
    "        e_obj = maximum object error \n",
    "        e_att = maximum attribute error \n",
    "    \n",
    "    OUTPUT\n",
    "        \n",
    "    '''\n",
    "    print(\"Expand co-cluster method.\")\n",
    "    added_att = True\n",
    "    \n",
    "    if not C[0]: # nothing good to discover\n",
    "        pass\n",
    "    else:\n",
    "        curr_cost = cost_function(len(C[1]), len(C[0]),0,0)\n",
    "        noise_added = 0 # the quantanty of noise added in the cluster during the process\n",
    "        while(added_att):\n",
    "            # try to add new objects to cocluster C\n",
    "            try_new_objs = set(np.arange(0,len(D))).difference(C[1]) # get the objects not present in C\n",
    "            if VERBOSE:\n",
    "                print(\"# Try to extend the list of Objects #\")\n",
    "            for obj in try_new_objs:\n",
    "                obj_quantanty = 0\n",
    "                for att in C[0]:\n",
    "                    if D[obj][int(att)] == 1:\n",
    "                        obj_quantanty += 1\n",
    "                if not_too_noisy(obj_quantanty, C, e_obj, e_att, att_data_dict, E, \"obj\"):\n",
    "#                     print(\"Ruído valor: \",not_noise_val)\n",
    "                    if VERBOSE:\n",
    "                        print(\"OBJ->\"+str(obj),end=\" | \")\n",
    "                    new_cost = cost_function(len(C[1])+1,len(C[0]),0,(noise_added+(len(C[0])-obj_quantanty)))\n",
    "                    if new_cost <= curr_cost:\n",
    "                        if VERBOSE:\n",
    "                            print(\"New_cost:\"+str(new_cost)+\" ; Curr_cost:\"+str(curr_cost))\n",
    "                        \n",
    "                        C[1].add(obj)\n",
    "                        curr_cost = new_cost\n",
    "                        noise_added += (len(C[0])-obj_quantanty)\n",
    "                else:\n",
    "                    if VERBOSE:\n",
    "                        print(\"Object too noisy to be added. (Obj: \"+str(obj)+\")\")\n",
    "\n",
    "            added_att = False\n",
    "            # try to add new attributes to cocluster C\n",
    "            if VERBOSE:\n",
    "                print(\"# Try to extend the list of Attributes #\")\n",
    "            \n",
    "            while(len(E) != 0):\n",
    "                att = E.pop(0)\n",
    "                if VERBOSE:\n",
    "                    print(\"ATT->\"+str(att), end= \" | \")\n",
    "                att_obj_quantanty = len(C[1].intersection(set(att_data_dict[att])))\n",
    "                if not_too_noisy(att_obj_quantanty, C, e_obj, e_att, att_data_dict, E, \"att\"):\n",
    "                    new_cost = cost_function(len(C[1]),len(C[0])+1,0,(noise_added+(len(C[1])-att_obj_quantanty)))\n",
    "                    \n",
    "                    if VERBOSE:\n",
    "                        print(\"New_cost:\"+str(new_cost)+\" ; Curr_cost:\"+str(curr_cost))\n",
    "                    \n",
    "                    if new_cost <= curr_cost:\n",
    "                        C[0].append(str(att))\n",
    "                        curr_cost = new_cost\n",
    "                        added_att = True\n",
    "                        noise_added += (len(C[1])-att_obj_quantanty)\n",
    "                        break\n",
    "                else:\n",
    "                    if VERBOSE:\n",
    "                        print(\"Attribute too noisy to be added. (Att: \"+str(att)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find overlapped co-clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOverlap(SetsC,SetsR):\n",
    "    '''\n",
    "    INPUT\n",
    "        SetsC: It has K sets of attributes regarding each attribute cluster\n",
    "        SetsR: It has K sets of objects regarding each object cluster\n",
    "    \n",
    "    OUTPUT\n",
    "        columnClusters: A list with the attribute clusters\n",
    "        rowClusters: A list with the object clusters\n",
    "    '''\n",
    "    newSetsColumns = []\n",
    "    newSetsRows = []\n",
    "    \n",
    "    # merge sets that can overlap\n",
    "    merge(SetsC,SetsR,newSetsColumns,newSetsRows)\n",
    "\n",
    "    #Removing sets with redundant information\n",
    "    removeSubsets(newSetsColumns,newSetsRows)\n",
    "    \n",
    "    columnClusters = newSetsColumns\n",
    "    rowClusters = newSetsRows\n",
    "    return columnClusters,rowClusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_data):\n",
    "    '''\n",
    "    This method will assign the variables used by the algorithm.\n",
    "    \n",
    "    INPUT\n",
    "        input_data: A panda dataframe of the input data file.\n",
    "    \n",
    "    OUTPUT\n",
    "        D: A binary matrix from the input data.\n",
    "        N: A noise binary matrix with the same size of D.\n",
    "        data_dict: A dictionary to store D as a vertical representation.\n",
    "        data_res_dict: A copy of data_dict used to sort the attributes of D and find unconvered elements.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    data_pd = input_data #txt file with sequence of check-ins (POI)\n",
    "    frequence_per_poi_dict = {} # store the frequence of a POI as \"POI\": num_of_occurrences\n",
    "    poi_at_trajs_dict_set = {}  # store a set with each index line (tid trajectory) that contains a given POI.\n",
    "                            # \"POI\": set(0,1,4,...); It is the S variable\n",
    "#     global data_res_dict\n",
    "    uncover_poi_dict = {} # It is the s* variable\n",
    "#     global D # input data as a binary matrix\n",
    "#     global N # noise matrix with the same size of D\n",
    "    num_of_objects = 0\n",
    "    num_of_attributes = 0\n",
    "    map_id_to_attribute = {} # map the \n",
    "    map_attribute_to_id = {} # map the\n",
    "    trajectory_dict = {} # it stores the trajectories with its check-ins. \"TID\": [POI1,POI2,...]\n",
    "#     max_val_att = 0 \n",
    "    att_id = 0 # assign an ID to each attribute\n",
    "    \n",
    "    # read each line\n",
    "    for index, row in data_pd.iterrows():\n",
    "        num_of_objects+=1\n",
    "        object_data = row[0].split(\" \")\n",
    "#         trajectory_dict[str(index)] = {}\n",
    "#         trajectory_dict[str(index)] = object_data\n",
    "        trajectory_dict[str(index)] = []\n",
    "        \n",
    "#         for attribute in object_data: # we look at each item of the given transaction\n",
    "        for att_j in range(len(object_data)): # we look at each item of the given transaction\n",
    "            attribute = object_data[att_j]\n",
    "            \n",
    "            if attribute != \"\":\n",
    "#                 if int(attribute) > max_val_att:\n",
    "#                     max_val_att = int(attribute)\n",
    "#                 if attribute not in map_unique_attributes_dataset:\n",
    "#                 if attribute not in map_attribute_to_id.keys():\n",
    "    \n",
    "                if attribute not in map_attribute_to_id: # mapping\n",
    "#                     unique_attributes_dataset.append(attribute)\n",
    "                    map_attribute_to_id[attribute] = str(att_id)\n",
    "                    map_id_to_attribute[str(att_id)] = attribute\n",
    "                    att_id += 1\n",
    "                \n",
    "                # substitute the check-in by its ID\n",
    "                trajectory_dict[str(index)].append(map_attribute_to_id[attribute])\n",
    "                \n",
    "                # store the indeces containing a given POI\n",
    "                if map_attribute_to_id[attribute] in poi_at_trajs_dict_set:\n",
    "#                     data_dict[map_attribute_to_id[attribute]].append(index)\n",
    "                    poi_at_trajs_dict_set[map_attribute_to_id[attribute]].add(str(index))\n",
    "                else:\n",
    "#                     data_dict[map_attribute_to_id[attribute]] = [index]\n",
    "                    poi_at_trajs_dict_set[map_attribute_to_id[attribute]] = set([str(index)])\n",
    "                \n",
    "                # store the frequence for each POI\n",
    "                if map_attribute_to_id[attribute] in frequence_per_poi_dict:\n",
    "                    current_value = frequence_per_poi_dict[map_attribute_to_id[attribute]]\n",
    "                    frequence_per_poi_dict[map_attribute_to_id[attribute]] = current_value + 1\n",
    "                else:\n",
    "                    frequence_per_poi_dict[map_attribute_to_id[attribute]] = 1\n",
    "            \n",
    "#             # simulate the idea of a linked list structure with a dict to facility the check\n",
    "#             if att_j == 0 and len(object_data) > 1:\n",
    "# #                 print(\"IF 1\")\n",
    "# #                 print(object_data[att_j])\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[attribute]] = {\"Previous\": \"None\", \"Next\": \"None\"}\n",
    "#             elif att_j == 0 and len(object_data) == 1:\n",
    "# #                 print(\"IF 2\")\n",
    "# #                 print(object_data[att_j])\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[attribute]] = {\"Previous\": \"None\", \"Next\": \"None\"}\n",
    "#             elif att_j > 0 and att_j < (len(object_data)-1):\n",
    "# #                 print(\"IF 3\")\n",
    "# #                 print(object_data[att_j])\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[attribute]] = {\"Previous\": map_attribute_to_id[object_data[att_j-1]],\n",
    "#                                                                                \"Next\": \"None\"}\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[object_data[att_j-1]]][\"Next\"] = map_attribute_to_id[attribute]\n",
    "#             else:\n",
    "# #                 print(\"IF 4\")\n",
    "# #                 print(object_data[att_j])\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[attribute]] = {\"Previous\": map_attribute_to_id[object_data[att_j-1]],\n",
    "#                                                                                \"Next\": \"None\"}\n",
    "#                 trajectory_dict[str(index)][map_attribute_to_id[object_data[att_j-1]]][\"Next\"] = map_attribute_to_id[attribute]\n",
    "            \n",
    "        \n",
    "#     print(\"Trajectory dict:\"+str(trajectory_dict))\n",
    "                    \n",
    "    uncover_poi_dict = poi_at_trajs_dict_set.copy()\n",
    "#     num_of_attributes = len(data_dict)\n",
    "#     num_of_attributes = max_val_att+1\n",
    "#     num_of_attributes = len(map_attribute_to_id)\n",
    "    print(\"######################################\")\n",
    "    print(\"Number of trajectories: \"+str(index+1))\n",
    "    print(\"Number of unique check-ins: \"+str(len(map_attribute_to_id)))\n",
    "    print(\"########################################\")\n",
    "    if VERBOSE:\n",
    "        print(\"Map_attribute_to_id:\"+str(map_attribute_to_id))\n",
    "        print(\"\")\n",
    "        print(\"Map_id_to_attribute:\"+str(map_id_to_attribute))\n",
    "        print(\"\")\n",
    "        print(\"Frequence_per_poi:\"+str(frequence_per_poi_dict))\n",
    "        print(\"\")\n",
    "        print(\"Trajectories: \"+str(trajectory_dict))\n",
    "        print(\"\")\n",
    "        print(\"POI occurring at trajectories: \"+str(poi_at_trajs_dict_set))\n",
    "        print(\"Get data is DONE!\")\n",
    "        \n",
    "    \n",
    "#     D = np.zeros((num_of_objects,num_of_attributes),dtype=int)\n",
    "#     for key, values in poi_at_trajs_dict.items():\n",
    "#         print(\"key:\"+str(key)+\" Values:\"+str(values))\n",
    "#         for line in values:\n",
    "# #             D[line][int(key)] = 1\n",
    "# #             D[line][map_unique_attributes_dataset[key]] = 1\n",
    "# #             print(line,key)\n",
    "# #             print(type(line),type(key))\n",
    "#             D[line][int(key)] = 1\n",
    "#     N = np.zeros((num_of_objects,num_of_attributes),dtype=int)\n",
    "    \n",
    "#     return D, N, poi_at_trajs_dict, data_res_dict, map_id_to_attribute\n",
    "    return map_id_to_attribute, frequence_per_poi_dict, poi_at_trajs_dict_set, trajectory_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fila 1:  <class 'collections.deque'>\n",
      "Fila 2:  deque([{'hotel': 4}, {'padaria': 2}])\n",
      "deque([{'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}])\n",
      "deque([{'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}, {'festa': 1}])\n",
      "deque([{'aeroporto': 1}, {'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}, {'festa': 1}])\n",
      "{'hotel': 4}\n",
      "deque([{'aeroporto': 1}, {'padaria': 3}, {'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}, {'festa': 1}])\n",
      "7\n",
      "{'festa': 1}\n",
      "<class 'dict'>\n",
      "{'trabalho': 9}\n",
      "trabalho\n",
      "4\n",
      "Fila 1:  deque([{'aeroporto': 1}, {'padaria': 3}, {'hotel': 4}, {'casa': 7}, {'trabalho': 9}, {'padaria': 2}])\n",
      "Fila 2:  deque([{'hotel': 4}, {'padaria': 2}])\n",
      "Fila 1:  deque([{'padaria': 3}, {'hotel': 4}, {'casa': 7}, {'trabalho': 9}])\n",
      "Fila 1:  deque([{'hotel': 4}, {'casa': 7}, {'trabalho': 9}])\n",
      "Element poped:  {'padaria': 3}\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "my_fila = deque([{'hotel':4},{'casa':7},{'trabalho':9},{'padaria':2}])\n",
    "my_fila2 = deque()\n",
    "my_fila2.append({'hotel':4})\n",
    "my_fila2.append({'padaria':2})\n",
    "print('Fila 1: ',type(my_fila))\n",
    "print('Fila 2: ',my_fila2)\n",
    "print(my_fila)\n",
    "my_fila.append({'festa':1})\n",
    "print(my_fila)\n",
    "my_fila.appendleft({'aeroporto':1})\n",
    "print(my_fila)\n",
    "print(my_fila[1])\n",
    "my_fila.insert(1,{'padaria':3})\n",
    "print(my_fila)\n",
    "print(len(my_fila))\n",
    "print(my_fila.pop())\n",
    "print(type(my_fila[2]))\n",
    "r = my_fila[4]\n",
    "print(r)\n",
    "print(list(r.keys())[0])\n",
    "print(my_fila.index(my_fila[4],2,len(my_fila)))\n",
    "print('Fila 1: ',my_fila)\n",
    "print('Fila 2: ',my_fila2)\n",
    "my_fila.pop()#delete from the right end\n",
    "my_fila.popleft()#delete from the left end\n",
    "print('Fila 1: ',my_fila)\n",
    "f = my_fila.popleft()\n",
    "print('Fila 1: ',my_fila)\n",
    "print('Element poped: ',f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = {'C':4,'B':6,'A':[4,2]}\n",
    "print(g)\n",
    "print(len(g))\n",
    "print('F' in g)\n",
    "print(6 in g)\n",
    "print(g.values())\n",
    "h = set([1,2,2])\n",
    "print(h)\n",
    "h.add(3)\n",
    "print(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too noisy (line,col)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_too_noisy(count_presence, C, e_obj, e_att, att_data_dict, E, dimension):\n",
    "    num_of_atts = len(C[0])\n",
    "    num_of_objs = len(C[1])\n",
    "    if dimension == \"obj\":\n",
    "        # obj must be present in at least (1-e_obj).||C_a||\n",
    "        return count_presence >= ((1-e_obj) * num_of_atts) # return true if the obj is not too noisy\n",
    "    else:\n",
    "        # col must be present in at least (1-e_tt).||C_o||\n",
    "        return count_presence >= ((1-e_att) * num_of_objs) # return true if the att is not too noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(numOfObj, numOfAtt, cov=0, noise=0):\n",
    "    if VERBOSE:\n",
    "        print('Num. objs: {0:2d}, Num. att: {1:2d}, Num. covered: {2:2d}, Num. noise: {3:2d}'.format(numOfObj,numOfAtt,cov,noise))\n",
    "#     return ((numOfObj+numOfAtt) - (numOfObj*numOfAtt)) + cov + (2*noise)\n",
    "    return ((numOfObj+numOfAtt) - (numOfObj*numOfAtt)) + cov + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(SetsC, SetsR, newSetsColumns, newSetsRows):\n",
    "    num_of_cluster = len(SetsC)\n",
    "    \n",
    "    # keep the original cluster\n",
    "    for set_column_i in range(len(SetsC)):\n",
    "        newSetsRows.append(SetsR[set_column_i])\n",
    "        newSetsColumns.append(SetsC[set_column_i])\n",
    "    \n",
    "    revisit = True\n",
    "    while(revisit):\n",
    "        revisit = False\n",
    "        tmp_r = []\n",
    "        tmp_c = []\n",
    "        size = len(newSetsColumns)\n",
    "        for i in range(size-1):\n",
    "            first_r = newSetsRows[i]\n",
    "            first_c = newSetsColumns[i]\n",
    "            for j in range(i+1,size):\n",
    "                p_intersec_r = first_r.intersection(newSetsRows[j])\n",
    "                p_intersec_c = first_c.union(newSetsColumns[j])\n",
    "                if len(p_intersec_r) > 0:\n",
    "                    tmp_r.append(p_intersec_r)\n",
    "                    tmp_c.append(p_intersec_c)\n",
    "        \n",
    "        for i in range(len(tmp_c)):\n",
    "            change = True\n",
    "            for j in range(len(newSetsColumns)):\n",
    "                if tmp_c[i].issubset(newSetsColumns[j]):\n",
    "                    change = False\n",
    "            if change:\n",
    "                newSetsRows.append(tmp_r[i])\n",
    "                newSetsColumns.append(tmp_c[i])\n",
    "                revisit = True\n",
    "    \n",
    "    # start to find and merge all possible overlapping clusters\n",
    "    for set_column_i in range(num_of_cluster):\n",
    "        tested_pattern = SetsR[set_column_i]\n",
    "        new_cols = SetsC[set_column_i]\n",
    "        overlapped = False\n",
    "        \n",
    "        # finds with who the tested pattern overlaps\n",
    "        set_overlap_id = []\n",
    "        for i in range(num_of_cluster):\n",
    "            if i != set_column_i:\n",
    "                if len(tested_pattern.intersection(SetsR[i])) > 0:\n",
    "                    set_overlap_id.append(i)    \n",
    "        \n",
    "        # Discover if exist overlaps between the sets in set_overlap_id\n",
    "        sub_overlap_id = []\n",
    "        for i in range(len(set_overlap_id)):\n",
    "            try_combine_ids = [set_overlap_id[i]]\n",
    "            added = False\n",
    "            for j in range(i+1,len(set_overlap_id)):\n",
    "                if len(SetsR[set_overlap_id[i]].intersection(SetsR[set_overlap_id[j]])) > 0:\n",
    "                    try_combine_ids.append(set_overlap_id[j])\n",
    "                    added = True\n",
    "            sub_overlap_id.append(try_combine_ids)\n",
    "        #Check if some pattern is isolated and was not added\n",
    "        for id_pattern in set_overlap_id:\n",
    "            added = False\n",
    "            for combined_ids in sub_overlap_id:\n",
    "                if id_pattern in combined_ids:\n",
    "                    added = True\n",
    "            if added == False:\n",
    "                sub_overlap_id.append([id_pattern])\n",
    "        \n",
    "        # merge the analysed pattern with the overlapped combined patterns ids\n",
    "        for pattern_ids in sub_overlap_id:\n",
    "            tested_pattern = SetsR[set_column_i]\n",
    "            new_cols = SetsC[set_column_i]\n",
    "            for pattern_id in pattern_ids:\n",
    "                tmp = tested_pattern.intersection(SetsR[pattern_id])\n",
    "                tested_pattern = tmp\n",
    "                new_cols = new_cols.union(SetsC[pattern_id])\n",
    "            # save the new patterns\n",
    "            newSetsColumns.append(new_cols)\n",
    "            newSetsRows.append(tested_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSubsets(setsColumns,setsRows):\n",
    "    finalSetsCols = []\n",
    "    finalSetsRows = []\n",
    "    \n",
    "    again = True\n",
    "    # We are out of the loop when we do not have any subset to remove\n",
    "    while(again):\n",
    "        again = False\n",
    "        for i in range(len(setsColumns)):\n",
    "            isSubset = False\n",
    "            currC = setsColumns[i]\n",
    "            currR = setsRows[i]\n",
    "            \n",
    "            for j in range(len(setsColumns)):\n",
    "                if i != j:\n",
    "                    nextC = setsColumns[j]\n",
    "                    nextR = setsRows[j]\n",
    "                    if currC.issubset(nextC) and currR.issubset(nextR):\n",
    "                        isSubset = True\n",
    "        \n",
    "            if isSubset or len(setsRows[i]) == 0:\n",
    "                setsColumns.pop(i)\n",
    "                setsRows.pop(i)\n",
    "                again = True\n",
    "                break\n",
    "    \n",
    "    #converting data type back to list\n",
    "    for i in range(len(setsColumns)):\n",
    "        setsColumns[i] = list(setsColumns[i])\n",
    "        setsRows[i] = list(setsRows[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort attributes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted att:  {'0': 100, '45': 45, '10': 10, '65': 9, '87': 2}\n"
     ]
    }
   ],
   "source": [
    "test_dict_freq = {'10':10,'45':45,'65':9,'87':2,'0':100}\n",
    "sorted_attributes = sort_attributes(test_dict_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_attributes(data_res):\n",
    "    \n",
    "    try:\n",
    "        ##usar este for caso o value seja uma lista\n",
    "        freq_res_dict = {}\n",
    "        for key,value in data_res.items():\n",
    "            freq_res_dict[key] = len(value)\n",
    "\n",
    "        # Create a list of tuples sorted by index 1 i.e. value field     \n",
    "        listofTuples = sorted(freq_res_dict.items() , reverse=True, key=lambda x: x[1])# usar se value for lista\n",
    "        # Iterate over the sorted sequence\n",
    "        # for elem in listofTuples :\n",
    "        #     print(elem[0] , \" ::\" , elem[1] )\n",
    "    #     print(listofTuples)\n",
    "        sorted_attributes = [elem[0] for elem in listofTuples]\n",
    "    except:\n",
    "        ## este é usado caso value seja um número\n",
    "        sorted_attributes = {k: v for k, v in sorted(data_res.items(), reverse=True, key=lambda item: item[1])}\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(\"Sorted att: \",sorted_attributes)\n",
    "    return sorted_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update residual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_residual_dataset(res_data, attributes_cocluster, objects_cocluster):\n",
    "    for key, value in res_data.items():\n",
    "        if key in attributes_cocluster:\n",
    "            diff_objs = set(res_data[key]).difference(set(objects_cocluster))\n",
    "            res_data[key] = list(diff_objs)\n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results - check path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path_method):\n",
    "    current_dir = os.getcwd()\n",
    "    print(current_dir)\n",
    "    res = os.path.exists(path_method)\n",
    "    # clean the folder to save new data\n",
    "    if res:\n",
    "        #check if it is empty\n",
    "        dir_empty = os.listdir(path_method)\n",
    "        if len(dir_empty) != 0:\n",
    "    #         shutil.rmtree(\"OutputAnalysis/kmeans/\")\n",
    "            rm = !rm -r --preserve-root './OutputAnalysis/ococlus/'*\n",
    "            if not rm:\n",
    "                print(\"OCoClus' folder was cleaned.\")\n",
    "    #             os.chdir(path_method)\n",
    "            else:\n",
    "                print(\"sad\")\n",
    "                print(rm)\n",
    "        else:\n",
    "    #         print(\"Empty!\")\n",
    "            pass\n",
    "    #         os.chdir(path_method)\n",
    "    else: # nothing exist so create it\n",
    "        # trying to insert to flase directory \n",
    "        try: \n",
    "    #         os.chdir(fd) \n",
    "            os.mkdir(path_method)\n",
    "            print(\"The path was created: \"+path_method)\n",
    "\n",
    "        # Caching the exception     \n",
    "        except: \n",
    "            print(\"Something wrong with specified directory. Exception- \", sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save clustering result into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def writeFileOutput(cols, rows, dataset, method='OCoClus', fileName='OCoClusResult'):\n",
    "def writeFileOutput(co_clusters, dataset, method='OCoClus', fileName='OCoClusResult'):\n",
    "    text = \"\"\n",
    "#    for c in range(len(data.rows_)):\n",
    "#        res = [i for i, val in enumerate(data.columns_[c]) if val]\n",
    "#        for j in res:\n",
    "#            text += str(j)+\" \"\n",
    "\n",
    "#        res = [i for i, val in enumerate(data.rows_[c]) if val]\n",
    "#        text += \"[\"\n",
    "#        for j in res:\n",
    "#            text += str(j)+\" \"\n",
    "#        text += \"]\\n\"\n",
    "    \n",
    "    num_of_clusters = len(co_clusters)\n",
    "    \n",
    "#     for c in range(len(cols)):\n",
    "    for c in range(num_of_clusters):\n",
    "#         for i in cols[c]:\n",
    "        for i in co_clusters[c][0]: # get the attributes in cluster c\n",
    "            text += str(i)+\" \"\n",
    "        \n",
    "        text += \"(\"+str(len(co_clusters[c][1]))+\") [\" # get the number of objects in clusters c\n",
    "        for j in range(len(co_clusters[c][1])): # save in the file each obj\n",
    "            if j+1 != len(co_clusters[c][1]):\n",
    "                text += str(co_clusters[c][1][j])+\" \"\n",
    "            else:\n",
    "                text += str(co_clusters[c][1][j])\n",
    "        text += \"]\\n\"\n",
    "    \n",
    "    #print(text)\n",
    "    if method == 'Dhillon':\n",
    "        f = open('./datasets/outputs/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./datasets/outputs/\"+fileName+\".txt\")\n",
    "    elif method == 'Kluger':\n",
    "        f = open('./datasets/outputs/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./datasets/outputs/\"+fileName+\".txt\")\n",
    "    elif method == 'OCoClus':\n",
    "        f = open('./OutputAnalysis/ococlus/'+dataset+'/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./OutputAnalysis/ococlus/\"+dataset+\"/\"+fileName+\".txt\")\n",
    "    else:\n",
    "        print(\"The output file was not generated. Method option not recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rec_error(data,clusters):\n",
    "    '''\n",
    "    This evaluation measure is computed during the algorithm life time.\n",
    "    '''\n",
    "    reconstructed_ococlus = np.zeros(data.shape,dtype=int)\n",
    "    for nc in range(len(clusters)):\n",
    "        for i in clusters[nc][1]: # object cluster\n",
    "            for j in clusters[nc][0]: # attribute cluster\n",
    "                reconstructed_ococlus[int(i)][int(j)] = 1\n",
    "    print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_ococlus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omega format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clustering_output_omega(co_clusters):\n",
    "# def build_clustering_output_omega(rowClusters,columnClusters):\n",
    "    '''\n",
    "    Build the clustering output format to use in the omega index evaluation from Remy Cazabet version.\n",
    "    It is optional and we just present this version as a complementary information. If you are interested,\n",
    "    check it out on his team work group at https://github.com/isaranto/omega_index.\n",
    "    '''\n",
    "    \n",
    "    num_of_clusters = len(co_clusters)    \n",
    "    clustering = {}\n",
    "    \n",
    "    for nc in range(num_of_clusters):\n",
    "        rowCluster = co_clusters[nc][1]\n",
    "        columnCluster = co_clusters[nc][0]\n",
    "        clustering[\"c\"+str(nc)] = []\n",
    "        \n",
    "        for i in rowCluster:\n",
    "            for j in columnCluster:\n",
    "                clustering[\"c\"+str(nc)].append((\"01\"+str(i)+\"02\"+str(j)))\n",
    "        \n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eXascale Infolab \n",
    "We used the xmeasure and OvpNMI project that pushished evaluation measures for overlapping task. We can check it on https://github.com/eXascaleInfolab/xmeasures or https://exascale.info/. Look their project on github to know how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmeasures_format(dict_gt):\n",
    "    '''\n",
    "    This function build the xmeasure format to use it on their evaluation measure.\n",
    "    '''\n",
    "    newData = []\n",
    "    for i in range(len(dict_gt)):\n",
    "#         print(dict_gt['c'+str(i)])\n",
    "        stringLine = dict_gt['c'+str(i)][0]\n",
    "        for j in range(1,len(dict_gt['c'+str(i)])):\n",
    "#             stringLine = stringLine+\" \"+dict_gt['c'+str(i)][j]\n",
    "            stringLine += \" \"+dict_gt['c'+str(i)][j]\n",
    "        newData.append(stringLine)\n",
    "    \n",
    "    return newData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
