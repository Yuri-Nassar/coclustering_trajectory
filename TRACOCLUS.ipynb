{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "from timeit import default_timer as timer\n",
    "import datetime\n",
    "import sys, os, shutil, gc\n",
    "from csv import DictWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Version of the packages in this work.'''\n",
    "# # print(\"Pandas version: \",pd.__version__)\n",
    "# print(\"Pandas version: 0.25.1\")\n",
    "# # print(\"Numpy version: \",np.__version__)\n",
    "# print(\"Numpy version: 1.16.4\")\n",
    "# # print(\"Sys python version: \",sys.version)\n",
    "# print(\"Sys python version: 3.7.1 | package by conda-forge [MSC v.1900 64 bit (AMD64)]\")\n",
    "# print(\"IPython: 7.8.0\")\n",
    "# print(\"IPython genutils: 0.2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#toy example dataset\n",
    "# input_test2.dat and input_test2.dat\n",
    "\n",
    "#synthetic datasets\n",
    "# synthetic-1_fimi.dat, synthetic-2_fimi.dat and synthetic-3_fimi.dat\n",
    "\n",
    "experiment = 'Toy' # Syn: run the synthetic data analysis; Real: run the real data analysis; \n",
    "                   # Toy: run the toy example.\n",
    "toy_number = \"2\" # sufix of a given toy dataset [1,1-1,2]\n",
    "\n",
    "find_overlap = False # True: find overlapped co-clusters; False: Find no overlapped co-clusters\n",
    "VERBOSE = True # True: print results as a verbose mode; False: print the main results\n",
    "num_of_sim = 1 # number of simulations to perform\n",
    "k = -1 #max number of co-cluster that could be found. -1: default [driven by cost function]\n",
    "e_obj = 0.4 # maximum error tolerance for object. -1: default [accept the maximum error]\n",
    "e_att = .8 # maximum error tolerance for attribute. -1: default [accept the maximum error]\n",
    "\n",
    "if VERBOSE: print('--->Verbose mode ON.<---')\n",
    "\n",
    "print(\"Executing TRACOCLUS method\")\n",
    "if experiment == 'Syn':\n",
    "    path = \"./data/synthetic/fimi/\"\n",
    "    syn_datasets = [path+\"synthetic-1_fimi.dat\",path+\"synthetic-2_fimi.dat\",path+\"synthetic-3_fimi.dat\"]\n",
    "    path_method = \"OutputAnalysis\\ococlus\"\n",
    "    check_path(path_method)\n",
    "\n",
    "    for ds in range(len(syn_datasets)):\n",
    "        ds_name = \"Syn-\"+str(ds+1)\n",
    "        print(\"\\nDataset: \"+ds_name)\n",
    "        res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "\n",
    "        for run in range(num_of_sim):\n",
    "            print(\"Run-\"+str(run+1))\n",
    "\n",
    "            df_fimi = pd.read_csv(syn_datasets[ds], header=None, names=[\"transation\"])\n",
    "            D,co_clusters = OCoClus(df_fimi,k,e_obj,e_att) # Calling OCoClus main method\n",
    "\n",
    "            print(\"\")\n",
    "            Rec_error(D,co_clusters)\n",
    "#             print(co_clusters)\n",
    "            \n",
    "            omega_format = build_clustering_output_omega(co_clusters)\n",
    "            OCoClus_clustering_xm = xmeasures_format(omega_format)# save to XMEASURES format C++ version\n",
    "            df_gt = pd.DataFrame(OCoClus_clustering_xm)\n",
    "            path = path_method+\"/\"+ds_name\n",
    "            df_gt.to_csv(path.replace(\"\\\\\",\"/\")+\"/run_\"+str(run+1)+\"_res_ococlus_\"+ds_name+\"_co.cnl\", \n",
    "                         header= False,index=False, encoding='utf8')\n",
    "            del omega_format, df_gt, OCoClus_clustering_xm\n",
    "            gc.collect()\n",
    "elif experiment == 'Real':\n",
    "    k = 10\n",
    "    print('Real data clustering.')\n",
    "    path = \"./data/real_application/\"\n",
    "    real_datasets = [path+\"cal500_fimi.dat\",path+\"covid19_fimi.dat\"]\n",
    "    path_method = \"OutputAnalysis\\ococlus\"\n",
    "    check_path(path_method)\n",
    "    \n",
    "    for ds in range(len(real_datasets)):\n",
    "        ds_name = real_datasets[ds].replace(\"/\",\"_\").split(\"_\")[4].capitalize()\n",
    "        print(\"\\nDataset: \"+ds_name)\n",
    "        res = os.mkdir(path_method+\"\\\\\"+ds_name)\n",
    "        \n",
    "        df_fimi = pd.read_csv(real_datasets[ds], header=None, names=[\"transation\"])\n",
    "        D,co_clusters = OCoClus(df_fimi,k,e_obj,e_att) # Calling OCoClus main method\n",
    "        writeFileOutput(co_clusters,ds_name,method='OCoClus',fileName='OCoClusResult_'+ds_name)\n",
    "        \n",
    "    print('DONE!')\n",
    "elif experiment == 'Toy':\n",
    "    print('Toy example')\n",
    "    VERBOSE = False\n",
    "#     input_data_pd = pd.read_csv(\"./data/toy_example/toy\"+toy_number+\"_traj.dat\", header=None, names=[\"transation\"])\n",
    "#     input_data_pd = pd.read_csv(\"./data/toy_example/toy\"+toy_number+\"_traj.dat\", header=None, names=[\"transation\"])\n",
    "    input_data_pd = pd.read_csv(\"./data/real_application/foursquare_NY/fs_ny_week_sequences.dat\", header=None, names=[\"transation\"])\n",
    "#     print(input_data_pd)\n",
    "    D,co_clusters = TRACOCLUS(input_data_pd,k,e_obj,e_att)\n",
    "    print(co_clusters)\n",
    "    #Compute the measures\n",
    "#     print(\"\")\n",
    "#     Rec_error(D,co_clusters)\n",
    "else:\n",
    "    print('ERROR! Choose a valid option for the experiment analysis.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TraCoClus algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main algorithm of TraCoClus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = '0-3-1-4-0-1'\n",
    "er2 = er.split('-')\n",
    "print(er2)\n",
    "# print(er2.nunique())\n",
    "def change_vect_cont(queue):\n",
    "    queue[0] = 'Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vect = [1,2,8,4]\n",
    "change_vect_cont(my_vect)\n",
    "print(my_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tested_atts = np.log2(10)\n",
    "vec_log_num = [1,10,20,30,40,50,60,70,80,90,100]#,250,500]#,750,1000]#,1500,2000,3000,5000,7500,10000]\n",
    "base = [2,10]\n",
    "df_log_base_diff = pd.DataFrame(columns = ['Input x','Log Result','Log Base'])\n",
    "popupalte_df_dict = {'Input x':0,'Log Result':0,'Log Base':''}\n",
    "\n",
    "for base_i in base:\n",
    "    popupalte_df_dict['Log Base'] = 'Log_'+str(base_i)\n",
    "    for num in vec_log_num:\n",
    "        popupalte_df_dict['Input x'] = num\n",
    "#         if base_i == 2:\n",
    "#         popupalte_df_dict['Log Result'] = np.log2(num)    \n",
    "        popupalte_df_dict['Log Result'] = np.log(num)/np.log(base_i)\n",
    "#         else:\n",
    "#             popupalte_df_dict['Log Result'] = np.log10(num)\n",
    "        df_log_base_diff = df_log_base_diff.append(popupalte_df_dict, ignore_index=True)\n",
    "df_log_base_diff.head()\n",
    "sns.lineplot(data=df_log_base_diff, x=\"Input x\", y=\"Log Result\", hue=\"Log Base\", style=\"Log Base\",\n",
    "             markers=True, dashes=False)\n",
    "# plt.show()\n",
    "# print(max_tested_atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1,'b':2,'c':3,'d':4}\n",
    "list_values = a.values()\n",
    "print(np.mean(list(list_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run simulation: 1\n",
      "sjgs2.dat\n",
      "['sjgs2', 'dat']\n",
      "######################################\n",
      "Number of trajectories: 3189\n",
      "Number of unique check-ins: 77\n",
      "########################################\n",
      "Original number of elements:  77\n",
      "Element analysis: False. We consider the original number of elements:  77\n",
      "Tirando a duvida sobre o tamanho da lista de elementos:  77\n",
      "Searching for candidate: 1408\n",
      "Total clustering time in minutes:  8.08\n",
      "Total clustering time:  0h:8m:4s (9/12/2021 - 23:53:16)\n",
      "\n",
      "### Clustering statistics ###\n",
      "Number of co-clusters:  24\n",
      "AVG rows:68.50±19.65[CV:28.68%], AVG cost:-51.96±24.60[CV:-47.34%]\n",
      "AVG sequece length:2.00±0.00[CV:0.00%]\n",
      "Number of unique elements grouped: 18\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = False\n",
    "k = -1 #max number of co-cluster that could be found. -1: default [driven by cost function]\n",
    "e_obj = 0 # maximum error tolerance for object. -1: default [accept the maximum error]\n",
    "e_att = 0\n",
    "\n",
    "cc_prune_ref = 'rows' # 1. 'rows'; 2. 'cost'; 3. 'combine'\n",
    "cc_ref_list = ['rows','cost','combine']\n",
    "cc_type_process = 'sample' # 1. incremental [evaluate the candidate during the process]; 2. sample [consider the set of candidates]\n",
    "cc_type_analysis = 'z_score' # # 1. mean; 2. z_score\n",
    "metric_stat_list = ['mean','z_score']\n",
    "#Rows and cost are inverse among them.\n",
    "#Rows grow positively while cost negatively.\n",
    "cc_z_threshold = .1 # general z_thres; the rows are the reference, i.e., rows_z = -1 and cost_z = 1 (other side)\n",
    "cc_z_threshold_r = 3 #e.g., -1, we want the right-side, so we keep values equal or bigger than 1 sd bellow the avg\n",
    "cc_z_threshold_c = -3 #e.g., 1, we want the left-side, so we keep values equal or smaller than 1 sd above the avg\n",
    "# cc_z_threshold\n",
    "# z_thres_list = [-1,0,1]\n",
    "z_thres_list = [-.5,0,.5]\n",
    "\n",
    "# input_data_pd = pd.read_csv(\"./data/toy_example/toy2_traj.dat\", header=None, names=[\"transation\"])\n",
    "# input_data_pd = pd.read_csv(\"./data/real_application/foursquare_NY/fs_ny_week_sequences.dat\", header=None, names=[\"transation\"])\n",
    "# input_data_pd = pd.read_csv(\"./data/real_application/foursquare_NY/preprocessed/fs_ny_top_10_week_sequences.dat\", header=None, names=[\"transation\"])\n",
    "\n",
    "# input_data_pd = './data/toy_example/toy2_traj.dat'\n",
    "# input_data_pd = './data/real_application/foursquare_NY/fs_ny_week_sequences.dat'\n",
    "# input_data_pd = './data/real_application/foursquare_NY/preprocessed/fs_ny_top_10_week_sequences.dat'\n",
    "# path = './data/real_application/foursquare_NY/preprocessed/'\n",
    "datasets = ['fs_ny_top_users_10.dat','fs_ny_top_users_81.dat','fs_ny_top_users_193.dat','sjgs.dat','sjgs2.dat']\n",
    "SCABILITY = False\n",
    "STORE_CLUS_STATS = False\n",
    "element_analysis = False # True: get elements with freq >= AVG ; False: consider all number of elements\n",
    "# scability_els_list = [13,28,43,58,73,88,93,108,123,138]\n",
    "# scability_els_list_rstart = [138]\n",
    "# scability_els_list = [58]\n",
    "# scability_els_list = [88]\n",
    "scability_els_list = [93]\n",
    "clustering_perf = Performance()\n",
    "# D,co_clusters = TRACOCLUS(input_data_pd,cc_prune_ref,k,e_obj,e_att)\n",
    "# for i in range(1,len(datasets)):\n",
    "\n",
    "# for i in range(1,3):#datasets\n",
    "for i in range(4,5):# reiniciar do dataset ...\n",
    "#     for run_sim in range(5):#run simulations\n",
    "    for run_sim in range(0,1):# reiniciar da run simulations #....\n",
    "        print('\\nRun simulation: '+str(run_sim+1))\n",
    "        for num_of_els in scability_els_list:\n",
    "#         for num_of_els in scability_els_list_rstart:# reiniciar a partir do element ...\n",
    "            print(datasets[i])\n",
    "        #     for cc_prune_ref in cc_ref_list:\n",
    "            D,co_clusters = TRACOCLUS(datasets[i],cc_prune_ref,k,e_obj,e_att)\n",
    "#     clustering_perf.plot_scability_test('scability_num_of_els_users_d'+str(i))\n",
    "#     clustering_perf.plot_scability_rows_cost('scability_num_of_els_users_d'+str(i))\n",
    "#     for j in range(1,len(cc_ref_list)):\n",
    "#         cc_prune_ref = cc_ref_list[j]\n",
    "#         if cc_type_analysis == 'mean':\n",
    "#             D,co_clusters = TRACOCLUS(datasets[i],cc_prune_ref,k,e_obj,e_att)\n",
    "#         else:\n",
    "#             for cc_z_threshold in z_thres_list:\n",
    "#                 D,co_clusters = TRACOCLUS(datasets[i],cc_prune_ref,k,e_obj,e_att)\n",
    "# TRACOCLUS(input_data_pd,k,e_obj,e_att)\n",
    "# print('\\nNumber of found co-clusters: ',len(co_clusters))\n",
    "# print('Final co-clusters: ',co_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering_perf.df_scability.head()\n",
    "# clustering_perf.df_scab_rows_cost.head()\n",
    "# clustering_perf.df_scab_rows_cost.info()\n",
    "# df = pd.read_csv('coclustering_file_outputs/df_scability.csv', sep=\";\")\n",
    "# df = pd.read_csv('coclustering_file_outputs/df_scab_rows_cost.csv', sep=\";\")\n",
    "# df = pd.read_csv('coclustering_file_outputs/df_clustering_stats.csv', encoding='utf8' , sep=';')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splice-junction gene sequence\n",
    "\n",
    "k:4 ; z:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #1 - Candidate 0 [Absolute:2152 | Relative:0.67 | Cost: -4301 | Ov_coef: 0.00 | Seq length: 3]\n",
      "Attributes sequence \"G-C-T\" and trajectories \"'1779', '2606', '587', '240', '1392', '1149', '753', '2516',[...]\".\n",
      "\n",
      "Cluster #2 - Candidate 2 [Absolute:1987 | Relative:0.62 | Cost: -2733 | Ov_coef: 0.09 | Seq length: 3]\n",
      "Attributes sequence \"T-C-A\" and trajectories \"'1187', '680', '1656', '1392', '2505', '2916', '2752', '1149',[...]\".\n",
      "\n",
      "Cluster #3 - Candidate 3 [Absolute:1720 | Relative:0.54 | Cost: -1780 | Ov_coef: 0.10 | Seq length: 3]\n",
      "Attributes sequence \"A-G-T\" and trajectories \"'2606', '680', '587', '2916', '2505', '2752', '2879', '1149',[...]\".\n",
      "\n",
      "Cluster #4 - Candidate 4 [Absolute:1906 | Relative:0.60 | Cost: -1070 | Ov_coef: 0.11 | Seq length: 3]\n",
      "Attributes sequence \"A-T-G\" and trajectories \"'1779', '1187', '680', '240', '1392', '2505', '1656', '2916',[...]\".\n",
      "\n",
      "Trajectory \"50\" belongs to User: \"EI\"\n",
      "Max sequence length: 3\n",
      "Levels: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5']\n",
      "Columns alluvial df: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5', 'count', 'cluster']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clustering_perf.test_norm_dist()\n",
    "# clustering_perf.test_skewness()\n",
    "# df = pd.read_csv('data/real_application/foursquare_NY/fs_ny_top_users_10.csv', sep=\";\")\n",
    "# clustering_perf.analysis_entropy_purity(create_df_map_traj_user(df))\n",
    "clustering_perf.get_clusters()\n",
    "clustering_perf.create_alluvial_diagram()\n",
    "# clustering_perf.show_boxplot()\n",
    "# clustering_perf.plot_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splice-junction gene sequence\n",
    "\n",
    "k:3 ; z:0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #1 - Candidate 0 [Absolute:2152 | Relative:0.67 | Cost: -4301 | Ov_coef: 0.00 | Seq length: 3]\n",
      "Attributes sequence \"G-C-T\" and trajectories \"'1779', '2606', '587', '240', '1392', '1149', '753', '2516',[...]\".\n",
      "\n",
      "Cluster #2 - Candidate 2 [Absolute:1987 | Relative:0.62 | Cost: -2733 | Ov_coef: 0.09 | Seq length: 3]\n",
      "Attributes sequence \"T-C-A\" and trajectories \"'1187', '680', '1656', '1392', '2505', '2916', '2752', '1149',[...]\".\n",
      "\n",
      "Cluster #3 - Candidate 4 [Absolute:1906 | Relative:0.60 | Cost: -1070 | Ov_coef: 0.11 | Seq length: 3]\n",
      "Attributes sequence \"A-T-G\" and trajectories \"'1779', '1187', '680', '240', '1392', '2505', '1656', '2916',[...]\".\n",
      "\n",
      "Trajectory \"50\" belongs to User: \"EI\"\n",
      "Max sequence length: 3\n",
      "Levels: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5']\n",
      "Columns alluvial df: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5', 'count', 'cluster']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering_perf.get_clusters()\n",
    "clustering_perf.create_alluvial_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splice-junction gene sequence\n",
    "\n",
    "k:6 ; z:-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #1 - Candidate 0 [Absolute:2152 | Relative:0.67 | Cost: -4301 | Ov_coef: 0.00 | Seq length: 3]\n",
      "Attributes sequence \"G-C-T\" and trajectories \"'1779', '2606', '587', '240', '1392', '1149', '753', '2516',[...]\".\n",
      "\n",
      "Cluster #2 - Candidate 1 [Absolute:847 | Relative:0.27 | Cost: -1286 | Ov_coef: 0.09 | Seq length: 3]\n",
      "Attributes sequence \"A-C-G\" and trajectories \"'2474', '2384', '2606', '2916', '1392', '2505', '1000', '753',[...]\".\n",
      "\n",
      "Cluster #3 - Candidate 2 [Absolute:1987 | Relative:0.62 | Cost: -2733 | Ov_coef: 0.09 | Seq length: 3]\n",
      "Attributes sequence \"T-C-A\" and trajectories \"'1187', '680', '1656', '1392', '2505', '2916', '2752', '1149',[...]\".\n",
      "\n",
      "Cluster #4 - Candidate 3 [Absolute:1720 | Relative:0.54 | Cost: -1780 | Ov_coef: 0.10 | Seq length: 3]\n",
      "Attributes sequence \"A-G-T\" and trajectories \"'2606', '680', '587', '2916', '2505', '2752', '2879', '1149',[...]\".\n",
      "\n",
      "Cluster #5 - Candidate 4 [Absolute:1906 | Relative:0.60 | Cost: -1070 | Ov_coef: 0.11 | Seq length: 3]\n",
      "Attributes sequence \"A-T-G\" and trajectories \"'1779', '1187', '680', '240', '1392', '2505', '1656', '2916',[...]\".\n",
      "\n",
      "Cluster #6 - Candidate 5 [Absolute:3 | Relative:0.00 | Cost: -1 | Ov_coef: 0.00 | Seq length: 2]\n",
      "Attributes sequence \"N-C\" and trajectories \"'365', '1804', '485'\".\n",
      "\n",
      "Trajectory \"50\" belongs to User: \"EI\"\n",
      "Max sequence length: 3\n",
      "Levels: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5']\n",
      "Columns alluvial df: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5', 'count', 'cluster']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering_perf.get_clusters()\n",
    "clustering_perf.create_alluvial_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splice-junction gene sequence 2\n",
    "\n",
    "k:177 ; z:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #1 - Candidate 0 [Absolute:119 | Relative:0.04 | Cost: -117 | Ov_coef: 0.00 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CAG\" and trajectories \"'1751', '978', '986', '1392', '987', '2650', '1387', '1251',[...]\".\n",
      "\n",
      "Cluster #2 - Candidate 1 [Absolute:71 | Relative:0.02 | Cost: -62 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CTG\" and trajectories \"'1751', '1580', '2983', '2779', '2695', '1421', '2672', '863',[...]\".\n",
      "\n",
      "Cluster #3 - Candidate 2 [Absolute:93 | Relative:0.03 | Cost: -86 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CAG\" and trajectories \"'3002', '978', '2983', '725', '987', '1108', '1388', '1279',[...]\".\n",
      "\n",
      "Cluster #4 - Candidate 3 [Absolute:63 | Relative:0.02 | Cost: -59 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"CCT-CAG\" and trajectories \"'1585', '3126', '895', '153', '2713', '956', '892', '2227',[...]\".\n",
      "\n",
      "Cluster #5 - Candidate 4 [Absolute:133 | Relative:0.04 | Cost: -115 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GTG\" and trajectories \"'1285', '690', '110', '241', '1500', '2814', '675', '1310',[...]\".\n",
      "\n",
      "Cluster #6 - Candidate 5 [Absolute:43 | Relative:0.01 | Cost: -37 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CCC\" and trajectories \"'86', '1751', '113', '1736', '3000', '1214', '2761', '932',[...]\".\n",
      "\n",
      "Cluster #7 - Candidate 6 [Absolute:43 | Relative:0.01 | Cost: -33 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CCT\" and trajectories \"'2838', '126', '129', '1319', '2071', '1929', '1048', '1221',[...]\".\n",
      "\n",
      "Cluster #8 - Candidate 7 [Absolute:53 | Relative:0.02 | Cost: -22 | Ov_coef: 0.15 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CTG\" and trajectories \"'1751', '1118', '1324', '1214', '1493', '932', '882', '1106',[...]\".\n",
      "\n",
      "Cluster #9 - Candidate 8 [Absolute:70 | Relative:0.02 | Cost: -62 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CTC\" and trajectories \"'1520', '1006', '1516', '1381', '1524', '229', '1935', '452',[...]\".\n",
      "\n",
      "Cluster #10 - Candidate 9 [Absolute:44 | Relative:0.01 | Cost: -37 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CCT\" and trajectories \"'766', '2502', '597', '455', '932', '1433', '1048', '2678',[...]\".\n",
      "\n",
      "Cluster #11 - Candidate 10 [Absolute:38 | Relative:0.01 | Cost: -24 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CCC\" and trajectories \"'1258', '2983', '2919', '1370', '1174', '1394', '3054', '1337',[...]\".\n",
      "\n",
      "Cluster #12 - Candidate 11 [Absolute:76 | Relative:0.02 | Cost: -63 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"CTG-GTG\" and trajectories \"'548', '1520', '1516', '1524', '1500', '438', '1514', '148',[...]\".\n",
      "\n",
      "Cluster #13 - Candidate 12 [Absolute:76 | Relative:0.02 | Cost: -47 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GAG\" and trajectories \"'978', '986', '1404', '987', '1333', '1812', '3165', '1262',[...]\".\n",
      "\n",
      "Cluster #14 - Candidate 13 [Absolute:43 | Relative:0.01 | Cost: -20 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"CCT-CTG\" and trajectories \"'986', '2300', '161', '1387', '1108', '455', '2137', '3066',[...]\".\n",
      "\n",
      "Cluster #15 - Candidate 14 [Absolute:71 | Relative:0.02 | Cost: -46 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CTC-CAG\" and trajectories \"'2535', '229', '1196', '933', '3090', '1421', '1772', '546',[...]\".\n",
      "\n",
      "Cluster #16 - Candidate 16 [Absolute:55 | Relative:0.02 | Cost: -47 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"AGG-CTG\" and trajectories \"'163', '1585', '518', '1483', '475', '157', '1119', '744',[...]\".\n",
      "\n",
      "Cluster #17 - Candidate 17 [Absolute:48 | Relative:0.02 | Cost: -37 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"GGG-CAG\" and trajectories \"'2983', '1815', '729', '583', '695', '685', '2651', '739',[...]\".\n",
      "\n",
      "Cluster #18 - Candidate 18 [Absolute:57 | Relative:0.02 | Cost: -25 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CCT-CCC\" and trajectories \"'3030', '1754', '1075', '1688', '1388', '1319', '1855', '2892',[...]\".\n",
      "\n",
      "Cluster #19 - Candidate 19 [Absolute:30 | Relative:0.01 | Cost: -24 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"CTG-TCT\" and trajectories \"'946', '1019', '1892', '2508', '1964', '3027', '2225', '264',[...]\".\n",
      "\n",
      "Cluster #20 - Candidate 20 [Absolute:58 | Relative:0.02 | Cost: -40 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CAG\" and trajectories \"'825', '1697', '992', '1929', '826', '1449', '1346', '1310',[...]\".\n",
      "\n",
      "Cluster #21 - Candidate 21 [Absolute:62 | Relative:0.02 | Cost: -58 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"GAG-GTG\" and trajectories \"'50', '657', '1611', '190', '2392', '2249', '179', '124',[...]\".\n",
      "\n",
      "Cluster #22 - Candidate 22 [Absolute:40 | Relative:0.01 | Cost: -33 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"CCC-AAG\" and trajectories \"'1313', '3000', '149', '2236', '377', '2950', '2585', '375',[...]\".\n",
      "\n",
      "Cluster #23 - Candidate 23 [Absolute:42 | Relative:0.01 | Cost: -34 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CCA\" and trajectories \"'1258', '2115', '1393', '438', '820', '2592', '1068', '384',[...]\".\n",
      "\n",
      "Cluster #24 - Candidate 24 [Absolute:49 | Relative:0.02 | Cost: -34 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CTC\" and trajectories \"'1118', '2535', '3181', '3129', '2567', '1595', '1106', '1004',[...]\".\n",
      "\n",
      "Cluster #25 - Candidate 25 [Absolute:36 | Relative:0.01 | Cost: -33 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"GGG-TGG\" and trajectories \"'163', '1926', '1555', '30', '147', '563', '95', '116',[...]\".\n",
      "\n",
      "Cluster #26 - Candidate 26 [Absolute:42 | Relative:0.01 | Cost: -39 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"GGA-AGG\" and trajectories \"'1939', '452', '2882', '2953', '834', '716', '531', '3028',[...]\".\n",
      "\n",
      "Cluster #27 - Candidate 27 [Absolute:54 | Relative:0.02 | Cost: -49 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"CCT-GCC\" and trajectories \"'3030', '2011', '956', '3179', '132', '1259', '3066', '820',[...]\".\n",
      "\n",
      "Cluster #28 - Candidate 29 [Absolute:56 | Relative:0.02 | Cost: -49 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TTT-TCT\" and trajectories \"'1392', '2287', '1128', '588', '2227', '1563', '2763', '1913',[...]\".\n",
      "\n",
      "Cluster #29 - Candidate 30 [Absolute:62 | Relative:0.02 | Cost: -37 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"CCA-CAG\" and trajectories \"'1285', '3037', '1258', '810', '1393', '194', '932', '882',[...]\".\n",
      "\n",
      "Cluster #30 - Candidate 31 [Absolute:35 | Relative:0.01 | Cost: -24 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TGG-CTG\" and trajectories \"'112', '241', '124', '2150', '101', '91', '122', '1957',[...]\".\n",
      "\n",
      "Cluster #31 - Candidate 32 [Absolute:39 | Relative:0.01 | Cost: -36 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"GGG-GTG\" and trajectories \"'335', '149', '663', '71', '3045', '1936', '481', '327',[...]\".\n",
      "\n",
      "Cluster #32 - Candidate 35 [Absolute:31 | Relative:0.01 | Cost: -12 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CCT\" and trajectories \"'2921', '2660', '1314', '870', '1387', '2374', '853', '3027',[...]\".\n",
      "\n",
      "Cluster #33 - Candidate 40 [Absolute:43 | Relative:0.01 | Cost: -28 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"AGG-AAG\" and trajectories \"'1320', '1324', '1579', '375', '438', '2990', '2563', '1329',[...]\".\n",
      "\n",
      "Cluster #34 - Candidate 42 [Absolute:46 | Relative:0.01 | Cost: -23 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"GTG-GAG\" and trajectories \"'548', '2650', '1555', '518', '1611', '1397', '933', '179',[...]\".\n",
      "\n",
      "Cluster #35 - Candidate 45 [Absolute:57 | Relative:0.02 | Cost: -42 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CCT\" and trajectories \"'3037', '1313', '927', '1550', '1153', '1493', '244', '1476',[...]\".\n",
      "\n",
      "Cluster #36 - Candidate 46 [Absolute:34 | Relative:0.01 | Cost: -32 | Ov_coef: 0.00 | Seq length: 2]\n",
      "Attributes sequence \"CCA-GGC\" and trajectories \"'2300', '2296', '1251', '540', '3054', '3008', '590', '2865',[...]\".\n",
      "\n",
      "Cluster #37 - Candidate 47 [Absolute:53 | Relative:0.02 | Cost: -43 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TCC-CAG\" and trajectories \"'86', '766', '1075', '452', '1397', '1295', '1284', '2055',[...]\".\n",
      "\n",
      "Cluster #38 - Candidate 50 [Absolute:42 | Relative:0.01 | Cost: -28 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CTG-GGA\" and trajectories \"'1516', '1524', '1675', '260', '3154', '893', '2731', '3167',[...]\".\n",
      "\n",
      "Cluster #39 - Candidate 55 [Absolute:30 | Relative:0.01 | Cost: -24 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"TTC-AGG\" and trajectories \"'308', '750', '299', '2377', '1679', '2533', '2222', '2404',[...]\".\n",
      "\n",
      "Cluster #40 - Candidate 59 [Absolute:46 | Relative:0.01 | Cost: -40 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CTC\" and trajectories \"'825', '1754', '1027', '1963', '1443', '2723', '3142', '218',[...]\".\n",
      "\n",
      "Cluster #41 - Candidate 60 [Absolute:57 | Relative:0.02 | Cost: -49 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GGG-AAG\" and trajectories \"'743', '2338', '671', '1837', '1926', '227', '757', '3163',[...]\".\n",
      "\n",
      "Cluster #42 - Candidate 62 [Absolute:37 | Relative:0.01 | Cost: -10 | Ov_coef: 0.14 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CTG\" and trajectories \"'1363', '1392', '3000', '300', '882', '1004', '1457', '1337',[...]\".\n",
      "\n",
      "Cluster #43 - Candidate 63 [Absolute:43 | Relative:0.01 | Cost: -12 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"GAG-CAG\" and trajectories \"'2862', '2650', '413', '149', '933', '301', '530', '1391',[...]\".\n",
      "\n",
      "Cluster #44 - Candidate 65 [Absolute:31 | Relative:0.01 | Cost: -25 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"GGC-AGG\" and trajectories \"'2223', '1697', '265', '518', '3050', '297', '2074', '1879',[...]\".\n",
      "\n",
      "Cluster #45 - Candidate 66 [Absolute:29 | Relative:0.01 | Cost: -20 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCT-TTC\" and trajectories \"'1313', '1320', '1550', '1742', '934', '815', '422', '419',[...]\".\n",
      "\n",
      "Cluster #46 - Candidate 69 [Absolute:54 | Relative:0.02 | Cost: -42 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GCT-GCC\" and trajectories \"'3126', '1313', '2146', '268', '2833', '3170', '1333', '124',[...]\".\n",
      "\n",
      "Cluster #47 - Candidate 70 [Absolute:34 | Relative:0.01 | Cost: -14 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"GGA-CCC\" and trajectories \"'2071', '3061', '1333', '882', '1344', '2673', '1355', '880',[...]\".\n",
      "\n",
      "Cluster #48 - Candidate 71 [Absolute:56 | Relative:0.02 | Cost: -50 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TCC-TGG\" and trajectories \"'1732', '2', '1108', '143', '2020', '590', '1273', '3095',[...]\".\n",
      "\n",
      "Cluster #49 - Candidate 74 [Absolute:34 | Relative:0.01 | Cost: -22 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"GGC-CCT\" and trajectories \"'1006', '412', '214', '894', '2888', '751', '1228', '488',[...]\".\n",
      "\n",
      "Cluster #50 - Candidate 75 [Absolute:44 | Relative:0.01 | Cost: -22 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"CTC-AGG\" and trajectories \"'1285', '133', '2170', '518', '475', '218', '501', '287',[...]\".\n",
      "\n",
      "Cluster #51 - Candidate 79 [Absolute:53 | Relative:0.02 | Cost: -14 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GGA\" and trajectories \"'1790', '690', '895', '1314', '929', '1370', '1397', '892',[...]\".\n",
      "\n",
      "Cluster #52 - Candidate 80 [Absolute:50 | Relative:0.02 | Cost: -7 | Ov_coef: 0.16 | Seq length: 2]\n",
      "Attributes sequence \"GTG-GGG\" and trajectories \"'1393', '30', '1936', '481', '558', '1310', '327', '517',[...]\".\n",
      "\n",
      "Cluster #53 - Candidate 85 [Absolute:55 | Relative:0.02 | Cost: -23 | Ov_coef: 0.14 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CCC\" and trajectories \"'86', '3002', '2899', '1108', '3027', '853', '1593', '2150',[...]\".\n",
      "\n",
      "Cluster #54 - Candidate 86 [Absolute:49 | Relative:0.02 | Cost: -36 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"GAG-AAG\" and trajectories \"'1166', '1022', '2115', '2249', '2880', '124', '31', '3025',[...]\".\n",
      "\n",
      "Cluster #55 - Candidate 88 [Absolute:49 | Relative:0.02 | Cost: -28 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GCT-CTG\" and trajectories \"'153', '712', '109', '2538', '1413', '512', '88', '1279',[...]\".\n",
      "\n",
      "Cluster #56 - Candidate 89 [Absolute:43 | Relative:0.01 | Cost: -25 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GCC-CCT\" and trajectories \"'161', '1075', '229', '2071', '132', '1284', '3066', '3009',[...]\".\n",
      "\n",
      "Cluster #57 - Candidate 92 [Absolute:29 | Relative:0.01 | Cost: -21 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCA-TCC\" and trajectories \"'1241', '2063', '1861', '1397', '995', '2572', '1284', '1452',[...]\".\n",
      "\n",
      "Cluster #58 - Candidate 95 [Absolute:32 | Relative:0.01 | Cost: -28 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"CTT-TTC\" and trajectories \"'1483', '1142', '1259', '1450', '1125', '394', '1003', '1074',[...]\".\n",
      "\n",
      "Cluster #59 - Candidate 97 [Absolute:32 | Relative:0.01 | Cost: -11 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"TGG-GGG\" and trajectories \"'1729', '143', '3179', '2020', '685', '864', '1632', '854',[...]\".\n",
      "\n",
      "Cluster #60 - Candidate 98 [Absolute:35 | Relative:0.01 | Cost: -15 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GTG-CAG\" and trajectories \"'2223', '2281', '2249', '1855', '3170', '1283', '69', '714',[...]\".\n",
      "\n",
      "Cluster #61 - Candidate 100 [Absolute:63 | Relative:0.02 | Cost: -45 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"AGG-AGA\" and trajectories \"'743', '757', '636', '3159', '377', '1062', '1573', '853',[...]\".\n",
      "\n",
      "Cluster #62 - Candidate 101 [Absolute:42 | Relative:0.01 | Cost: -29 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GGC-CTC\" and trajectories \"'1585', '766', '190', '1033', '2695', '1119', '2419', '260',[...]\".\n",
      "\n",
      "Cluster #63 - Candidate 102 [Absolute:29 | Relative:0.01 | Cost: -22 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCA-GAA\" and trajectories \"'1435', '123', '2332', '364', '1080', '1310', '2592', '1657',[...]\".\n",
      "\n",
      "Cluster #64 - Candidate 103 [Absolute:36 | Relative:0.01 | Cost: -26 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GCC-TCC\" and trajectories \"'2535', '2383', '1589', '1500', '147', '182', '2658', '2768',[...]\".\n",
      "\n",
      "Cluster #65 - Candidate 107 [Absolute:30 | Relative:0.01 | Cost: -24 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TTT-GCT\" and trajectories \"'2907', '2115', '835', '3027', '2763', '2352', '63', '1382',[...]\".\n",
      "\n",
      "Cluster #66 - Candidate 110 [Absolute:42 | Relative:0.01 | Cost: -26 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCC-TTC\" and trajectories \"'113', '1174', '2071', '2015', '1254', '92', '1986', '3184',[...]\".\n",
      "\n",
      "Cluster #67 - Candidate 113 [Absolute:66 | Relative:0.02 | Cost: -19 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"CCT-GGG\" and trajectories \"'743', '1580', '129', '2833', '259', '952', '2007', '225',[...]\".\n",
      "\n",
      "Cluster #68 - Candidate 116 [Absolute:31 | Relative:0.01 | Cost: -19 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"GAA-CAG\" and trajectories \"'2841', '1347', '2446', '297', '2430', '1908', '1703', '1204',[...]\".\n",
      "\n",
      "Cluster #69 - Candidate 117 [Absolute:35 | Relative:0.01 | Cost: -15 | Ov_coef: 0.17 | Seq length: 2]\n",
      "Attributes sequence \"CCA-TCT\" and trajectories \"'214', '1963', '2114', '899', '599', '904', '488', '837',[...]\".\n",
      "\n",
      "Cluster #70 - Candidate 122 [Absolute:36 | Relative:0.01 | Cost: -26 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GCC-CTC\" and trajectories \"'2624', '2877', '1295', '932', '40', '1355', '1463', '1812',[...]\".\n",
      "\n",
      "Cluster #71 - Candidate 124 [Absolute:53 | Relative:0.02 | Cost: -24 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"GGA-GGC\" and trajectories \"'1006', '2951', '2838', '1573', '1493', '2149', '2482', '3061',[...]\".\n",
      "\n",
      "Cluster #72 - Candidate 127 [Absolute:40 | Relative:0.01 | Cost: -30 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"ATG-AAG\" and trajectories \"'412', '475', '1020', '2011', '3066', '557', '2958', '612',[...]\".\n",
      "\n",
      "Cluster #73 - Candidate 129 [Absolute:47 | Relative:0.01 | Cost: -22 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CCC-TGC\" and trajectories \"'1836', '1108', '2670', '1365', '1986', '1468', '1234', '1337',[...]\".\n",
      "\n",
      "Cluster #74 - Candidate 132 [Absolute:35 | Relative:0.01 | Cost: -3 | Ov_coef: 0.11 | Seq length: 2]\n",
      "Attributes sequence \"CTG-GCT\" and trajectories \"'3126', '30', '241', '834', '1344', '1704', '296', '2675',[...]\".\n",
      "\n",
      "Cluster #75 - Candidate 148 [Absolute:38 | Relative:0.01 | Cost: -30 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GAA-AAG\" and trajectories \"'1285', '518', '1123', '475', '1742', '3162', '521', '884',[...]\".\n",
      "\n",
      "Cluster #76 - Candidate 149 [Absolute:41 | Relative:0.01 | Cost: -20 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GGG-GAG\" and trajectories \"'10', '30', '2591', '652', '1687', '202', '2979', '1909',[...]\".\n",
      "\n",
      "Cluster #77 - Candidate 153 [Absolute:46 | Relative:0.01 | Cost: -16 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GGC-CTG\" and trajectories \"'2919', '1855', '834', '505', '213', '454', '1825', '2158',[...]\".\n",
      "\n",
      "Cluster #78 - Candidate 154 [Absolute:29 | Relative:0.01 | Cost: -15 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"AGA-GCC\" and trajectories \"'2896', '1258', '1725', '810', '3159', '2071', '2880', '2419',[...]\".\n",
      "\n",
      "Cluster #79 - Candidate 158 [Absolute:33 | Relative:0.01 | Cost: -21 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CCC-TCT\" and trajectories \"'646', '2042', '1464', '3184', '2109', '526', '1377', '794',[...]\".\n",
      "\n",
      "Cluster #80 - Candidate 161 [Absolute:35 | Relative:0.01 | Cost: -17 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"AGG-CAG\" and trajectories \"'308', '2318', '1811', '1955', '751', '609', '184', '493',[...]\".\n",
      "\n",
      "Cluster #81 - Candidate 164 [Absolute:46 | Relative:0.01 | Cost: -21 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CCT\" and trajectories \"'3030', '927', '1320', '2713', '1196', '1929', '422', '419',[...]\".\n",
      "\n",
      "Cluster #82 - Candidate 170 [Absolute:39 | Relative:0.01 | Cost: -23 | Ov_coef: 0.24 | Seq length: 2]\n",
      "Attributes sequence \"CTT-CTC\" and trajectories \"'2132', '1476', '2339', '894', '1463', '330', '2684', '2866',[...]\".\n",
      "\n",
      "Cluster #83 - Candidate 171 [Absolute:37 | Relative:0.01 | Cost: -26 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GCT-GTG\" and trajectories \"'671', '2279', '2227', '3061', '812', '1403', '2224', '2601',[...]\".\n",
      "\n",
      "Cluster #84 - Candidate 176 [Absolute:35 | Relative:0.01 | Cost: -13 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TGG-CCA\" and trajectories \"'588', '1675', '988', '1272', '2339', '2114', '2974', '148',[...]\".\n",
      "\n",
      "Cluster #85 - Candidate 181 [Absolute:60 | Relative:0.02 | Cost: -15 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCC-AGG\" and trajectories \"'30', '2761', '306', '675', '558', '3127', '1279', '3079',[...]\".\n",
      "\n",
      "Cluster #86 - Candidate 186 [Absolute:29 | Relative:0.01 | Cost: -15 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"TGC-CAG\" and trajectories \"'661', '133', '1411', '725', '1927', '3134', '2872', '484',[...]\".\n",
      "\n",
      "Cluster #87 - Candidate 187 [Absolute:41 | Relative:0.01 | Cost: -29 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CTG-ACC\" and trajectories \"'1137', '1313', '2535', '1027', '1239', '2577', '1887', '1514',[...]\".\n",
      "\n",
      "Cluster #88 - Candidate 188 [Absolute:43 | Relative:0.01 | Cost: -34 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"GGC-AGC\" and trajectories \"'690', '2512', '895', '70', '30', '892', '1119', '1936',[...]\".\n",
      "\n",
      "Cluster #89 - Candidate 194 [Absolute:48 | Relative:0.02 | Cost: -35 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CTT\" and trajectories \"'1002', '2980', '2763', '2441', '1195', '3034', '3184', '1186',[...]\".\n",
      "\n",
      "Cluster #90 - Candidate 199 [Absolute:32 | Relative:0.01 | Cost: -17 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"TGG-CTC\" and trajectories \"'2512', '2535', '229', '2466', '2245', '300', '812', '336',[...]\".\n",
      "\n",
      "Cluster #91 - Candidate 201 [Absolute:44 | Relative:0.01 | Cost: -21 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"CCA-GGA\" and trajectories \"'3037', '750', '452', '2798', '2838', '1815', '2882', '3054',[...]\".\n",
      "\n",
      "Cluster #92 - Candidate 204 [Absolute:35 | Relative:0.01 | Cost: -24 | Ov_coef: 0.11 | Seq length: 2]\n",
      "Attributes sequence \"TTC-TGT\" and trajectories \"'2738', '2951', '1393', '3142', '893', '2608', '1764', '396',[...]\".\n",
      "\n",
      "Cluster #93 - Candidate 205 [Absolute:50 | Relative:0.02 | Cost: -29 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CAG-AAA\" and trajectories \"'1446', '295', '826', '1195', '2366', '3154', '1413', '231',[...]\".\n",
      "\n",
      "Cluster #94 - Candidate 208 [Absolute:31 | Relative:0.01 | Cost: -13 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TCA-GCC\" and trajectories \"'702', '986', '1697', '1404', '1725', '371', '1577', '484',[...]\".\n",
      "\n",
      "Cluster #95 - Candidate 211 [Absolute:42 | Relative:0.01 | Cost: -28 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GCT-GAG\" and trajectories \"'1559', '1675', '1679', '260', '716', '233', '356', '411',[...]\".\n",
      "\n",
      "Cluster #96 - Candidate 215 [Absolute:40 | Relative:0.01 | Cost: -21 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GGG-GGC\" and trajectories \"'743', '3129', '1004', '2734', '741', '22', '496', '1335',[...]\".\n",
      "\n",
      "Cluster #97 - Candidate 223 [Absolute:31 | Relative:0.01 | Cost: -22 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"TCT-TTG\" and trajectories \"'413', '885', '2430', '2801', '2533', '859', '1236', '1403',[...]\".\n",
      "\n",
      "Cluster #98 - Candidate 225 [Absolute:31 | Relative:0.01 | Cost: -20 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TGT-TTC\" and trajectories \"'2650', '140', '2752', '134', '289', '3181', '2579', '1694',[...]\".\n",
      "\n",
      "Cluster #99 - Candidate 226 [Absolute:32 | Relative:0.01 | Cost: -23 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"CTT-TCC\" and trajectories \"'1397', '1008', '1284', '2533', '884', '3184', '1234', '1277',[...]\".\n",
      "\n",
      "Cluster #100 - Candidate 230 [Absolute:45 | Relative:0.01 | Cost: -15 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"CCA-GCC\" and trajectories \"'2333', '227', '1550', '3159', '988', '1272', '1449', '2822',[...]\".\n",
      "\n",
      "Cluster #101 - Candidate 231 [Absolute:46 | Relative:0.01 | Cost: -28 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"TGC-TGG\" and trajectories \"'1732', '2', '757', '725', '2296', '2648', '266', '143',[...]\".\n",
      "\n",
      "Cluster #102 - Candidate 232 [Absolute:40 | Relative:0.01 | Cost: -24 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"GAC-CTG\" and trajectories \"'3030', '3037', '2862', '2502', '1027', '3129', '2814', '312',[...]\".\n",
      "\n",
      "Cluster #103 - Candidate 236 [Absolute:32 | Relative:0.01 | Cost: -17 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCC-AGA\" and trajectories \"'265', '2452', '1090', '2150', '2005', '1101', '1228', '1050',[...]\".\n",
      "\n",
      "Cluster #104 - Candidate 237 [Absolute:34 | Relative:0.01 | Cost: -22 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"AGC-TGA\" and trajectories \"'690', '2296', '1713', '2352', '558', '2608', '2688', '2762',[...]\".\n",
      "\n",
      "Cluster #105 - Candidate 240 [Absolute:37 | Relative:0.01 | Cost: -18 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CTC-TTT\" and trajectories \"'1392', '1675', '1679', '2858', '1900', '3184', '1991', '3021',[...]\".\n",
      "\n",
      "Cluster #106 - Candidate 246 [Absolute:32 | Relative:0.01 | Cost: -12 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"GGG-ATG\" and trajectories \"'307', '2814', '509', '632', '619', '1952', '2997', '1880',[...]\".\n",
      "\n",
      "Cluster #107 - Candidate 250 [Absolute:30 | Relative:0.01 | Cost: -19 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"ACC-AGG\" and trajectories \"'1861', '725', '2742', '1187', '1579', '194', '2813', '433',[...]\".\n",
      "\n",
      "Cluster #108 - Candidate 253 [Absolute:32 | Relative:0.01 | Cost: -24 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"TGT-GGA\" and trajectories \"'1651', '2502', '1410', '2245', '1195', '2404', '2990', '2021',[...]\".\n",
      "\n",
      "Cluster #109 - Candidate 256 [Absolute:29 | Relative:0.01 | Cost: -5 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CCT-GGC\" and trajectories \"'3052', '853', '1949', '2482', '2760', '1600', '782', '1044',[...]\".\n",
      "\n",
      "Cluster #110 - Candidate 260 [Absolute:32 | Relative:0.01 | Cost: -6 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CAG\" and trajectories \"'276', '1394', '803', '2366', '1048', '2495', '1101', '3165',[...]\".\n",
      "\n",
      "Cluster #111 - Candidate 268 [Absolute:32 | Relative:0.01 | Cost: -25 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CCC-ATC\" and trajectories \"'113', '661', '2300', '2585', '92', '1004', '1468', '1683',[...]\".\n",
      "\n",
      "Cluster #112 - Candidate 274 [Absolute:44 | Relative:0.01 | Cost: -25 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TCC-TTC\" and trajectories \"'153', '1410', '1393', '1119', '1835', '1107', '1105', '1117',[...]\".\n",
      "\n",
      "Cluster #113 - Candidate 280 [Absolute:43 | Relative:0.01 | Cost: -11 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CTC-ACC\" and trajectories \"'2351', '229', '1190', '1464', '2339', '328', '893', '336',[...]\".\n",
      "\n",
      "Cluster #114 - Candidate 287 [Absolute:47 | Relative:0.01 | Cost: -30 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GAG-GAA\" and trajectories \"'1751', '335', '657', '2800', '147', '179', '2332', '231',[...]\".\n",
      "\n",
      "Cluster #115 - Candidate 292 [Absolute:30 | Relative:0.01 | Cost: -20 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"AGA-AGC\" and trajectories \"'743', '2650', '571', '1073', '607', '2090', '2557', '833',[...]\".\n",
      "\n",
      "Cluster #116 - Candidate 293 [Absolute:30 | Relative:0.01 | Cost: -8 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CCA-CCT\" and trajectories \"'2773', '1926', '1939', '1034', '2838', '934', '1489', '261',[...]\".\n",
      "\n",
      "Cluster #117 - Candidate 296 [Absolute:30 | Relative:0.01 | Cost: -9 | Ov_coef: 0.16 | Seq length: 2]\n",
      "Attributes sequence \"GGC-TGA\" and trajectories \"'214', '2728', '2666', '1275', '868', '3008', '1306', '156',[...]\".\n",
      "\n",
      "Cluster #118 - Candidate 301 [Absolute:34 | Relative:0.01 | Cost: -8 | Ov_coef: 0.50 | Seq length: 2]\n",
      "Attributes sequence \"TGT-GTG\" and trajectories \"'388', '1022', '1393', '1339', '379', '2252', '2999', '1588',[...]\".\n",
      "\n",
      "Cluster #119 - Candidate 311 [Absolute:31 | Relative:0.01 | Cost: -23 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TTT-TTG\" and trajectories \"'1122', '3046', '1814', '1027', '211', '503', '220', '819',[...]\".\n",
      "\n",
      "Cluster #120 - Candidate 328 [Absolute:39 | Relative:0.01 | Cost: -5 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CTC\" and trajectories \"'1339', '1214', '1319', '1284', '882', '1900', '1277', '1764',[...]\".\n",
      "\n",
      "Cluster #121 - Candidate 332 [Absolute:36 | Relative:0.01 | Cost: -14 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"ACC-TGG\" and trajectories \"'218', '583', '501', '1986', '531', '3021', '209', '1764',[...]\".\n",
      "\n",
      "Cluster #122 - Candidate 336 [Absolute:32 | Relative:0.01 | Cost: -13 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"GCT-GGC\" and trajectories \"'2694', '1555', '50', '2502', '275', '2279', '241', '2946',[...]\".\n",
      "\n",
      "Cluster #123 - Candidate 338 [Absolute:34 | Relative:0.01 | Cost: -14 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GGA-GAA\" and trajectories \"'2694', '1751', '1729', '2170', '132', '1195', '521', '2073',[...]\".\n",
      "\n",
      "Cluster #124 - Candidate 339 [Absolute:36 | Relative:0.01 | Cost: -18 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"ACA-CAG\" and trajectories \"'270', '1381', '295', '244', '3034', '1031', '1984', '1384',[...]\".\n",
      "\n",
      "Cluster #125 - Candidate 346 [Absolute:32 | Relative:0.01 | Cost: -11 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"TTT-TTC\" and trajectories \"'71', '3142', '1289', '2526', '1254', '1348', '51', '1208',[...]\".\n",
      "\n",
      "Cluster #126 - Candidate 348 [Absolute:36 | Relative:0.01 | Cost: -23 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"GGT-GGG\" and trajectories \"'2983', '2798', '10', '1920', '834', '566', '482', '611',[...]\".\n",
      "\n",
      "Cluster #127 - Candidate 352 [Absolute:34 | Relative:0.01 | Cost: -23 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GTA-AGC\" and trajectories \"'229', '29', '2136', '652', '744', '24', '349', '328',[...]\".\n",
      "\n",
      "Cluster #128 - Candidate 370 [Absolute:34 | Relative:0.01 | Cost: -22 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"GAC-CTC\" and trajectories \"'877', '1203', '2352', '3114', '1464', '1107', '856', '2221',[...]\".\n",
      "\n",
      "Cluster #129 - Candidate 390 [Absolute:38 | Relative:0.01 | Cost: -12 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"AGG-GCC\" and trajectories \"'50', '992', '2208', '266', '994', '1284', '2020', '566',[...]\".\n",
      "\n",
      "Cluster #130 - Candidate 396 [Absolute:32 | Relative:0.01 | Cost: -7 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TGT-GGG\" and trajectories \"'3163', '2584', '505', '312', '213', '2339', '1530', '563',[...]\".\n",
      "\n",
      "Cluster #131 - Candidate 399 [Absolute:40 | Relative:0.01 | Cost: -13 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CCA-GCT\" and trajectories \"'1651', '2728', '1559', '540', '1556', '2339', '2098', '2946',[...]\".\n",
      "\n",
      "Cluster #132 - Candidate 410 [Absolute:41 | Relative:0.01 | Cost: -19 | Ov_coef: 0.20 | Seq length: 2]\n",
      "Attributes sequence \"CCT-TCC\" and trajectories \"'1006', '927', '2300', '307', '1314', '2899', '1203', '1289',[...]\".\n",
      "\n",
      "Cluster #133 - Candidate 423 [Absolute:35 | Relative:0.01 | Cost: -1 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"GGA-CAG\" and trajectories \"'2268', '229', '3054', '328', '893', '1337', '336', '888',[...]\".\n",
      "\n",
      "Cluster #134 - Candidate 428 [Absolute:30 | Relative:0.01 | Cost: -2 | Ov_coef: 0.11 | Seq length: 2]\n",
      "Attributes sequence \"CTC-TGG\" and trajectories \"'702', '1790', '2324', '2719', '1272', '652', '3105', '1586',[...]\".\n",
      "\n",
      "Cluster #135 - Candidate 436 [Absolute:33 | Relative:0.01 | Cost: -17 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"TGC-AGC\" and trajectories \"'241', '1668', '2860', '2548', '1582', '1955', '254', '1076',[...]\".\n",
      "\n",
      "Cluster #136 - Candidate 444 [Absolute:32 | Relative:0.01 | Cost: -9 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"GAA-GCC\" and trajectories \"'1852', '161', '2360', '1117', '1092', '2791', '2810', '2705',[...]\".\n",
      "\n",
      "Cluster #137 - Candidate 447 [Absolute:34 | Relative:0.01 | Cost: -13 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"AGA-AAG\" and trajectories \"'133', '2767', '110', '2877', '1641', '3140', '1346', '2202',[...]\".\n",
      "\n",
      "Cluster #138 - Candidate 452 [Absolute:36 | Relative:0.01 | Cost: -13 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GGC-TTC\" and trajectories \"'2862', '275', '2648', '1393', '2145', '1048', '394', '2786',[...]\".\n",
      "\n",
      "Cluster #139 - Candidate 467 [Absolute:39 | Relative:0.01 | Cost: -22 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"ACA-GTG\" and trajectories \"'2559', '1920', '2145', '2332', '1279', '2124', '2756', '2614',[...]\".\n",
      "\n",
      "Cluster #140 - Candidate 470 [Absolute:47 | Relative:0.01 | Cost: -5 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CTG-GAG\" and trajectories \"'1392', '1790', '1022', '147', '2563', '1343', '880', '2397',[...]\".\n",
      "\n",
      "Cluster #141 - Candidate 490 [Absolute:43 | Relative:0.01 | Cost: -17 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TGG-CAG\" and trajectories \"'2', '153', '241', '1443', '143', '1259', '194', '306',[...]\".\n",
      "\n",
      "Cluster #142 - Candidate 495 [Absolute:29 | Relative:0.01 | Cost: -6 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"CCT-GAC\" and trajectories \"'1743', '2792', '1206', '2940', '1490', '1044', '1048', '856',[...]\".\n",
      "\n",
      "Cluster #143 - Candidate 498 [Absolute:49 | Relative:0.02 | Cost: -18 | Ov_coef: 0.17 | Seq length: 2]\n",
      "Attributes sequence \"GGA-AGA\" and trajectories \"'743', '2217', '1651', '757', '2953', '1913', '2833', '481',[...]\".\n",
      "\n",
      "Cluster #144 - Candidate 502 [Absolute:40 | Relative:0.01 | Cost: -13 | Ov_coef: 0.14 | Seq length: 2]\n",
      "Attributes sequence \"TCC-TGC\" and trajectories \"'2131', '1992', '2537', '236', '2768', '169', '1456', '2025',[...]\".\n",
      "\n",
      "Cluster #145 - Candidate 528 [Absolute:37 | Relative:0.01 | Cost: -16 | Ov_coef: 0.14 | Seq length: 2]\n",
      "Attributes sequence \"CCC-TGA\" and trajectories \"'1128', '475', '1174', '988', '1272', '3028', '509', '2438',[...]\".\n",
      "\n",
      "Cluster #146 - Candidate 535 [Absolute:32 | Relative:0.01 | Cost: -10 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"GGC-AAG\" and trajectories \"'1755', '406', '297', '253', '2967', '3114', '2277', '2221',[...]\".\n",
      "\n",
      "Cluster #147 - Candidate 548 [Absolute:29 | Relative:0.01 | Cost: -4 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TTC-CTT\" and trajectories \"'289', '1484', '1688', '241', '2392', '1067', '1679', '2275',[...]\".\n",
      "\n",
      "Cluster #148 - Candidate 558 [Absolute:46 | Relative:0.01 | Cost: -27 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"TCC-CTC\" and trajectories \"'1314', '929', '1397', '1008', '933', '1259', '2892', '3009',[...]\".\n",
      "\n",
      "Cluster #149 - Candidate 618 [Absolute:30 | Relative:0.01 | Cost: -9 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"CAG-AAC\" and trajectories \"'1118', '1062', '815', '2332', '3032', '1131', '738', '2243',[...]\".\n",
      "\n",
      "Cluster #150 - Candidate 639 [Absolute:29 | Relative:0.01 | Cost: -4 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"AAT-GTG\" and trajectories \"'1520', '2562', '385', '1516', '2298', '469', '1524', '1987',[...]\".\n",
      "\n",
      "Cluster #151 - Candidate 644 [Absolute:36 | Relative:0.01 | Cost: -14 | Ov_coef: 0.25 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CAA\" and trajectories \"'2523', '1688', '17', '3045', '1593', '2965', '2166', '1389',[...]\".\n",
      "\n",
      "Cluster #152 - Candidate 650 [Absolute:33 | Relative:0.01 | Cost: -8 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TCT-GCC\" and trajectories \"'1520', '1585', '690', '793', '1370', '3051', '2343', '926',[...]\".\n",
      "\n",
      "Cluster #153 - Candidate 653 [Absolute:40 | Relative:0.01 | Cost: -7 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GAG-ATG\" and trajectories \"'2338', '1483', '2132', '71', '1593', '233', '2990', '1978',[...]\".\n",
      "\n",
      "Cluster #154 - Candidate 663 [Absolute:41 | Relative:0.01 | Cost: -16 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CTC-TGC\" and trajectories \"'1732', '2512', '149', '1027', '1571', '2724', '933', '1295',[...]\".\n",
      "\n",
      "Cluster #155 - Candidate 708 [Absolute:29 | Relative:0.01 | Cost: -2 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCT-TCA\" and trajectories \"'290', '413', '615', '2284', '3090', '3162', '1574', '521',[...]\".\n",
      "\n",
      "Cluster #156 - Candidate 721 [Absolute:37 | Relative:0.01 | Cost: -16 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CAC\" and trajectories \"'306', '1134', '2343', '894', '774', '2610', '773', '1513',[...]\".\n",
      "\n",
      "Cluster #157 - Candidate 733 [Absolute:36 | Relative:0.01 | Cost: -6 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GGA-GGG\" and trajectories \"'3000', '2446', '1945', '1573', '3054', '1586', '262', '2007',[...]\".\n",
      "\n",
      "Cluster #158 - Candidate 755 [Absolute:29 | Relative:0.01 | Cost: -6 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"AGT-GCC\" and trajectories \"'571', '1555', '270', '646', '707', '633', '1418', '301',[...]\".\n",
      "\n",
      "Cluster #159 - Candidate 789 [Absolute:41 | Relative:0.01 | Cost: -14 | Ov_coef: 0.11 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CTC\" and trajectories \"'3037', '2042', '1129', '2227', '2892', '182', '1464', '2517',[...]\".\n",
      "\n",
      "Cluster #160 - Candidate 791 [Absolute:33 | Relative:0.01 | Cost: -5 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"AGC-AGG\" and trajectories \"'190', '2195', '3169', '1846', '625', '2718', '734', '1895',[...]\".\n",
      "\n",
      "Cluster #161 - Candidate 811 [Absolute:29 | Relative:0.01 | Cost: -6 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GTT\" and trajectories \"'1061', '289', '1484', '2841', '1204', '2967', '1490', '1239',[...]\".\n",
      "\n",
      "Cluster #162 - Candidate 815 [Absolute:51 | Relative:0.02 | Cost: -13 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCC-TCC\" and trajectories \"'895', '1075', '1035', '1393', '30', '1008', '933', '892',[...]\".\n",
      "\n",
      "Cluster #163 - Candidate 840 [Absolute:36 | Relative:0.01 | Cost: -1 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"TGC-TTC\" and trajectories \"'1688', '2218', '531', '1134', '2495', '2622', '1955', '1078',[...]\".\n",
      "\n",
      "Cluster #164 - Candidate 841 [Absolute:30 | Relative:0.01 | Cost: -10 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"TTT-TAA\" and trajectories \"'2402', '289', '2856', '83', '1062', '1043', '2260', '2812',[...]\".\n",
      "\n",
      "Cluster #165 - Candidate 879 [Absolute:42 | Relative:0.01 | Cost: -11 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GTC\" and trajectories \"'766', '381', '900', '2419', '1468', '2998', '530', '1031',[...]\".\n",
      "\n",
      "Cluster #166 - Candidate 881 [Absolute:39 | Relative:0.01 | Cost: -6 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GCT-GCA\" and trajectories \"'3030', '568', '1916', '1436', '985', '2160', '2420', '1601',[...]\".\n",
      "\n",
      "Cluster #167 - Candidate 899 [Absolute:35 | Relative:0.01 | Cost: -3 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"CTC-ACA\" and trajectories \"'1381', '1814', '190', '1239', '1384', '2544', '990', '1568',[...]\".\n",
      "\n",
      "Cluster #168 - Candidate 953 [Absolute:32 | Relative:0.01 | Cost: -6 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"TTT-TCC\" and trajectories \"'1611', '2245', '520', '849', '3095', '1343', '1494', '1533',[...]\".\n",
      "\n",
      "Cluster #169 - Candidate 987 [Absolute:35 | Relative:0.01 | Cost: -7 | Ov_coef: 0.25 | Seq length: 2]\n",
      "Attributes sequence \"GAG-GTA\" and trajectories \"'2057', '636', '307', '83', '259', '700', '1989', '664',[...]\".\n",
      "\n",
      "Cluster #170 - Candidate 988 [Absolute:29 | Relative:0.01 | Cost: -3 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GGG-ACA\" and trajectories \"'2928', '2742', '2132', '2384', '1946', '1236', '882', '1407',[...]\".\n",
      "\n",
      "Cluster #171 - Candidate 1004 [Absolute:32 | Relative:0.01 | Cost: -3 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GAT-GAA\" and trajectories \"'1715', '1580', '1818', '288', '1403', '1409', '1348', '322',[...]\".\n",
      "\n",
      "Cluster #172 - Candidate 1008 [Absolute:30 | Relative:0.01 | Cost: -6 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"GCT-CTC\" and trajectories \"'1285', '771', '518', '1485', '1309', '2072', '958', '2677',[...]\".\n",
      "\n",
      "Cluster #173 - Candidate 1020 [Absolute:31 | Relative:0.01 | Cost: -6 | Ov_coef: 0.40 | Seq length: 2]\n",
      "Attributes sequence \"GAC-ACC\" and trajectories \"'1613', '900', '218', '2199', '501', '2028', '2654', '325',[...]\".\n",
      "\n",
      "Cluster #174 - Candidate 1077 [Absolute:38 | Relative:0.01 | Cost: -7 | Ov_coef: 0.25 | Seq length: 2]\n",
      "Attributes sequence \"GGC-TGC\" and trajectories \"'214', '317', '2249', '1306', '1421', '509', '2420', '3057',[...]\".\n",
      "\n",
      "Cluster #175 - Candidate 1119 [Absolute:36 | Relative:0.01 | Cost: -13 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"TCT-TTC\" and trajectories \"'413', '1324', '1432', '1457', '1329', '1459', '1223', '1200',[...]\".\n",
      "\n",
      "Cluster #176 - Candidate 1128 [Absolute:43 | Relative:0.01 | Cost: -12 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"CCC-ACC\" and trajectories \"'3030', '3000', '1417', '922', '2822', '1986', '2538', '1273',[...]\".\n",
      "\n",
      "Cluster #177 - Candidate 1337 [Absolute:30 | Relative:0.01 | Cost: -10 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"TTC-TAG\" and trajectories \"'1732', '1128', '1022', '821', '1063', '1126', '1614', '3055',[...]\".\n",
      "\n",
      "Trajectory \"50\" belongs to User: \"EI\"\n",
      "Max sequence length: 2\n",
      "Levels: ['lvl1', 'lvl2', 'lvl3', 'lvl4']\n",
      "Columns alluvial df: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'count', 'cluster']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering_perf.get_clusters()\n",
    "clustering_perf.create_alluvial_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splice-junction gene sequence 2\n",
    "\n",
    "k:68 ; z:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #1 - Candidate 0 [Absolute:119 | Relative:0.04 | Cost: -117 | Ov_coef: 0.00 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CAG\" and trajectories \"'1751', '978', '986', '1392', '987', '2650', '1387', '1251',[...]\".\n",
      "\n",
      "Cluster #2 - Candidate 1 [Absolute:71 | Relative:0.02 | Cost: -62 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CTG\" and trajectories \"'1751', '1580', '2983', '2779', '2695', '1421', '2672', '863',[...]\".\n",
      "\n",
      "Cluster #3 - Candidate 2 [Absolute:93 | Relative:0.03 | Cost: -86 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CAG\" and trajectories \"'3002', '978', '2983', '725', '987', '1108', '1388', '1279',[...]\".\n",
      "\n",
      "Cluster #4 - Candidate 3 [Absolute:63 | Relative:0.02 | Cost: -59 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"CCT-CAG\" and trajectories \"'1585', '3126', '895', '153', '2713', '956', '892', '2227',[...]\".\n",
      "\n",
      "Cluster #5 - Candidate 4 [Absolute:133 | Relative:0.04 | Cost: -115 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GTG\" and trajectories \"'1285', '690', '110', '241', '1500', '2814', '675', '1310',[...]\".\n",
      "\n",
      "Cluster #6 - Candidate 5 [Absolute:43 | Relative:0.01 | Cost: -37 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CCC\" and trajectories \"'86', '1751', '113', '1736', '3000', '1214', '2761', '932',[...]\".\n",
      "\n",
      "Cluster #7 - Candidate 6 [Absolute:43 | Relative:0.01 | Cost: -33 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CCT\" and trajectories \"'2838', '126', '129', '1319', '2071', '1929', '1048', '1221',[...]\".\n",
      "\n",
      "Cluster #8 - Candidate 7 [Absolute:53 | Relative:0.02 | Cost: -22 | Ov_coef: 0.15 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CTG\" and trajectories \"'1751', '1118', '1324', '1214', '1493', '932', '882', '1106',[...]\".\n",
      "\n",
      "Cluster #9 - Candidate 8 [Absolute:70 | Relative:0.02 | Cost: -62 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CTC\" and trajectories \"'1520', '1006', '1516', '1381', '1524', '229', '1935', '452',[...]\".\n",
      "\n",
      "Cluster #10 - Candidate 9 [Absolute:44 | Relative:0.01 | Cost: -37 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CCT\" and trajectories \"'766', '2502', '597', '455', '932', '1433', '1048', '2678',[...]\".\n",
      "\n",
      "Cluster #11 - Candidate 11 [Absolute:76 | Relative:0.02 | Cost: -63 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"CTG-GTG\" and trajectories \"'548', '1520', '1516', '1524', '1500', '438', '1514', '148',[...]\".\n",
      "\n",
      "Cluster #12 - Candidate 12 [Absolute:76 | Relative:0.02 | Cost: -47 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GAG\" and trajectories \"'978', '986', '1404', '987', '1333', '1812', '3165', '1262',[...]\".\n",
      "\n",
      "Cluster #13 - Candidate 13 [Absolute:43 | Relative:0.01 | Cost: -20 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"CCT-CTG\" and trajectories \"'986', '2300', '161', '1387', '1108', '455', '2137', '3066',[...]\".\n",
      "\n",
      "Cluster #14 - Candidate 14 [Absolute:71 | Relative:0.02 | Cost: -46 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CTC-CAG\" and trajectories \"'2535', '229', '1196', '933', '3090', '1421', '1772', '546',[...]\".\n",
      "\n",
      "Cluster #15 - Candidate 16 [Absolute:55 | Relative:0.02 | Cost: -47 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"AGG-CTG\" and trajectories \"'163', '1585', '518', '1483', '475', '157', '1119', '744',[...]\".\n",
      "\n",
      "Cluster #16 - Candidate 17 [Absolute:48 | Relative:0.02 | Cost: -37 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"GGG-CAG\" and trajectories \"'2983', '1815', '729', '583', '695', '685', '2651', '739',[...]\".\n",
      "\n",
      "Cluster #17 - Candidate 18 [Absolute:57 | Relative:0.02 | Cost: -25 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CCT-CCC\" and trajectories \"'3030', '1754', '1075', '1688', '1388', '1319', '1855', '2892',[...]\".\n",
      "\n",
      "Cluster #18 - Candidate 20 [Absolute:58 | Relative:0.02 | Cost: -40 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CAG\" and trajectories \"'825', '1697', '992', '1929', '826', '1449', '1346', '1310',[...]\".\n",
      "\n",
      "Cluster #19 - Candidate 21 [Absolute:62 | Relative:0.02 | Cost: -58 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"GAG-GTG\" and trajectories \"'50', '657', '1611', '190', '2392', '2249', '179', '124',[...]\".\n",
      "\n",
      "Cluster #20 - Candidate 23 [Absolute:42 | Relative:0.01 | Cost: -34 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CCA\" and trajectories \"'1258', '2115', '1393', '438', '820', '2592', '1068', '384',[...]\".\n",
      "\n",
      "Cluster #21 - Candidate 24 [Absolute:49 | Relative:0.02 | Cost: -34 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CTC\" and trajectories \"'1118', '2535', '3181', '3129', '2567', '1595', '1106', '1004',[...]\".\n",
      "\n",
      "Cluster #22 - Candidate 26 [Absolute:42 | Relative:0.01 | Cost: -39 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"GGA-AGG\" and trajectories \"'1939', '452', '2882', '2953', '834', '716', '531', '3028',[...]\".\n",
      "\n",
      "Cluster #23 - Candidate 27 [Absolute:54 | Relative:0.02 | Cost: -49 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"CCT-GCC\" and trajectories \"'3030', '2011', '956', '3179', '132', '1259', '3066', '820',[...]\".\n",
      "\n",
      "Cluster #24 - Candidate 29 [Absolute:56 | Relative:0.02 | Cost: -49 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TTT-TCT\" and trajectories \"'1392', '2287', '1128', '588', '2227', '1563', '2763', '1913',[...]\".\n",
      "\n",
      "Cluster #25 - Candidate 30 [Absolute:62 | Relative:0.02 | Cost: -37 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"CCA-CAG\" and trajectories \"'1285', '3037', '1258', '810', '1393', '194', '932', '882',[...]\".\n",
      "\n",
      "Cluster #26 - Candidate 40 [Absolute:43 | Relative:0.01 | Cost: -28 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"AGG-AAG\" and trajectories \"'1320', '1324', '1579', '375', '438', '2990', '2563', '1329',[...]\".\n",
      "\n",
      "Cluster #27 - Candidate 42 [Absolute:46 | Relative:0.01 | Cost: -23 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"GTG-GAG\" and trajectories \"'548', '2650', '1555', '518', '1611', '1397', '933', '179',[...]\".\n",
      "\n",
      "Cluster #28 - Candidate 45 [Absolute:57 | Relative:0.02 | Cost: -42 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CCT\" and trajectories \"'3037', '1313', '927', '1550', '1153', '1493', '244', '1476',[...]\".\n",
      "\n",
      "Cluster #29 - Candidate 47 [Absolute:53 | Relative:0.02 | Cost: -43 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TCC-CAG\" and trajectories \"'86', '766', '1075', '452', '1397', '1295', '1284', '2055',[...]\".\n",
      "\n",
      "Cluster #30 - Candidate 50 [Absolute:42 | Relative:0.01 | Cost: -28 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CTG-GGA\" and trajectories \"'1516', '1524', '1675', '260', '3154', '893', '2731', '3167',[...]\".\n",
      "\n",
      "Cluster #31 - Candidate 59 [Absolute:46 | Relative:0.01 | Cost: -40 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CTC\" and trajectories \"'825', '1754', '1027', '1963', '1443', '2723', '3142', '218',[...]\".\n",
      "\n",
      "Cluster #32 - Candidate 60 [Absolute:57 | Relative:0.02 | Cost: -49 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GGG-AAG\" and trajectories \"'743', '2338', '671', '1837', '1926', '227', '757', '3163',[...]\".\n",
      "\n",
      "Cluster #33 - Candidate 63 [Absolute:43 | Relative:0.01 | Cost: -12 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"GAG-CAG\" and trajectories \"'2862', '2650', '413', '149', '933', '301', '530', '1391',[...]\".\n",
      "\n",
      "Cluster #34 - Candidate 69 [Absolute:54 | Relative:0.02 | Cost: -42 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GCT-GCC\" and trajectories \"'3126', '1313', '2146', '268', '2833', '3170', '1333', '124',[...]\".\n",
      "\n",
      "Cluster #35 - Candidate 71 [Absolute:56 | Relative:0.02 | Cost: -50 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TCC-TGG\" and trajectories \"'1732', '2', '1108', '143', '2020', '590', '1273', '3095',[...]\".\n",
      "\n",
      "Cluster #36 - Candidate 75 [Absolute:44 | Relative:0.01 | Cost: -22 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"CTC-AGG\" and trajectories \"'1285', '133', '2170', '518', '475', '218', '501', '287',[...]\".\n",
      "\n",
      "Cluster #37 - Candidate 79 [Absolute:53 | Relative:0.02 | Cost: -14 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GGA\" and trajectories \"'1790', '690', '895', '1314', '929', '1370', '1397', '892',[...]\".\n",
      "\n",
      "Cluster #38 - Candidate 80 [Absolute:50 | Relative:0.02 | Cost: -7 | Ov_coef: 0.16 | Seq length: 2]\n",
      "Attributes sequence \"GTG-GGG\" and trajectories \"'1393', '30', '1936', '481', '558', '1310', '327', '517',[...]\".\n",
      "\n",
      "Cluster #39 - Candidate 85 [Absolute:55 | Relative:0.02 | Cost: -23 | Ov_coef: 0.14 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CCC\" and trajectories \"'86', '3002', '2899', '1108', '3027', '853', '1593', '2150',[...]\".\n",
      "\n",
      "Cluster #40 - Candidate 86 [Absolute:49 | Relative:0.02 | Cost: -36 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"GAG-AAG\" and trajectories \"'1166', '1022', '2115', '2249', '2880', '124', '31', '3025',[...]\".\n",
      "\n",
      "Cluster #41 - Candidate 88 [Absolute:49 | Relative:0.02 | Cost: -28 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GCT-CTG\" and trajectories \"'153', '712', '109', '2538', '1413', '512', '88', '1279',[...]\".\n",
      "\n",
      "Cluster #42 - Candidate 89 [Absolute:43 | Relative:0.01 | Cost: -25 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GCC-CCT\" and trajectories \"'161', '1075', '229', '2071', '132', '1284', '3066', '3009',[...]\".\n",
      "\n",
      "Cluster #43 - Candidate 100 [Absolute:63 | Relative:0.02 | Cost: -45 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"AGG-AGA\" and trajectories \"'743', '757', '636', '3159', '377', '1062', '1573', '853',[...]\".\n",
      "\n",
      "Cluster #44 - Candidate 101 [Absolute:42 | Relative:0.01 | Cost: -29 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GGC-CTC\" and trajectories \"'1585', '766', '190', '1033', '2695', '1119', '2419', '260',[...]\".\n",
      "\n",
      "Cluster #45 - Candidate 110 [Absolute:42 | Relative:0.01 | Cost: -26 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCC-TTC\" and trajectories \"'113', '1174', '2071', '2015', '1254', '92', '1986', '3184',[...]\".\n",
      "\n",
      "Cluster #46 - Candidate 113 [Absolute:66 | Relative:0.02 | Cost: -19 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"CCT-GGG\" and trajectories \"'743', '1580', '129', '2833', '259', '952', '2007', '225',[...]\".\n",
      "\n",
      "Cluster #47 - Candidate 124 [Absolute:53 | Relative:0.02 | Cost: -24 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"GGA-GGC\" and trajectories \"'1006', '2951', '2838', '1573', '1493', '2149', '2482', '3061',[...]\".\n",
      "\n",
      "Cluster #48 - Candidate 129 [Absolute:47 | Relative:0.01 | Cost: -22 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CCC-TGC\" and trajectories \"'1836', '1108', '2670', '1365', '1986', '1468', '1234', '1337',[...]\".\n",
      "\n",
      "Cluster #49 - Candidate 153 [Absolute:46 | Relative:0.01 | Cost: -16 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GGC-CTG\" and trajectories \"'2919', '1855', '834', '505', '213', '454', '1825', '2158',[...]\".\n",
      "\n",
      "Cluster #50 - Candidate 164 [Absolute:46 | Relative:0.01 | Cost: -21 | Ov_coef: 0.12 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CCT\" and trajectories \"'3030', '927', '1320', '2713', '1196', '1929', '422', '419',[...]\".\n",
      "\n",
      "Cluster #51 - Candidate 181 [Absolute:60 | Relative:0.02 | Cost: -15 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCC-AGG\" and trajectories \"'30', '2761', '306', '675', '558', '3127', '1279', '3079',[...]\".\n",
      "\n",
      "Cluster #52 - Candidate 188 [Absolute:43 | Relative:0.01 | Cost: -34 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"GGC-AGC\" and trajectories \"'690', '2512', '895', '70', '30', '892', '1119', '1936',[...]\".\n",
      "\n",
      "Cluster #53 - Candidate 194 [Absolute:48 | Relative:0.02 | Cost: -35 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CTT\" and trajectories \"'1002', '2980', '2763', '2441', '1195', '3034', '3184', '1186',[...]\".\n",
      "\n",
      "Cluster #54 - Candidate 201 [Absolute:44 | Relative:0.01 | Cost: -21 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"CCA-GGA\" and trajectories \"'3037', '750', '452', '2798', '2838', '1815', '2882', '3054',[...]\".\n",
      "\n",
      "Cluster #55 - Candidate 205 [Absolute:50 | Relative:0.02 | Cost: -29 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CAG-AAA\" and trajectories \"'1446', '295', '826', '1195', '2366', '3154', '1413', '231',[...]\".\n",
      "\n",
      "Cluster #56 - Candidate 211 [Absolute:42 | Relative:0.01 | Cost: -28 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GCT-GAG\" and trajectories \"'1559', '1675', '1679', '260', '716', '233', '356', '411',[...]\".\n",
      "\n",
      "Cluster #57 - Candidate 230 [Absolute:45 | Relative:0.01 | Cost: -15 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"CCA-GCC\" and trajectories \"'2333', '227', '1550', '3159', '988', '1272', '1449', '2822',[...]\".\n",
      "\n",
      "Cluster #58 - Candidate 231 [Absolute:46 | Relative:0.01 | Cost: -28 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"TGC-TGG\" and trajectories \"'1732', '2', '757', '725', '2296', '2648', '266', '143',[...]\".\n",
      "\n",
      "Cluster #59 - Candidate 274 [Absolute:44 | Relative:0.01 | Cost: -25 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TCC-TTC\" and trajectories \"'153', '1410', '1393', '1119', '1835', '1107', '1105', '1117',[...]\".\n",
      "\n",
      "Cluster #60 - Candidate 280 [Absolute:43 | Relative:0.01 | Cost: -11 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CTC-ACC\" and trajectories \"'2351', '229', '1190', '1464', '2339', '328', '893', '336',[...]\".\n",
      "\n",
      "Cluster #61 - Candidate 287 [Absolute:47 | Relative:0.01 | Cost: -30 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"GAG-GAA\" and trajectories \"'1751', '335', '657', '2800', '147', '179', '2332', '231',[...]\".\n",
      "\n",
      "Cluster #62 - Candidate 470 [Absolute:47 | Relative:0.01 | Cost: -5 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CTG-GAG\" and trajectories \"'1392', '1790', '1022', '147', '2563', '1343', '880', '2397',[...]\".\n",
      "\n",
      "Cluster #63 - Candidate 490 [Absolute:43 | Relative:0.01 | Cost: -17 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TGG-CAG\" and trajectories \"'2', '153', '241', '1443', '143', '1259', '194', '306',[...]\".\n",
      "\n",
      "Cluster #64 - Candidate 498 [Absolute:49 | Relative:0.02 | Cost: -18 | Ov_coef: 0.17 | Seq length: 2]\n",
      "Attributes sequence \"GGA-AGA\" and trajectories \"'743', '2217', '1651', '757', '2953', '1913', '2833', '481',[...]\".\n",
      "\n",
      "Cluster #65 - Candidate 558 [Absolute:46 | Relative:0.01 | Cost: -27 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"TCC-CTC\" and trajectories \"'1314', '929', '1397', '1008', '933', '1259', '2892', '3009',[...]\".\n",
      "\n",
      "Cluster #66 - Candidate 815 [Absolute:51 | Relative:0.02 | Cost: -13 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCC-TCC\" and trajectories \"'895', '1075', '1035', '1393', '30', '1008', '933', '892',[...]\".\n",
      "\n",
      "Cluster #67 - Candidate 879 [Absolute:42 | Relative:0.01 | Cost: -11 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GTC\" and trajectories \"'766', '381', '900', '2419', '1468', '2998', '530', '1031',[...]\".\n",
      "\n",
      "Cluster #68 - Candidate 1128 [Absolute:43 | Relative:0.01 | Cost: -12 | Ov_coef: 0.10 | Seq length: 2]\n",
      "Attributes sequence \"CCC-ACC\" and trajectories \"'3030', '3000', '1417', '922', '2822', '1986', '2538', '1273',[...]\".\n",
      "\n",
      "Trajectory \"50\" belongs to User: \"EI\"\n",
      "Max sequence length: 2\n",
      "Levels: ['lvl1', 'lvl2', 'lvl3', 'lvl4']\n",
      "Columns alluvial df: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'count', 'cluster']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clustering_perf.get_clusters()\n",
    "clustering_perf.create_alluvial_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splice-junction gene sequence 2\n",
    "\n",
    "k:24 ; z:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #1 - Candidate 0 [Absolute:119 | Relative:0.04 | Cost: -117 | Ov_coef: 0.00 | Seq length: 2]\n",
      "Attributes sequence \"CTG-CAG\" and trajectories \"'1751', '978', '986', '1392', '987', '2650', '1387', '1251',[...]\".\n",
      "\n",
      "Cluster #2 - Candidate 1 [Absolute:71 | Relative:0.02 | Cost: -62 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CTG\" and trajectories \"'1751', '1580', '2983', '2779', '2695', '1421', '2672', '863',[...]\".\n",
      "\n",
      "Cluster #3 - Candidate 2 [Absolute:93 | Relative:0.03 | Cost: -86 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CCC-CAG\" and trajectories \"'3002', '978', '2983', '725', '987', '1108', '1388', '1279',[...]\".\n",
      "\n",
      "Cluster #4 - Candidate 3 [Absolute:63 | Relative:0.02 | Cost: -59 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"CCT-CAG\" and trajectories \"'1585', '3126', '895', '153', '2713', '956', '892', '2227',[...]\".\n",
      "\n",
      "Cluster #5 - Candidate 4 [Absolute:133 | Relative:0.04 | Cost: -115 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GTG\" and trajectories \"'1285', '690', '110', '241', '1500', '2814', '675', '1310',[...]\".\n",
      "\n",
      "Cluster #6 - Candidate 8 [Absolute:70 | Relative:0.02 | Cost: -62 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"CAG-CTC\" and trajectories \"'1520', '1006', '1516', '1381', '1524', '229', '1935', '452',[...]\".\n",
      "\n",
      "Cluster #7 - Candidate 11 [Absolute:76 | Relative:0.02 | Cost: -63 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"CTG-GTG\" and trajectories \"'548', '1520', '1516', '1524', '1500', '438', '1514', '148',[...]\".\n",
      "\n",
      "Cluster #8 - Candidate 12 [Absolute:76 | Relative:0.02 | Cost: -47 | Ov_coef: 0.07 | Seq length: 2]\n",
      "Attributes sequence \"CAG-GAG\" and trajectories \"'978', '986', '1404', '987', '1333', '1812', '3165', '1262',[...]\".\n",
      "\n",
      "Cluster #9 - Candidate 14 [Absolute:71 | Relative:0.02 | Cost: -46 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"CTC-CAG\" and trajectories \"'2535', '229', '1196', '933', '3090', '1421', '1772', '546',[...]\".\n",
      "\n",
      "Cluster #10 - Candidate 16 [Absolute:55 | Relative:0.02 | Cost: -47 | Ov_coef: 0.02 | Seq length: 2]\n",
      "Attributes sequence \"AGG-CTG\" and trajectories \"'163', '1585', '518', '1483', '475', '157', '1119', '744',[...]\".\n",
      "\n",
      "Cluster #11 - Candidate 18 [Absolute:57 | Relative:0.02 | Cost: -25 | Ov_coef: 0.09 | Seq length: 2]\n",
      "Attributes sequence \"CCT-CCC\" and trajectories \"'3030', '1754', '1075', '1688', '1388', '1319', '1855', '2892',[...]\".\n",
      "\n",
      "Cluster #12 - Candidate 20 [Absolute:58 | Relative:0.02 | Cost: -40 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"TTT-CAG\" and trajectories \"'825', '1697', '992', '1929', '826', '1449', '1346', '1310',[...]\".\n",
      "\n",
      "Cluster #13 - Candidate 21 [Absolute:62 | Relative:0.02 | Cost: -58 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"GAG-GTG\" and trajectories \"'50', '657', '1611', '190', '2392', '2249', '179', '124',[...]\".\n",
      "\n",
      "Cluster #14 - Candidate 27 [Absolute:54 | Relative:0.02 | Cost: -49 | Ov_coef: 0.01 | Seq length: 2]\n",
      "Attributes sequence \"CCT-GCC\" and trajectories \"'3030', '2011', '956', '3179', '132', '1259', '3066', '820',[...]\".\n",
      "\n",
      "Cluster #15 - Candidate 29 [Absolute:56 | Relative:0.02 | Cost: -49 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TTT-TCT\" and trajectories \"'1392', '2287', '1128', '588', '2227', '1563', '2763', '1913',[...]\".\n",
      "\n",
      "Cluster #16 - Candidate 30 [Absolute:62 | Relative:0.02 | Cost: -37 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"CCA-CAG\" and trajectories \"'1285', '3037', '1258', '810', '1393', '194', '932', '882',[...]\".\n",
      "\n",
      "Cluster #17 - Candidate 45 [Absolute:57 | Relative:0.02 | Cost: -42 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CCT\" and trajectories \"'3037', '1313', '927', '1550', '1153', '1493', '244', '1476',[...]\".\n",
      "\n",
      "Cluster #18 - Candidate 60 [Absolute:57 | Relative:0.02 | Cost: -49 | Ov_coef: 0.04 | Seq length: 2]\n",
      "Attributes sequence \"GGG-AAG\" and trajectories \"'743', '2338', '671', '1837', '1926', '227', '757', '3163',[...]\".\n",
      "\n",
      "Cluster #19 - Candidate 69 [Absolute:54 | Relative:0.02 | Cost: -42 | Ov_coef: 0.08 | Seq length: 2]\n",
      "Attributes sequence \"GCT-GCC\" and trajectories \"'3126', '1313', '2146', '268', '2833', '3170', '1333', '124',[...]\".\n",
      "\n",
      "Cluster #20 - Candidate 71 [Absolute:56 | Relative:0.02 | Cost: -50 | Ov_coef: 0.03 | Seq length: 2]\n",
      "Attributes sequence \"TCC-TGG\" and trajectories \"'1732', '2', '1108', '143', '2020', '590', '1273', '3095',[...]\".\n",
      "\n",
      "Cluster #21 - Candidate 85 [Absolute:55 | Relative:0.02 | Cost: -23 | Ov_coef: 0.14 | Seq length: 2]\n",
      "Attributes sequence \"TCT-CCC\" and trajectories \"'86', '3002', '2899', '1108', '3027', '853', '1593', '2150',[...]\".\n",
      "\n",
      "Cluster #22 - Candidate 100 [Absolute:63 | Relative:0.02 | Cost: -45 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"AGG-AGA\" and trajectories \"'743', '757', '636', '3159', '377', '1062', '1573', '853',[...]\".\n",
      "\n",
      "Cluster #23 - Candidate 113 [Absolute:66 | Relative:0.02 | Cost: -19 | Ov_coef: 0.06 | Seq length: 2]\n",
      "Attributes sequence \"CCT-GGG\" and trajectories \"'743', '1580', '129', '2833', '259', '952', '2007', '225',[...]\".\n",
      "\n",
      "Cluster #24 - Candidate 181 [Absolute:60 | Relative:0.02 | Cost: -15 | Ov_coef: 0.05 | Seq length: 2]\n",
      "Attributes sequence \"CCC-AGG\" and trajectories \"'30', '2761', '306', '675', '558', '3127', '1279', '3079',[...]\".\n",
      "\n",
      "Trajectory \"50\" belongs to User: \"EI\"\n",
      "Max sequence length: 2\n",
      "Levels: ['lvl1', 'lvl2', 'lvl3', 'lvl4']\n",
      "Columns alluvial df: ['lvl1', 'lvl2', 'lvl3', 'lvl4', 'count', 'cluster']\n"
     ]
    }
   ],
   "source": [
    "clustering_perf.get_clusters()\n",
    "clustering_perf.create_alluvial_diagram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O valor da estatística de shapiro-wilk = 0.9266944527626038\n",
      "O valor do p-value de shapiro-wilk = 0.4161774218082428\n",
      "0.17709753067016487\n",
      "0.9123891112746063\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "y = [1.90642, 2.22488, 2.10288, 1.69742, 1.52229, 3.15435, 2.61826, 1.98492, 1.42738, 1.99568]\n",
    "shapiro_stat, shapiro_p_valor = stats.shapiro(y)\n",
    "print('O valor da estatística de shapiro-wilk = '+str(shapiro_stat))\n",
    "print('O valor do p-value de shapiro-wilk = '+str(shapiro_p_valor))\n",
    "mean = np.mean(y)\n",
    "std = np.std(y,ddof=1)\n",
    "ks_stat, ks_p_value = stats.kstest(y,cdf='norm',args=(mean,std), N=len(y))\n",
    "print(ks_stat)\n",
    "print(ks_p_value)\n",
    "# a = [-4,-3,-2,-1,0,1,2,3,4]\n",
    "# print(np.mean(a),np.std(a))\n",
    "# print('Número de desvios padrões (Z-score): ',(1.5-np.mean(a)/np.std(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_queue(poi_freq_dict):\n",
    "#     print([{k:v} for k,v in poi_freq_dict.items()])\n",
    "    queue = deque()\n",
    "    [queue.append([k,v]) for k,v in poi_freq_dict.items()]\n",
    "    return queue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequence(trajectory_dataset_dict_list, candidate_trajectories_sequence_set, test_traj_sequence):\n",
    "    '''This method receive 3 parameters: \n",
    "       1) trajectory dataset as a dict->list | x['key']:[...];\n",
    "       2) trajectories indeces that contains the tested check-ins as a set;\n",
    "       3) the given tested sequence of check-ins as a string\n",
    "    '''\n",
    "    new_set_trajectories = set()\n",
    "    position_pois_per_traj_list = {}\n",
    "#     test_traj_sequence = test_traj_sequence.strip()\n",
    "    # we will test if the test_traj_sequence exist in the candidate_trajectories that contains the elements.\n",
    "    for traj_id in candidate_trajectories_sequence_set:\n",
    "\n",
    "        try:\n",
    "#             traj_dataset = '-'.join(trajectory_dataset_dict_list[traj_id]).strip()\n",
    "            test_subsequence, positions_at_traj = is_subsequence(trajectory_dataset_dict_list[traj_id],test_traj_sequence.split('-'))\n",
    "            if test_subsequence:\n",
    "#                 print('OK->',end=' ')\n",
    "#                 print(traj_id,positions_at_traj)\n",
    "                new_set_trajectories.add(traj_id)\n",
    "                position_pois_per_traj_list[traj_id] = positions_at_traj\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "#         print('{}-> {}'.format(traj_id,trajectory_dataset_dict_list[traj_id]))\n",
    "#     print('Sequence \"{}\" is present in trajectories: {}'.format(test_traj_sequence,new_set_trajectories))\n",
    "    return new_set_trajectories, position_pois_per_traj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subsequence(sequence, subsequence):\n",
    "    '''This sub method receive two arrays: \n",
    "       first one is the sequence and second one is the tested subsequence.'''\n",
    "    n = len(sequence)\n",
    "    m = len(subsequence)\n",
    "    position_poi_sequence = []\n",
    "    \n",
    "    # Two pointers to traverse the arrays\n",
    "    i = 0; j = 0;\n",
    " \n",
    "    # Traverse both arrays simultaneously\n",
    "    while (i < n and j < m):\n",
    " \n",
    "        # If element matches\n",
    "        # increment both pointers\n",
    "        if (sequence[i] == subsequence[j]):\n",
    "            position_poi_sequence.append(str(i))\n",
    "            i += 1\n",
    "            j += 1\n",
    " \n",
    "            # If subsequence is completely\n",
    "            # traversed\n",
    "            if (j == m):\n",
    "                return True, position_poi_sequence\n",
    "         \n",
    "        # If not,\n",
    "        # increment i and reset j\n",
    "        else:\n",
    "            position_poi_sequence = []\n",
    "            i = i - j + 1\n",
    "            j = 0\n",
    "         \n",
    "    return False,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = {'0':['1','4','6','1','10'],'1':['3','6','7'],'2':['7','9','5'],'3':['4','6','1','10']\n",
    "                ,'4':['9','5']}\n",
    "test_candidate = set(['0','1','2','3','4'])\n",
    "test_sequence = '1-10'\n",
    "mySet, myPos = check_sequence(test_dataset,test_candidate,test_sequence)\n",
    "print([rows+test_sequence.split('-')[poi_id]+poi_pos for rows in mySet \n",
    " for poi_id in range(len(test_sequence.split('-'))) for poi_pos in myPos[rows][poi_id]])\n",
    "form_elements(mySet,test_sequence,myPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_elements(trajs_index_set, tested_sequence, poi_positions_trajectories_dict_list):\n",
    "    '''\n",
    "    This method returns a set of elements.\n",
    "    Each element is formed by the traj ID, poi ID in the sequence and its respective position at traj ID.\n",
    "    Ex: set(['013', '0104', '312', '3103'])\n",
    "        '013': 0-> traj ID, 1-> poi ID, and 3-> position of poi ID at traj ID\n",
    "    '''\n",
    "    tested_sequence = tested_sequence.split('-')\n",
    "    return set([trajID+tested_sequence[poi_id]+poi_pos for trajID in trajs_index_set \n",
    "                for poi_id in range(len(tested_sequence)) \n",
    "                for poi_pos in poi_positions_trajectories_dict_list[trajID][poi_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusA = set([1,2,3,4,5])\n",
    "dict_cc = {'0': {'cc_elements':set([5,6,7,8,9])},'1': {'cc_elements':set([4,5,6,7,8])},\n",
    "           '2': {'cc_elements':set([5,4,3,2,1])}}\n",
    "overlap_coefficient(clusA,dict_cc)\n",
    "# print(overlap_coefficient(clusA,dict_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_coefficient(clusterA, discovered_cc):\n",
    "    max_overlap = 0\n",
    "    \n",
    "    for key, value in discovered_cc.items():\n",
    "#         print(key,value)\n",
    "        elements_intersection = len(clusterA.intersection(discovered_cc[key]['cc_elements']))\n",
    "#         print(elements_intersection)\n",
    "        curr_overlap = elements_intersection/np.min([len(clusterA),len(discovered_cc[key]['cc_elements'])])\n",
    "#         print(curr_overlap)\n",
    "        \n",
    "        if curr_overlap > max_overlap:\n",
    "            max_overlap = curr_overlap\n",
    "            \n",
    "    return max_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {'c1':10,'c2':40,'c3':20,'c4':10,'c5':5,'c6':30,'c7':15,'c8':30,'c9':40,'c10':38}\n",
    "t_values = list(t.values())\n",
    "print(t_values,type(t_values))\n",
    "t_mean = np.mean(t_values)\n",
    "print(t_mean)\n",
    "# looking = True\n",
    "print('Before: ',t)\n",
    "# while looking:\n",
    "tmp = {}\n",
    "for key, value in t.items():\n",
    "    if value > t_mean:\n",
    "        tmp.update({key:value})\n",
    "#         looking = True\n",
    "#         break\n",
    "#     looking = False\n",
    "t = tmp\n",
    "print('After: ',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def TRACOCLUS(input_data, avg_cc_analysis, k=-1, e_obj=-1, e_att=-1):\n",
    "#     input_D = pd.read_csv(input_data, header=None, names=[\"transation\"])\n",
    "    \n",
    "    input_D = ''\n",
    "    split = input_data.split('.')\n",
    "    print(split)\n",
    "    if (split[0] != 'sjgs') and (split[0] != 'sjgs2') and (split[0] != 'splice_data'):\n",
    "        if split[-1] == 'dat':\n",
    "            path = './data/real_application/foursquare_NY/preprocessed/'\n",
    "            input_D = pd.read_csv(path+input_data, header=None, names=[\"transation\"])\n",
    "        else:\n",
    "            path = './data/real_application/foursquare_NY/'\n",
    "            input_D = pd.read_csv(path+input_data, header=None, names=[\"transation\"])\n",
    "    else:\n",
    "        if split[-1] == 'dat':\n",
    "            path = './data/real_application/gene_sequences/SJGS/preprocessed/'\n",
    "            input_D = pd.read_csv(path+input_data, header=None, names=[\"transation\"])\n",
    "        else:\n",
    "            path = './data/real_application/gene_sequences/SJGS/'\n",
    "            input_D = pd.read_csv(path+input_data, header=None, names=[\"transation\"])\n",
    "    \n",
    "    ### variable declaration\n",
    "    if k == -1:\n",
    "        k=sys.maxsize\n",
    "    if e_obj == -1:\n",
    "        e_obj = 1\n",
    "    if e_att == -1:\n",
    "        e_att = 1\n",
    "    \n",
    "    cost_model = sys.float_info.max # initial cost function of the model\n",
    "    num_of_coclusters = 0\n",
    "    D = []\n",
    "    final_coclusters = [] # store the attribute and objects clusters. final_coclusters[[C1_att,C1_obj],[Ck_att,Ck_obj]]\n",
    "    pattern_model = [set(),set()]# Union between the found co-clusters [list of obj,list of att]\n",
    "    cost_per_cocluster = []# stores the cost to build the cocluster\n",
    "    history_cost_model = []\n",
    "    ###\n",
    "    \n",
    "#     D,N,data_dict,data_res_dict,map_id_to_attribute = get_data(input_D)\n",
    "    \n",
    "    # Gamma: store the found co-clusters\n",
    "    overlap_coef_threshold = 0.5\n",
    "    INITIAL_COST = 100.0\n",
    "    final_coclusters = {}\n",
    "    final_clustered_elements = set()\n",
    "    final_coclustering_cost = 0\n",
    "    coclustering_sizes_remove = [] # stores the cluster to be removed from final_coclustering_size\n",
    "#     avg_cc_analysis = \"combine\" # 1. 'index_rows_set'; 2. 'cost_function'; 3. combine\n",
    "#     avg_cc_analysis = cc_analysis # 1. 'index_rows_set'; 2. 'cost_function'\n",
    "    final_coclustering_size = {} # stores the clusters and its num of rows\n",
    "    candidates_ref_values = {}\n",
    "    final_coclustering_avg_row_size = 0\n",
    "    total_of_iterations = 0\n",
    "#     clustering_perf = Performance(sns,plt)\n",
    "#     clustering_perf = Performance()\n",
    "\n",
    "    start_1 = timer()\n",
    "    # Initialize main data structures\n",
    "    map_id_to_attribute_dict, S_poi_freq_dict, poi_at_trajs_dict_set, trajs_data_dict_list = get_data(input_D)\n",
    "    time_p1 = timer()-start_1\n",
    "    clustering_perf.set_variables(len(trajs_data_dict_list))\n",
    "#     S_poi_freq_dict = sort_attributes(S_poi_freq_dict)\n",
    "    \n",
    "    ### select att-values at most elements by log2 of the length of the set\n",
    "#     i = num_elements_to_test('log2',len(S_poi_freq_dict))\n",
    "#     print('Limit log2: ',i)\n",
    "#     for key, value in S_poi_freq_dict.items():\n",
    "#         if i < 0:\n",
    "#             break\n",
    "#         else:\n",
    "#             max_list_with_log2[key]=value\n",
    "#             i-=1\n",
    "    \n",
    "    stop_scability = False\n",
    "    if SCABILITY:\n",
    "        stop_scability = False\n",
    "        if num_of_els <= len(S_poi_freq_dict):\n",
    "            print('Number of elements to test in the scability analysis: '+str(num_of_els))\n",
    "            S_poi_freq_dict = sort_attributes(S_poi_freq_dict)\n",
    "            max_list_of_elems = {}\n",
    "            i = 0\n",
    "            for key, value in S_poi_freq_dict.items():\n",
    "                if i < num_of_els:\n",
    "                    max_list_of_elems[key]=value\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "            S_poi_freq_dict = max_list_of_elems.copy()\n",
    "            print('Number of the most frequent elements: ',len(S_poi_freq_dict))\n",
    "        else:\n",
    "            print('Current dataset is done for scability analisys.')\n",
    "            stop_scability = True\n",
    "    else:\n",
    "        \n",
    "        print('Original number of elements: ',len(S_poi_freq_dict))\n",
    "        \n",
    "        ## select att-values by its frequence that are higher than the average\n",
    "        #if true it selects just the elements with frequency equal or bigger than the AVG; otherwise use all elements\n",
    "        if element_analysis:\n",
    "            max_list_of_elems = {}\n",
    "            average_freq = np.mean(list(S_poi_freq_dict.values()))\n",
    "            for key, value in S_poi_freq_dict.items():\n",
    "                if value > average_freq:\n",
    "                    max_list_of_elems[key]=value\n",
    "\n",
    "            S_poi_freq_dict = max_list_of_elems.copy()\n",
    "            print('Element analysis: True. Number of the most frequent elements: ',len(S_poi_freq_dict))\n",
    "        else:\n",
    "            print('Element analysis: False. We consider the original number of elements: ',len(S_poi_freq_dict))\n",
    "    \n",
    "    \n",
    "    print('Tirando a duvida sobre o tamanho da lista de elementos: ',len(S_poi_freq_dict))\n",
    "    \n",
    "    start = timer()\n",
    "    for iter_k in range(k):\n",
    "        if stop_scability:\n",
    "            break\n",
    "#     for iter_k in tqdm(range(len(S_poi_freq_dict)*len(S_poi_freq_dict)), colour='blue', desc='Searching for candidates'):\n",
    "        print('Searching for candidate: '+str(iter_k+1),end=\"\\r\")\n",
    "#         print('')\n",
    "        \n",
    "#         print('S: ',S_poi_freq_dict)\n",
    "        S_poi_freq_dict = sort_attributes(S_poi_freq_dict)\n",
    "#         S_poi_freq_dict = sort_attributes(max_list_with_log2)\n",
    "#         print('Current main list S: ',S_poi_freq_dict)\n",
    "        S_uppercase_queue_list = populate_queue(S_poi_freq_dict)\n",
    "        \n",
    "        ### Initialize the current co-cluster 'cocluster_*' (CC) and candidate co-cluster 'cc_candidate' (CC*)\n",
    "        cocluster_sequence_str = ''\n",
    "        cocluster_attributes_list = ''\n",
    "        cocluster_index_rows_set = set()\n",
    "        cocluster_elements_set = set()\n",
    "#         cocluster_cost_function = sys.maxsize\n",
    "        cocluster_cost_function = INITIAL_COST\n",
    "        cocluster_max_overlapped_coef = 1\n",
    "        cc_candidate = {}\n",
    "#         num_of_attributes = len(s_poi_freq_queue_list)\n",
    "\n",
    "        clustering_perf.append_result(total_of_iterations,iter_k,final_coclustering_cost)\n",
    "    \n",
    "#         num_att_to_test_S = len(S_uppercase_queue_list)\n",
    "#         while(num_att_to_test_S > 0):\n",
    "#             num_att_to_test_S -= 1\n",
    "#         while S_uppercase_queue_list: # loop it while queue is not empty\n",
    "        limit = num_elements_to_test('length',len(S_uppercase_queue_list))\n",
    "#         limit = num_elements_to_test('log2',len(S_uppercase_queue_list))\n",
    "        for iter_elements_freq in range(0,limit):\n",
    "#         for iter_elements_freq in tqdm(range(limit), colour='blue', desc='Testing element reference'):\n",
    "            \n",
    "#             if cocluster_sequence_str == '':\n",
    "        \n",
    "            S_poi_node_queue = S_uppercase_queue_list.popleft()\n",
    "#             head_sequence_str = S_poi_node_queue[0]\n",
    "#             trajectories_head_sequence_set = poi_at_trajs_dict_set[S_poi_node_queue[0]]\n",
    "#             tail_sequence_str = S_poi_node_queue[0]\n",
    "#             trajectories_tail_sequence_set = poi_at_trajs_dict_set[S_poi_node_queue[0]]\n",
    "            S_uppercase_queue_list.append(S_poi_node_queue)\n",
    "            s_lowercase_queue_list = S_uppercase_queue_list.copy()\n",
    "#             sequence_cc = S_poi_node_queue[0]\n",
    "            sequence_cc = {'cs_sequence_cc': S_poi_node_queue[0],\n",
    "                           'cs_traj_ids_set_cc': poi_at_trajs_dict_set[S_poi_node_queue[0]],\n",
    "                           'cs_elements_cc': set(),\n",
    "                           'clustered_elements': final_clustered_elements}\n",
    "\n",
    "            num_attributes_to_test_s = len(s_lowercase_queue_list)\n",
    "            while(num_attributes_to_test_s > 0): # if it completes one loop the process stops\n",
    "\n",
    "                s_poi_node_queue = s_lowercase_queue_list.popleft()\n",
    "#                     poi_node_queue = s_poi_freq_queue_list[0]\n",
    "                #s_lowercase_queue_list.append(s_poi_node_queue)# original: comentado para inserir apenas no update\n",
    "                cc_candidate = candidate_cocluster(trajs_data_dict_list, poi_at_trajs_dict_set,\n",
    "                                                   sequence_cc, s_poi_node_queue)\n",
    "\n",
    "                if ((cc_candidate != None) and (cc_candidate['cost_function'] <= cocluster_cost_function) and \n",
    "                    (candidate_deviation(avg_cc_analysis,cc_candidate,final_coclustering_size,\n",
    "                                         ('pass' if cc_type_process != 'incremental' else cc_type_process))) and \n",
    "                    (overlap_coefficient(cc_candidate['elements_set'],final_coclusters) <= overlap_coef_threshold)):\n",
    "                    \n",
    "                    over_coef_cc_candidate=overlap_coefficient(cc_candidate['elements_set'],final_coclusters)\n",
    "#                     print('Current co-cluster CC was improved!')\n",
    "\n",
    "                    ### update CC\n",
    "                    cocluster_sequence_str = cc_candidate['sequence_str']\n",
    "                    cocluster_attributes_list = cc_candidate['sequence_str'].split('-')\n",
    "                    cocluster_index_rows_set = cc_candidate['index_rows_set'].copy()\n",
    "                    cocluster_elements_set = cc_candidate['elements_set'].copy()\n",
    "                    cocluster_cost_function = cc_candidate['cost_function']\n",
    "                    cocluster_max_overlapped_coef = over_coef_cc_candidate\n",
    "\n",
    "                    ### update sequence_cc\n",
    "                    sequence_cc['cs_sequence_cc'] = cocluster_sequence_str\n",
    "                    sequence_cc['cs_traj_ids_set_cc'] = cocluster_index_rows_set\n",
    "                    sequence_cc['cs_elements_cc'] = cocluster_elements_set\n",
    "\n",
    "#                     update_queue_s(cocluster_sequence_str, sequence_cc['cs_sequence_cc'],\n",
    "#                                    s_lowercase_queue_list, s_poi_node_queue)\n",
    "                    update_queue_s(cocluster_sequence_str, s_lowercase_queue_list, s_poi_node_queue)\n",
    "\n",
    "                    num_attributes_to_test_s = len(s_lowercase_queue_list)# reassign the counter to restart\n",
    "\n",
    "\n",
    "#                     trajectories_head_sequence_set = cocluster_index_rows_set\n",
    "#                     head_sequence_str = cocluster_sequence_str\n",
    "#                     trajectories_tail_sequence_set = cocluster_index_rows_set\n",
    "#                     tail_sequence_str = cocluster_sequence_str\n",
    "\n",
    "\n",
    "                    total_of_iterations += 1\n",
    "        \n",
    "    #                         clustering_perf.append_result(total_of_iterations,iter_k,cocluster_cost_function)\n",
    "#                     else:# inserting back the element without update\n",
    "#                         s_lowercase_queue_list.append(s_poi_node_queue)\n",
    "#                         num_attributes_to_test_s -= 1\n",
    "#                         total_of_iterations += 1\n",
    "                else:# inserting back the element without update\n",
    "                    s_lowercase_queue_list.append(s_poi_node_queue)\n",
    "#                     print('Current co-cluster CC was NOT improved!')\n",
    "#                     trajectories_head_sequence_set = tmp_traj_set\n",
    "#                     head_sequence_str = tmp_head_sequence_str\n",
    "#                     trajectories_tail_sequence_set = tmp_traj_set\n",
    "#                     tail_sequence_str = tmp_tail_sequence_str\n",
    "                    num_attributes_to_test_s -= 1\n",
    "                    total_of_iterations += 1\n",
    "#                         clustering_perf.append_result(total_of_iterations,iter_k,cocluster_cost_function)\n",
    "\n",
    "#                 print('Queue s* AFTER to update: ',s_lowercase_queue_list)\n",
    "#                 print('')\n",
    "\n",
    "                ### Performance purpose ###\n",
    "                ### Descontinuado ###\n",
    "#                 if cocluster_cost_function != INITIAL_COST:\n",
    "#                     clustering_perf.append_result(total_of_iterations,iter_k,\n",
    "#                                                   (final_coclustering_cost+cocluster_cost_function))\n",
    "#                 else:\n",
    "#                     clustering_perf.append_result(total_of_iterations,iter_k,final_coclustering_cost)\n",
    "\n",
    "            ## END while POIs_to_test (POIs_queue) ##\n",
    "            #########################################\n",
    "\n",
    "            ### check if CC was identified. If don't, it tries the next element p\n",
    "            if cocluster_sequence_str == '':\n",
    "                sequence_cc['cs_sequence_cc'] = ''\n",
    "                sequence_cc['cs_sequence_cc'] = set()\n",
    "                \n",
    "            else: # co-cluster identified Step to store the found cocluster K\n",
    "#                 final_coclusters.update({str(iter_k):{'cc_objs':cocluster_index_rows_set,\n",
    "#                                                       'cc_atts':cocluster_sequence_str,\n",
    "#                                                       'cc_elements':cocluster_elements_set,\n",
    "#                                                       'cc_cost':cocluster_cost_function}})\n",
    "#                 final_clustered_elements = final_clustered_elements.union(cocluster_elements_set)\n",
    "#                 final_coclustering_cost += cocluster_cost_function\n",
    "#                 print('Main list S BEFORE to update: ',S_poi_freq_dict)\n",
    "#                 update_uppercase_S(cocluster_attributes_list, cocluster_index_rows_set, S_poi_freq_dict)\n",
    "#                 print('Main list S AFTER to update: ',S_poi_freq_dict)\n",
    "#                 partial = timer()\n",
    "#                 print('Cluster \"{}\" finished at time \"{}\".'.format((iter_k+1),(partial-start))\n",
    "                break\n",
    "            \n",
    "        ### END while S\n",
    "        #\n",
    "        \n",
    "        ## into loop of iteration_k\n",
    "        partial = timer()\n",
    "        if VERBOSE:\n",
    "            print('Cluster \"{}\" finished at time \"{}\".'.format(iter_k+1,partial-start))\n",
    "        \n",
    "        ### there is not any good co-cluster to identify anymore. Stop searching\n",
    "        if (cocluster_cost_function >= 0) or (cocluster_max_overlapped_coef > overlap_coef_threshold):\n",
    "            if VERBOSE:\n",
    "                print('There is not any good co-cluster to identify anymore.')\n",
    "                \n",
    "            if cc_type_process == 'sample':\n",
    "                candidate_ref = avg_cc_analysis\n",
    "                set_of_candidates = final_coclusters.copy()\n",
    "#                 candidates_ref_values = candidates_ref_values.copy()\n",
    "                \n",
    "                clustering_perf.store_dist(final_coclustering_size)\n",
    "                candidate_deviation(avg_cc_analysis,final_coclusters,final_coclustering_size,'sample')\n",
    "                end = timer()\n",
    "                time_p2 = end-start\n",
    "                total_time_alg = time_p1+time_p2\n",
    "                clustering_perf.store_data_scability_test(candidates_ref_values, num_of_els,\n",
    "                                                          format_time_minutes(total_time_alg), input_data, run_sim)\n",
    "                \n",
    "                print('\\nTotal clustering time in minutes: ',format_time_minutes(total_time_alg))\n",
    "                print('Total clustering time: ',format_time_output(end-start), end=\" \")\n",
    "                now = datetime.datetime.now()\n",
    "                print(\"(\"+str(now.day)+\"/\"+str(now.month)+\"/\"+str(now.year)+\" - \"+str(now.hour)+\":\"+str(now.minute)+\":\"+str(now.second)+\")\")\n",
    "                print('')\n",
    "                \n",
    "                if run_sim < 1 and STORE_CLUS_STATS:\n",
    "                    clustering_perf.compute_measures_at_once(set_of_candidates, candidates_ref_values,\n",
    "                                                             map_id_to_attribute_dict,\n",
    "                                                             trajs_data_dict_list, input_data,\n",
    "                                                             clustering_perf.store_dist(candidates_ref_values))\n",
    "#                 end = timer()\n",
    "#                 print('\\nTotal clustering time after measures_at_once method: ',format_time_output(end-start))\n",
    "            \n",
    "            break\n",
    "        else:\n",
    "            if VERBOSE:\n",
    "                    print('Co-cluster sequence \"{}\" present in \"{}\" trajectories.'.format(cocluster_sequence_str,\n",
    "                                                                                          len(cocluster_index_rows_set)))\n",
    "            final_coclusters.update({str(iter_k):{'cc_objs':cocluster_index_rows_set,\n",
    "                                                      'cc_atts':cocluster_sequence_str,\n",
    "                                                      'cc_elements':cocluster_elements_set,\n",
    "                                                      'cc_cost':cocluster_cost_function,\n",
    "                                                      'cc_over_coef':cocluster_max_overlapped_coef}})\n",
    "            final_clustered_elements = final_clustered_elements.union(cocluster_elements_set)\n",
    "            final_coclustering_cost += cocluster_cost_function\n",
    "            update_uppercase_S(cocluster_attributes_list, cocluster_index_rows_set, S_poi_freq_dict)\n",
    "            \n",
    "            \n",
    "            ### PERFORMANCE PURPOSE CODE ###\n",
    "            ### storing the candidates reference values to evaluate the candidate later\n",
    "            if avg_cc_analysis == \"rows\":\n",
    "#                 print('Rows')\n",
    "                final_coclustering_size.update({str(iter_k):len(cocluster_index_rows_set)})\n",
    "                candidates_ref_values.update({str(iter_k):{'rows':len(cocluster_index_rows_set),\n",
    "                                                           'cost':cocluster_cost_function}})\n",
    "            elif avg_cc_analysis == \"cost\":\n",
    "#                 print('Cost')\n",
    "                final_coclustering_size.update({str(iter_k):cocluster_cost_function})\n",
    "                candidates_ref_values.update({str(iter_k):{'rows':len(cocluster_index_rows_set),\n",
    "                                                           'cost':cocluster_cost_function}})\n",
    "            else:\n",
    "                final_coclustering_size.update({str(iter_k):{'rows':len(cocluster_index_rows_set),\n",
    "                                                             'cost':cocluster_cost_function}})\n",
    "                candidates_ref_values.update({str(iter_k):{'rows':len(cocluster_index_rows_set),\n",
    "                                                           'cost':cocluster_cost_function}})\n",
    "            \n",
    "#             def store_data_scability_rows_cost(self,cc_id,candidate,time_elapse,num_of_traj_points,\n",
    "#                                                num_of_els,run_sim,dataset):\n",
    "            clustering_perf.store_data_scability_rows_cost(int(iter_k+1),\n",
    "                                                           len(cocluster_index_rows_set),\n",
    "                                                           int(cocluster_cost_function),\n",
    "                                                           len(cocluster_sequence_str.split('-')),\n",
    "                                                           format_time_minutes(timer()-start_1),\n",
    "                                                           int(len(cocluster_elements_set)),\n",
    "                                                           int(num_of_els),\n",
    "                                                           int(run_sim),\n",
    "                                                           input_data)\n",
    "        \n",
    "    ## out of loop iteraton k\n",
    "#     end = timer()\n",
    "#     print('\\nTotal clustering time: ',format_time_output(end-start))\n",
    "#     if cc_type_analysis == 'mean':\n",
    "#         print('Process: {}; Metric: {}; Co-cluster ref: {}'.format(cc_type_process,cc_type_analysis,avg_cc_analysis))\n",
    "#     else:\n",
    "#         print('Process: {}; Metric: {}; Co-cluster ref: {}; Z-score: {}'.format(cc_type_process,cc_type_analysis,avg_cc_analysis,cc_z_threshold))\n",
    "    clustering_perf.summary_clusters(final_coclusters, map_id_to_attribute_dict, trajs_data_dict_list)\n",
    "#     clustering_perf.calculate_entropy_purity(input_data)\n",
    "    del map_id_to_attribute_dict,trajs_data_dict_list,candidates_ref_values,cocluster_index_rows_set,cocluster_elements_set\n",
    "    del final_clustered_elements, S_poi_freq_dict, poi_at_trajs_dict_set\n",
    "    return D,final_coclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 10.142857142857142  Var: 9.963197585235825  Round: 10.0\n"
     ]
    }
   ],
   "source": [
    "f = [34,5,6,7,8,9,2]\n",
    "print('Mean:',np.mean(f),' Var:',np.std(f), ' Round:',np.round(np.std(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def avg_cluster_size(ref_analysis, test_value, set_of_clusters):\n",
    "# def avg_cluster_size(ref_analysis,set_of_clusters):\n",
    "def candidate_deviation(ref,value_ref,set_of_clusters,cc_type_process='incremental'):\n",
    "    '''\n",
    "    Method to return the avg number of the reference in the set of co-clusters.\n",
    "    If the set is bigger than 1 it calculates the avg, otherwise it is 0.\n",
    "    Parameters:\n",
    "        ref_analysis: 1. index_rows_set -> considers the rows; 2. cost_function -> considers the cost.\n",
    "        test_value: The value to test.\n",
    "        set_of_clusters: The current set of co-clusters containing its values for the ref_analysis    \n",
    "    '''\n",
    "   \n",
    "    if len(set_of_clusters) >= 2:\n",
    "        try:# single ref\n",
    "            mean = np.mean(list(set_of_clusters.values()))\n",
    "            std = np.std(list(set_of_clusters.values()))\n",
    "        except:# double ref\n",
    "            sum_rows = []\n",
    "            sum_cost = []\n",
    "            for key,value in set_of_clusters.items():\n",
    "                sum_rows.append(set_of_clusters[key]['rows'])\n",
    "                sum_cost.append(set_of_clusters[key]['cost'])\n",
    "            mean_rows = np.mean(sum_rows)\n",
    "            mean_cost = np.mean(sum_cost)\n",
    "            std_rows = np.std(sum_rows)\n",
    "            std_cost = np.std(sum_cost)\n",
    "        \n",
    "        if cc_type_process == 'incremental':\n",
    "            if ref == \"rows\":\n",
    "                ### normal mean\n",
    "                if cc_type_analysis == 'mean':\n",
    "                    return len(value_ref['index_rows_set']) >= np.floor(mean)\n",
    "                else:\n",
    "                ### z-score: we consider values greater than z_thres once it is a positive distribution\n",
    "                    try:\n",
    "                        z = (len(value_ref['index_rows_set'])-mean)/std\n",
    "                    except:\n",
    "                        z = (value_ref-mean)/std\n",
    "                    print('Z-score(rows): ',z)\n",
    "                    return z >= cc_z_threshold_r\n",
    "            elif ref == \"cost\":\n",
    "                ### normal mean\n",
    "                if cc_type_analysis == 'mean':\n",
    "                    return value_ref['cost_function'] <= np.ceil(mean)\n",
    "                else:\n",
    "                ### z-score: we consider values smaller than z_thres once it is a negative distribution\n",
    "                    try:\n",
    "                        z = (value_ref['cost_function']-mean)/std\n",
    "                    except:\n",
    "                        z = (value_ref-mean)/std\n",
    "                    print('Z-score(cost): ',z)\n",
    "                    return z <= cc_z_threshold_c\n",
    "            else:#combine\n",
    "                ### normal mean\n",
    "                if cc_type_analysis == 'mean':\n",
    "#                     print('Mean(rows):',mean_rows,' Mean(cost):',mean_cost)\n",
    "                    return ((len(value_ref['index_rows_set']) >= np.floor(mean_rows)) or \n",
    "                            (value_ref['cost_function'] <= np.ceil(mean_cost)))\n",
    "                else:\n",
    "                ### z-score\n",
    "                    z_rows = (len(value_ref['index_rows_set'])-mean_rows)/std_rows\n",
    "                    z_cost = (value_ref['cost_function']-mean_cost)/std_cost\n",
    "#                     print('Z-score(rows): ',z_rows,' Z-score(cost): ',z_cost)\n",
    "                    return ((z_rows >= cc_z_threshold_r) or (z_cost <= cc_z_threshold_c))\n",
    "                \n",
    "        elif cc_type_process == 'sample':\n",
    "            candidates_to_remove = []\n",
    "            try:# single ref: rows OR cost\n",
    "                mean\n",
    "                if cc_type_analysis == 'mean':\n",
    "                    print('Mean:',mean)\n",
    "                    for key,value in set_of_clusters.items():\n",
    "#                         print('Candidate-'+key+' Mean:',mean,' Value ref:',value,end='')\n",
    "                        if ref == 'rows' and value < mean:\n",
    "#                             print(' -> Remove')\n",
    "                            candidates_to_remove.append(key)\n",
    "                        elif ref == 'cost' and value > mean:\n",
    "#                             print(' -> Remove')\n",
    "                            candidates_to_remove.append(key)\n",
    "                        else:\n",
    "#                             print(' -> Keep')\n",
    "                            pass\n",
    "                else:#z-score\n",
    "                    for key,value in set_of_clusters.items():\n",
    "                        z = (value-mean)/std\n",
    "#                         print('Candidate-'+key+' Z-score:',z,end='')\n",
    "                        if ref == 'rows' and z < cc_z_threshold_r:\n",
    "                            candidates_to_remove.append(key)\n",
    "#                             print(' -> Remove')\n",
    "                        elif ref == 'cost' and z > -cc_z_threshold_c:\n",
    "                            candidates_to_remove.append(key)\n",
    "#                             print(' -> Remove')\n",
    "                        else:\n",
    "#                             print(' -> Keep')\n",
    "                            pass\n",
    "\n",
    "            except:#double ref combine: rows AND cost\n",
    "                if cc_type_analysis == 'mean':\n",
    "                    for key,value in set_of_clusters.items():\n",
    "#                         print('Candidate-'+key+' Mean(rows):',mean_rows,' Mean(cost):',mean_cost,end='')\n",
    "                        if (set_of_clusters[key]['rows'] < mean_rows) and (set_of_clusters[key]['cost'] > mean_cost):\n",
    "#                             print(' -> Remove')\n",
    "                            candidates_to_remove.append(key)\n",
    "                        else:\n",
    "#                             print(' -> Keep')\n",
    "                            pass\n",
    "\n",
    "                else:#z-score\n",
    "                    for key,value in set_of_clusters.items():\n",
    "                        z_rows = (set_of_clusters[key]['rows']-mean_rows)/std_rows\n",
    "                        z_cost = (set_of_clusters[key]['cost']-mean_cost)/std_cost\n",
    "#                         print('Candidate-'+key+' Z-score(row):',z_rows,' Z-score(cost):',z_cost,end='')\n",
    "                        if (z_rows < cc_z_threshold_r) and (z_cost > cc_z_threshold_c):\n",
    "#                             print(' -> Remove')\n",
    "                            candidates_to_remove.append(key)\n",
    "                        else:\n",
    "#                             print(' -> Keep')\n",
    "                            pass\n",
    "                        \n",
    "#             print(\"Remove candidates: \",candidates_to_remove)\n",
    "#             print(\"Number of candidates to remove: \",len(candidates_to_remove))\n",
    "            for candidate in candidates_to_remove:#at this point, value_ref is the set of candidates\n",
    "                del value_ref[candidate]\n",
    "\n",
    "        else: # just pass step. Stores the candidate co-clusters and analyse them with sample analysis if desirable\n",
    "            return True               \n",
    "                \n",
    "    else:# pass step to reach a minimum number of elements to perform computation\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'r': 4, 'c': 7}\n",
      "1 {'r': 5, 'c': 9}\n"
     ]
    }
   ],
   "source": [
    "q = {'0':{'r':4,'c':7},'1':{'r':5,'c':9}}\n",
    "for key, value in q.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'None'\n",
    "print(q)\n",
    "if q == None:\n",
    "    print(\"None\")\n",
    "else:\n",
    "    print('Diff none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_cocluster(trajs_data_dict_list, poi_at_trajs_dict_set, sequence_cc, s_poi_node_queue):\n",
    "    INITIAL_COST = 100.0\n",
    "    ### Current sequence\n",
    "    ### The method tries to form two sequence, if the sequence is valid the method picks the best one\n",
    "    head_sequence_str = sequence_cc['cs_sequence_cc']\n",
    "    trajectories_head_sequence_set = sequence_cc['cs_traj_ids_set_cc']\n",
    "\n",
    "    tail_sequence_str = sequence_cc['cs_sequence_cc']\n",
    "    trajectories_tail_sequence_set = sequence_cc['cs_traj_ids_set_cc']\n",
    "\n",
    "    ### Try to expand the candidate sequence one element at a time if it forms a frequent sequence\n",
    "    ### Step to test ELEMENT at the HEAD ###\n",
    "    tmp_head_sequence_str = head_sequence_str\n",
    "    head_sequence_str = s_poi_node_queue[0]+'-'+head_sequence_str\n",
    "#     if VERBOSE:\n",
    "#         print('-> Head sequence: ',head_sequence_str)\n",
    "    tmp_traj_set = trajectories_head_sequence_set\n",
    "    trajectories_head_sequence_set = trajectories_head_sequence_set.intersection(poi_at_trajs_dict_set[s_poi_node_queue[0]])\n",
    "    trajectories_head_sequence_set, position_poi_per_traj_head = check_sequence(trajs_data_dict_list,\n",
    "                                                                                trajectories_head_sequence_set,\n",
    "                                                                                head_sequence_str)\n",
    "\n",
    "    if len(trajectories_head_sequence_set) > 0:\n",
    "#         if VERBOSE:\n",
    "#             print('Number of rows with this sequence: {}'.format(len(trajectories_head_sequence_set)))\n",
    "        elements_head_sequence = form_elements(trajectories_head_sequence_set,\n",
    "                                               head_sequence_str,\n",
    "                                               position_poi_per_traj_head)    \n",
    "        overlapped_elements = elements_head_sequence.intersection(sequence_cc['clustered_elements'])\n",
    "        cost_head_sequence = cost_function(len(trajectories_head_sequence_set),\n",
    "                                           len(head_sequence_str.split('-')),\n",
    "                                           len(overlapped_elements))\n",
    "#         overlap_coef_head = overlap_coefficient(elements_head_sequence,final_coclusters)\n",
    "#         print('Head cost: {} and overlap_coef: {}.'.format(cost_head_sequence,\n",
    "#                                                            overlap_coef_head))\n",
    "    else:\n",
    "#         if VERBOSE:\n",
    "#             print('Tested head sequence \"{}\" does NOT exist!'.format(head_sequence_str))\n",
    "        trajectories_head_sequence_set = tmp_traj_set\n",
    "        head_sequence_str = tmp_head_sequence_str\n",
    "        cost_head_sequence = INITIAL_COST #\n",
    "        overlap_coef_head = 1\n",
    "    #### END test HEAD sequence ####\n",
    "\n",
    "    #### Step test ELEMENT at the TAIL ####\n",
    "    tmp_tail_sequence_str = tail_sequence_str\n",
    "    tail_sequence_str = tail_sequence_str+'-'+s_poi_node_queue[0]\n",
    "#     if VERBOSE:\n",
    "#         print('-> Tail sequence: ',tail_sequence_str)\n",
    "    tmp_traj_set = trajectories_tail_sequence_set\n",
    "    trajectories_tail_sequence_set = trajectories_tail_sequence_set.intersection(poi_at_trajs_dict_set[s_poi_node_queue[0]])\n",
    "    trajectories_tail_sequence_set, position_poi_per_traj_tail = check_sequence(trajs_data_dict_list,\n",
    "                                                                                trajectories_tail_sequence_set,\n",
    "                                                                                tail_sequence_str)\n",
    "\n",
    "    if (len(trajectories_tail_sequence_set) > 0):\n",
    "#         if VERBOSE:\n",
    "#             print('Number of rows with this sequence: {}'.format(len(trajectories_tail_sequence_set)))\n",
    "        elements_tail_sequence = form_elements(trajectories_tail_sequence_set,\n",
    "                                               tail_sequence_str,\n",
    "                                               position_poi_per_traj_tail)\n",
    "        overlapped_elements = elements_tail_sequence.intersection(sequence_cc['clustered_elements'])\n",
    "        cost_tail_sequence = cost_function(len(trajectories_tail_sequence_set),\n",
    "                                           len(tail_sequence_str.split('-')),\n",
    "                                           len(overlapped_elements))\n",
    "#         overlap_coef_tail = overlap_coefficient(elements_tail_sequence,final_coclusters)\n",
    "#         print('Tail cost: {} and overlap_coef: {}.'.format(cost_tail_sequence,\n",
    "#                                                            overlap_coef_tail))\n",
    "    else:\n",
    "#         if VERBOSE:\n",
    "#             print('Tested tail sequence \"{}\" does NOT exist!'.format(tail_sequence_str))\n",
    "        trajectories_tail_sequence_set = tmp_traj_set\n",
    "        tail_sequence_str = tmp_tail_sequence_str\n",
    "        cost_tail_sequence = INITIAL_COST\n",
    "        overlap_coef_tail = 1\n",
    "    #### END test TAIL sequence ####\n",
    "    \n",
    "#     print('Current co-cluster cost: ',cocluster_cost_function)\n",
    "#     print('Queue s* BEFORE to upadate: ',s_lowercase_queue_list)\n",
    "\n",
    "    ### Step to test the best sequence if exist a sequence\n",
    "    if (cost_head_sequence < cost_tail_sequence) and (cost_head_sequence < 0):\n",
    "#         print('Co-cluster improved with HEAD sequence.')\n",
    "\n",
    "        # update the nodes of queue s.\n",
    "#         update_queue_s(cocluster_sequence_str, head_sequence_str,\n",
    "#                        s_lowercase_queue_list, s_poi_node_queue)\n",
    "#         update_queue_s(candidate_sequence['cs_sequence_cc'], head_sequence_str,\n",
    "#                        s_lowercase_queue_list, s_poi_node_queue)\n",
    "\n",
    "#         cocluster_sequence_str = head_sequence_str\n",
    "#         cocluster_attributes_list = head_sequence_str.split('-')\n",
    "#         cocluster_index_rows_set = trajectories_head_sequence_set.copy()\n",
    "#         cocluster_elements_set = elements_head_sequence.copy()\n",
    "#         cocluster_cost_function = cost_head_sequence\n",
    "#         cocluster_max_overlapped_coef = overlap_coef_head\n",
    "        \n",
    "        cc_candidate = {'sequence_str': head_sequence_str,\n",
    "                        'attributes_list': head_sequence_str.split('-'),\n",
    "                        'index_rows_set': trajectories_head_sequence_set.copy(),\n",
    "                        'elements_set': elements_head_sequence.copy(),\n",
    "                        'cost_function': cost_head_sequence}        \n",
    "        \n",
    "        return cc_candidate\n",
    "\n",
    "    elif (cost_tail_sequence < cost_head_sequence) and (cost_tail_sequence < 0):\n",
    "#         if VERBOSE:\n",
    "#             print('Co-cluster improved with TAIL sequence.')\n",
    "\n",
    "        # update the nodes of queue s.\n",
    "#         update_queue_s(cocluster_sequence_str,tail_sequence_str,\n",
    "#                        s_lowercase_queue_list,s_poi_node_queue)\n",
    "#         update_queue_s(candidate_sequence['cs_sequence_cc'], tail_sequence_str,\n",
    "#                        s_lowercase_queue_list, s_poi_node_queue)\n",
    "\n",
    "#         cocluster_sequence_str = tail_sequence_str\n",
    "#         cocluster_attributes_list = tail_sequence_str.split('-')\n",
    "#         cocluster_index_rows_set = trajectories_tail_sequence_set.copy()\n",
    "#         cocluster_elements_set = elements_tail_sequence.copy()\n",
    "#         cocluster_cost_function = cost_tail_sequence\n",
    "#         cocluster_max_overlapped_coef = overlap_coef_tail\n",
    "        \n",
    "        cc_candidate = {'sequence_str': tail_sequence_str,\n",
    "                        'attributes_list': tail_sequence_str.split('-'),\n",
    "                        'index_rows_set': trajectories_tail_sequence_set.copy(),\n",
    "                        'elements_set': elements_tail_sequence.copy(),\n",
    "                        'cost_function': cost_tail_sequence}        \n",
    "        \n",
    "        return cc_candidate\n",
    "    \n",
    "    else:# it does not found any sequence formed by the elements\n",
    "#         cc_candidate = {'sequence_str': None}\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory \"50\" belongs to User: \"293\"\n"
     ]
    }
   ],
   "source": [
    "create_alluvial_diagram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alluvial_diagram():\n",
    "    df_traj_user = pd.read_csv('./data/real_application/foursquare_NY/fs_ny_top_users_10.csv', sep=';')\n",
    "#     df_traj_user.drop(columns=['tid','lat_lon','time','day','type','root_type','rating','weather'],inplace=True)\n",
    "    df_traj_user = df_traj_user[['new_tid','label']]\n",
    "    #     print(df_traj_user.head())\n",
    "    \n",
    "    traj_id = 50\n",
    "    user_label = df_traj_user[df_traj_user['new_tid'] == traj_id]['label'].unique()[0]\n",
    "    print('Trajectory \"{}\" belongs to User: \"{}\"'.format(traj_id,\n",
    "                                                         user_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': 0, 'l2': None, 'l3': None}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = ['l1','l2','l3']\n",
    "t = dict.fromkeys(r)\n",
    "t['l1'] = 0\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Performance():\n",
    "#     perf_df_clustering_output_measures = pd.DataFrame(columns = ['Iteration_i','Candidate_iteration_k',\n",
    "#                                                             'Candidate_cost'])\n",
    "    \n",
    "#     df_quality_clustering = pd.DataFrame(columns= ['Dataset','Clustering_approach','Cocluster_reference',\n",
    "#                                                    'Cocluster_statistic','Num_of_candidates','Num_of_clusters',\n",
    "#                                                    'Overall_entropy','Purity'])\n",
    "#     df_scability = pd.DataFrame(columns = ['num_of_elements','time_minutes','dataset','run_simulation'])\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_scability = pd.DataFrame(columns = ['num_of_elements','time_minutes','run_simulation',\n",
    "                                                    'num_of_candidates','dataset'])\n",
    "        \n",
    "        self.df_scab_rows_cost = pd.DataFrame(columns = ['candidate_id','candidate_num_rows','candidate_cost',\n",
    "                                                         'time_discovered_minutes','num_of_traj_points','num_of_elements',\n",
    "                                                         'run_simulation','dataset'])\n",
    "        self.df_scab_rows_cost.candidate_id = self.df_scab_rows_cost.candidate_id.astype(float)\n",
    "        self.df_scab_rows_cost.candidate_num_rows = self.df_scab_rows_cost.candidate_num_rows.astype(float)\n",
    "        self.df_scab_rows_cost.candidate_cost = self.df_scab_rows_cost.candidate_cost.astype(float)\n",
    "    \n",
    "    def plot_scability_test(self,file):\n",
    "        self.dpi = 600\n",
    "        self.fig = plt.figure(figsize=(3, 2),dpi=self.dpi)\n",
    "        self.ax = sns.lineplot(x = \"num_of_elements\", y = \"time_minutes\", hue='dataset', err_style='bars', data = self.df_scability)\n",
    "        \n",
    "        self.ax.legend(loc='upper left', fontsize=4)\n",
    "        sns.despine(offset=0, trim=True, left=True)\n",
    "        self.ax.yaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "        self.ax.set_yticklabels(self.ax.get_ymajorticklabels(), fontsize = 6)\n",
    "        self.ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "        plt.xticks(horizontalalignment='center',fontsize=6)\n",
    "#         ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 6)\n",
    "#         ax.xaxis.set_major_locator(ticker.MultipleLocator(70))\n",
    "#         ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "        plt.ylabel('AVG time (minutes)',fontsize=7)\n",
    "        plt.xlabel('Number of elemetns',fontsize=7)\n",
    "        self.axes = plt.gca()\n",
    "        self.axes.yaxis.grid(color='black',linewidth=.1)\n",
    "        plt.tight_layout()\n",
    "#         fig.savefig('C:/Users/yurin/Downloads/'+file+'.png',transparent=True,bbox_inches = 'tight',pad_inches=0,dpi=dpi)\n",
    "        plt.show()\n",
    "#         print(self.df_scability.head())\n",
    "\n",
    "    def plot_scability_rows_cost(self,file):\n",
    "        self.dpi = 600\n",
    "        self.fig = plt.figure(figsize=(3, 2),dpi=self.dpi)\n",
    "        self.ax = sns.lineplot(x = \"candidate_id\",y = \"candidate_cost\",hue = \"dataset\",err_style='bars', data = self.df_scab_rows_cost)\n",
    "        \n",
    "#         self.ax.legend(loc='upper right', fontsize=4)\n",
    "        self.ax.legend(fontsize=4)\n",
    "        sns.despine(offset=0, trim=True, left=True)\n",
    "        self.ax.yaxis.set_major_locator(ticker.MultipleLocator(20))\n",
    "        self.ax.set_yticklabels(self.ax.get_ymajorticklabels(), fontsize = 6)\n",
    "        self.ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "        plt.xticks(horizontalalignment='center',fontsize=6)\n",
    "#         ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 6)\n",
    "#         ax.xaxis.set_major_locator(ticker.MultipleLocator(70))\n",
    "#         ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "        plt.ylabel('Cost value per candidate',fontsize=7)\n",
    "        plt.xlabel('Number of candidates',fontsize=7)\n",
    "#         self.axes = plt.gca()\n",
    "#         self.axes.yaxis.grid(color='black',linewidth=.1)\n",
    "#         plt.tight_layout()\n",
    "#         fig.savefig('C:/Users/yurin/Downloads/'+file+'.png',transparent=True,bbox_inches = 'tight',pad_inches=0,dpi=dpi)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "#         self.dpi = 600\n",
    "#         self.fig2 = plt.figure(figsize=(3, 2),dpi=self.dpi)\n",
    "#         self.ax2 = sns.lineplot(x = \"candidate_id\", y = \"candidate_num_rows\", hue = \"dataset\",data = self.df_scab_rows_cost)\n",
    "#         self.ax2.legend(loc='upper right', fontsize=4)\n",
    "#         sns.despine(offset=0, trim=True, left=True)\n",
    "#         self.ax2.yaxis.set_major_locator(ticker.MultipleLocator(20))\n",
    "#         self.ax2.set_yticklabels(self.ax2.get_ymajorticklabels(), fontsize = 6)\n",
    "#         self.ax2.yaxis.set_major_formatter(ticker.ScalarFormatter())\n",
    "#         plt.xticks(horizontalalignment='center',fontsize=6)\n",
    "#         plt.ylabel('Num of trajs. per candidate',fontsize=7)\n",
    "#         plt.xlabel('Number of candidates',fontsize=7)\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "    \n",
    "    def store_data_scability_rows_cost(self,cc_id,num_of_rows,cost,seq_len,time_elapse,num_of_traj_points,num_of_els,run_sim,dataset):\n",
    "        if dataset == 'fs_ny_top_users_193.dat':\n",
    "            self.__sdst_dataset = 'All users'\n",
    "        elif dataset == 'fs_ny_top_users_81.dat':\n",
    "            self.__sdst_dataset = 'Top 81 users'\n",
    "        elif dataset == 'fs_ny_top_users_10.dat':\n",
    "            self.__sdst_dataset = 'Top 10 users'\n",
    "        elif (dataset == 'sjgs.dat') or (dataset == 'sjgs2.dat'):\n",
    "            self.__sdst_dataset = dataset.split('.')[0].upper()\n",
    "        else:\n",
    "            self.__sdst_dataset = 'Undefined dataset.'\n",
    "        \n",
    "#         self.df_scab_rows_cost = self.df_scab_rows_cost.append({'candidate_id':cc_id,\n",
    "#                                                                 'candidate_num_rows':int(num_of_rows),\n",
    "#                                                                 'candidate_cost':int(cost),\n",
    "#                                                                 'time_discovered':time_elapse,\n",
    "#                                                                 'num_of_traj_points':int(num_of_traj_points),\n",
    "#                                                                 'num_of_elements':num_of_els,\n",
    "#                                                                 'run_simulation':run_sim,\n",
    "#                                                                 'dataset':self.__sdst_dataset},\n",
    "#                                                                ignore_index=True)\n",
    "        #list of column names\n",
    "        self.frc_field_names = ['candidate_id','candidate_num_rows','candidate_cost','candidate_seq_len',\n",
    "                                'time_discovered_minutes','num_of_traj_points','num_of_elements','run_simulation',\n",
    "                                'dataset']\n",
    "        \n",
    "        # Dictionary\n",
    "        self.frc_data = {'candidate_id':cc_id,'candidate_num_rows':int(num_of_rows),'candidate_cost':int(cost),\n",
    "                         'candidate_seq_len':int(seq_len),'time_discovered_minutes':time_elapse,\n",
    "                         'num_of_traj_points':int(num_of_traj_points),'num_of_elements':num_of_els,\n",
    "                         'run_simulation':run_sim,'dataset':self.__sdst_dataset}\n",
    "        \n",
    "        with open('./coclustering_file_outputs/df_scab_rows_cost.csv', 'a+', newline='') as self.frc_object:\n",
    "      \n",
    "            # Pass the file object and a list \n",
    "            # of column names to DictWriter()\n",
    "            # You will get a object of DictWriter\n",
    "            self.rc_dictwriter_object = DictWriter(self.frc_object, delimiter=\";\", fieldnames=self.frc_field_names)\n",
    "\n",
    "            #Pass the dictionary as an argument to the Writerow()\n",
    "            self.rc_dictwriter_object.writerow(self.frc_data)\n",
    "\n",
    "            #Close the file object\n",
    "            self.frc_object.close()\n",
    "    \n",
    "    def store_data_scability_test(self,candidates_ref_values,num_of_els,time_elapse,dataset,run_sim):\n",
    "#         datasets = ['fs_ny_top_users_193.dat','fs_ny_top_users_81.dat','fs_ny_top_users_10.dat']\n",
    "        if dataset == 'fs_ny_top_users_193.dat':\n",
    "            self.__sdst_dataset = 'All users'\n",
    "        elif dataset == 'fs_ny_top_users_81.dat':\n",
    "            self.__sdst_dataset = 'Top 81 users'\n",
    "        elif dataset == 'fs_ny_top_users_10.dat':\n",
    "            self.__sdst_dataset = 'Top 10 users'\n",
    "        elif (dataset == 'sjgs.dat') or (dataset == 'sjgs2.dat'):\n",
    "            self.__sdst_dataset = dataset.split('.')[0].upper()\n",
    "        else:\n",
    "            self.__sdst_dataset = 'Undefined dataset.'\n",
    "        \n",
    "#         print('Salvando scability.',end=' ')\n",
    "        \n",
    "#         for cc, value in candidates_ref_values.items():\n",
    "#             self.__sdst_cc_num_of_rows = candidates_ref_values[cc]['rows']\n",
    "#             self.__sdst_cc_cost = candidates_ref_values[cc]['cost']\n",
    "            \n",
    "#         self.df_scability = self.df_scability.append({'num_of_elements':str(num_of_els),\n",
    "#                                                       'time_minutes':time_elapse,\n",
    "#                                                       'run_simulation':run_sim,\n",
    "#                                                       'num_of_candidates':len(candidates_ref_values),\n",
    "#                                                       'dataset':self.__sdst_dataset},\n",
    "#                                                      ignore_index=True)\n",
    "        \n",
    "        #list of column names\n",
    "        self.sdst_field_names = ['num_of_elements','time_minutes','run_simulation','num_of_candidates','dataset']\n",
    "        \n",
    "        # Dictionary\n",
    "        self.sdst_data = {'num_of_elements':str(num_of_els),'time_minutes':time_elapse,'run_simulation':run_sim,\n",
    "                     'num_of_candidates':len(candidates_ref_values),'dataset':self.__sdst_dataset}\n",
    "        \n",
    "        with open('./coclustering_file_outputs/df_scability.csv', 'a+', newline='') as self.fsdst_object:\n",
    "      \n",
    "            # Pass the file object and a list \n",
    "            # of column names to DictWriter()\n",
    "            # You will get a object of DictWriter\n",
    "            self.sdst_dictwriter_object = DictWriter(self.fsdst_object, delimiter=\";\", fieldnames=self.sdst_field_names)\n",
    "\n",
    "            #Pass the dictionary as an argument to the Writerow()\n",
    "            self.sdst_dictwriter_object.writerow(self.sdst_data)\n",
    "\n",
    "            #Close the file object\n",
    "            self.fsdst_object.close()\n",
    "\n",
    "    \n",
    "    def compute_measures_at_once(self,set_of_candidates,candidates_ref_values,map_tid_to_el,trajs_data_dict_list,file_dataset,store_dist=None):\n",
    "        '''\n",
    "        Method to compute the measures at once for given dataset.\n",
    "        It is aimed to avoid unecessary recomputation for the candidates.\n",
    "        '''\n",
    "        print('Compute_measures_at_once method class Performance.')\n",
    "#         short_path_metrics(value_ref,set_of_clusters,cc_type_process='incremental'):\n",
    "        spm_metric_list = ['mean','z_score']\n",
    "        smp_ref_list = ['rows','cost','combine']\n",
    "        smp_z_thres_list = [-1,0,1]\n",
    "\n",
    "# short_path_metrics(ref,cc_type_analysis,value_ref,set_of_clusters,cc_type_process='incremental',cc_z_threshold=None):\n",
    "        for cc_type_analysis in spm_metric_list:\n",
    "            for ref in smp_ref_list:\n",
    "                if cc_type_analysis == 'mean':\n",
    "                    print('Process: {}; Metric: {}; Ref: {}'.format(cc_type_process,cc_type_analysis,ref))\n",
    "#                     candidates_to_remove = bad_candidates(set_of_clusters,cc_type_analysis,ref)\n",
    "#                     tmp_set_of_candidates = value_ref.copy()\n",
    "#                     for candidate in candidates_to_remove:#at this point, value_ref is the set of candidates\n",
    "# #                             del value_ref[candidate]\n",
    "#                         del tmp_set_of_candidates[candidate]\n",
    "#                         return tmp_set_of_candidates\n",
    "                    final_coclusters = short_path_metrics(ref,cc_type_analysis,set_of_candidates,candidates_ref_values,'sample')\n",
    "                    self.summary_clusters(final_coclusters, map_tid_to_el, trajs_data_dict_list)\n",
    "                    self.calculate_entropy_purity(file_dataset)\n",
    "                    self.__store_clustering_statistics(file_dataset,candidates_ref_values,cc_type_analysis,ref)\n",
    "                else:\n",
    "                    for cc_z_threshold in smp_z_thres_list:\n",
    "                        print('Process: {}; Metric: {}; Ref: {}; z_thres: {}'.format(cc_type_process,cc_type_analysis,ref,cc_z_threshold))\n",
    "#                         candidates_to_remove = bad_candidates(set_of_clusters,cc_type_analysis,ref,cc_z_threshold)\n",
    "#                         tmp_set_of_candidates = value_ref.copy()\n",
    "#                         for candidate in candidates_to_remove:#at this point, value_ref is the set of candidates\n",
    "# #                                 del value_ref[candidate]\n",
    "#                             del tmp_set_of_candidates[candidate]\n",
    "#                             return tmp_set_of_candidates\n",
    "                        final_coclusters = short_path_metrics(ref,cc_type_analysis,set_of_candidates,candidates_ref_values,'sample',cc_z_threshold)\n",
    "                        self.summary_clusters(final_coclusters, map_tid_to_el,trajs_data_dict_list)\n",
    "                        self.calculate_entropy_purity(file_dataset)\n",
    "                        self.__store_clustering_statistics(file_dataset,candidates_ref_values,cc_type_analysis,ref,cc_z_threshold)\n",
    "\n",
    "#         final_coclusters = short_path_metrics(set_of_candidates,candidates_ref_values,'sample')\n",
    "#         self.summary_clusters(final_coclusters, map_tid_to_el,trajs_data_dict_list)\n",
    "#         self.calculate_entropy_purity(file_dataset)\n",
    "        print('END of measures_at_once method')\n",
    "    \n",
    "    def __store_clustering_statistics(self,dataset,candidates_ref_values,cc_type_analysis,ref,cc_z_threshold=''):\n",
    "        \n",
    "        if dataset == 'fs_ny_top_users_193.dat':\n",
    "            self.__scstats_dataset = 'All users'\n",
    "        elif dataset == 'fs_ny_top_users_81.dat':\n",
    "            self.__scstats_dataset = 'Top 81 users'\n",
    "        elif dataset == 'fs_ny_top_users_10.dat':\n",
    "            self.__scstats_dataset = 'Top 10 users'\n",
    "        elif (dataset == 'sjgs.dat') or (dataset == 'sjgs2.dat'):\n",
    "            self.__scstats_dataset = datset.split('.')[0].upper()\n",
    "        else:\n",
    "            self.__scstats_dataset = 'Undefined dataset.'\n",
    "\n",
    "        #list of column names\n",
    "        self.scs_field_names = ['dataset','metric','cc_reference','num_of_candidates','num_of_clusters',\n",
    "                                'avg_std_cv_rows','avg_std_cv_cost','num_of_groupped_elements','avg_std_cv_seq_len',\n",
    "                                'avg_std_cv_relative_rows_compression','avg_std_cv_num_of_users','overall_entropy']\n",
    "        \n",
    "        if cc_z_threshold == '':\n",
    "            self.__scstats_metric = cc_type_analysis\n",
    "        else:\n",
    "            self.__scstats_metric = cc_type_analysis+'['+str(cc_z_threshold)+']'\n",
    "            \n",
    "        self.__scstats_num_of_cc = len(candidates_ref_values)\n",
    "        self.__scstats_avg_rows = np.round(np.mean(self.num_of_trajs_per_cluster),3)\n",
    "        self.__scstats_std_rows = np.round(np.std(self.num_of_trajs_per_cluster),3)\n",
    "        self.__scstats_cv_rows = np.round((self.__scstats_std_rows/self.__scstats_avg_rows)*100,3)\n",
    "        self.__scstats_str_rows = str(self.__scstats_avg_rows)+'\\u00B1'+str(self.__scstats_std_rows)+'['+str(self.__scstats_cv_rows)+']'\n",
    "        self.__scstats_avg_cost = np.round(np.mean(self.cost_value_per_cluster),3)\n",
    "        self.__scstats_std_cost = np.round(np.std(self.cost_value_per_cluster),3)\n",
    "        self.__scstats_cv_cost = np.round((self.__scstats_std_cost/self.__scstats_avg_cost)*100,3)\n",
    "        self.__scstats_str_cost = str(self.__scstats_avg_cost)+'\\u00B1'+str(self.__scstats_std_cost)+'['+str(self.__scstats_cv_cost)+']'\n",
    "        self.__scstats_avg_seq_len = np.round(np.mean(self.seq_len_per_cluster),3)\n",
    "        self.__scstats_std_seq_len = np.round(np.std(self.seq_len_per_cluster),3)\n",
    "        self.__scstats_cv_seq_len = np.round((self.__scstats_std_seq_len/self.__scstats_avg_seq_len)*100,3)\n",
    "        self.__scstats_str_seq_len = str(self.__scstats_avg_seq_len)+'\\u00B1'+str(self.__scstats_std_seq_len)+'['+str(self.__scstats_cv_seq_len)+']'\n",
    "        self.__scstats_avg_relative_compress = np.round(np.mean(self.relative_clusters_value),3)\n",
    "        self.__scstats_std_relative_compress = np.round(np.std(self.relative_clusters_value),3)\n",
    "        self.__scstats_cv_relative_compress = np.round((self.__scstats_std_relative_compress/self.__scstats_avg_relative_compress)*100,3)\n",
    "        self.__scstats_str_relative_compress = str(self.__scstats_avg_relative_compress)+'\\u00B1'+str(self.__scstats_std_relative_compress)+'['+str(self.__scstats_cv_relative_compress)+']'\n",
    "        self.__scstats_avg_num_of_users = np.round(np.mean(self.num_of_users_per_cluster),3)\n",
    "        self.__scstats_std_num_of_users = np.round(np.std(self.num_of_users_per_cluster),3)\n",
    "        self.__scstats_cv_num_of_users = np.round((self.__scstats_std_num_of_users/self.__scstats_avg_num_of_users)*100,3)\n",
    "        self.__scstats_str_num_of_users = str(self.__scstats_avg_num_of_users)+'\\u00B1'+str(self.__scstats_std_num_of_users)+'['+str(self.__scstats_cv_num_of_users)+']'\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Dictionary\n",
    "        self.scs_data = {'dataset':self.__scstats_dataset,'metric':self.__scstats_metric,'cc_reference':ref,\n",
    "                     'num_of_candidates':self.__scstats_num_of_cc,'num_of_clusters':len(self.perf_cc_clusters),\n",
    "                     'avg_std_cv_rows':self.__scstats_str_rows,\n",
    "                     'avg_std_cv_cost':self.__scstats_str_cost,\n",
    "                     'num_of_groupped_elements':len(self.unique_elements_grouped),\n",
    "                     'avg_std_cv_seq_len':self.__scstats_str_seq_len,\n",
    "                     'avg_std_cv_relative_rows_compression':self.__scstats_str_relative_compress,\n",
    "                     'avg_std_cv_num_of_users':self.__scstats_str_num_of_users,\n",
    "                     'overall_entropy':self.overall_entropy}\n",
    "        \n",
    "        with open('./coclustering_file_outputs/df_clustering_stats.csv','a+',newline='',encoding='utf8') as self.fscs_object:\n",
    "      \n",
    "            # Pass the file object and a list \n",
    "            # of column names to DictWriter()\n",
    "            # You will get a object of DictWriter\n",
    "            self.scs_dictwriter_object = DictWriter(self.fscs_object, delimiter=\";\", fieldnames=self.scs_field_names)\n",
    "\n",
    "            #Pass the dictionary as an argument to the Writerow()\n",
    "            self.scs_dictwriter_object.writerow(self.scs_data)\n",
    "\n",
    "            #Close the file object\n",
    "            self.fscs_object.close()\n",
    "    \n",
    "    def set_variables(self,num_objs):\n",
    "        self.perf_df_clustering_output_measures = pd.DataFrame(columns = ['Iteration_i','Candidate_iteration_k',\n",
    "                                                                          'Candidate_cost'])\n",
    "        self.total_num_of_objs_df = num_objs\n",
    "    \n",
    "    ### descontinuado\n",
    "    def append_result(self,it_i,cc_it_k,cc_cost):\n",
    "        self.perf_df_clustering_output_measures = self.perf_df_clustering_output_measures.append({'Iteration_i':int(it_i),\n",
    "                                                                                        'Candidate_iteration_k':'Candidate_'+str(cc_it_k),\n",
    "                                                                                        'Candidate_cost':float(cc_cost)},\n",
    "                                                                                        ignore_index=True)\n",
    "    def plot_cost(self):\n",
    "        '''\n",
    "        Method to show the cost function value along the iterations.\n",
    "        '''\n",
    "#         print(self.df_clustering_output_measures.head())\n",
    "#         self.df_clustering_output_measures['Cocluster_cost'] = self.df_clustering_output_measures['Cocluster_cost'] / self.df_clustering_output_measures['Cocluster_cost'].abs().max()\n",
    "#         print(self.df_clustering_output_measures.head())\n",
    "        a4_dims = (11.7, 8.27)\n",
    "        fig, ax = plt.subplots(figsize=a4_dims)\n",
    "        sns.lineplot(data=self.perf_df_clustering_output_measures, x=\"Iteration_i\", y=\"Candidate_cost\"\n",
    "                     , hue=\"Candidate_iteration_k\")#, style=\"Cluster_iteration_k\", markers=True, dashes=False)\n",
    "        \n",
    "        if self.perf_df_clustering_output_measures['Candidate_iteration_k'].nunique() > 15:\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "#         self.plt.show()\n",
    "\n",
    "    def summary_clusters(self, cc_dict, map_id_to_att,trajs_data_dict_list):\n",
    "        '''\n",
    "        Method to map back the attributes to its original value and put it avaible as final result visualization.\n",
    "        '''\n",
    "        self.perf_cc_clusters = {}\n",
    "        self.num_of_trajs_per_cluster = []\n",
    "        self.cost_value_per_cluster = []\n",
    "        self.seq_len_per_cluster = []\n",
    "        self.unique_elements_grouped = []\n",
    "        \n",
    "        for cluster_k,value in cc_dict.items():\n",
    "#             print('Cluster ',cluster_k)\n",
    "            remap_seq_output = []\n",
    "            for att_id in cc_dict[cluster_k]['cc_atts'].split('-'):\n",
    "                remap_seq_output.append(map_id_to_att[att_id])\n",
    "            \n",
    "            for el in remap_seq_output:\n",
    "                if el not in self.unique_elements_grouped:\n",
    "                    self.unique_elements_grouped.append(el)\n",
    "            \n",
    "            self.seq_len_per_cluster.append(len(remap_seq_output))\n",
    "            sequence = '-'.join(remap_seq_output).strip()\n",
    "            self.num_of_trajs_per_cluster.append(len(cc_dict[cluster_k]['cc_objs']))\n",
    "            self.cost_value_per_cluster.append(cc_dict[cluster_k]['cc_cost'])\n",
    "            \n",
    "            self.perf_cc_clusters.update({cluster_k:{'cc_atts':sequence,'cc_objs':cc_dict[cluster_k]['cc_objs'],\n",
    "                                                     'cc_cost':cc_dict[cluster_k]['cc_cost'],\n",
    "                                                     'cc_over_coef':cc_dict[cluster_k]['cc_over_coef']}})\n",
    "#             self.set_of_objects = self.set_of_objects.union(set(cc_dict[cluster_k]['cc_objs']))\n",
    "        \n",
    "#         self.perf_cc_clusters.update({'num_of_objects': len(self.set_of_objects)})\n",
    "        print('### Clustering statistics ###')\n",
    "        print('Number of co-clusters: ',len(self.perf_cc_clusters))\n",
    "        self.__avg_rows = np.round(np.mean(self.num_of_trajs_per_cluster),3)\n",
    "        self.__std_rows = np.round(np.std(self.num_of_trajs_per_cluster),3)\n",
    "        self.__cv_rows = np.round((self.__std_rows/self.__avg_rows)*100,3)\n",
    "        self.__avg_cost = np.round(np.mean(self.cost_value_per_cluster),3)\n",
    "        self.__std_cost = np.round(np.std(self.cost_value_per_cluster),3)\n",
    "        self.__cv_cost = np.round((self.__std_cost/self.__avg_cost)*100,3)\n",
    "        print('AVG rows:{:.2f}\\u00B1{:.2f}[CV:{:.2f}%], AVG cost:{:.2f}\\u00B1{:.2f}[CV:{:.2f}%]'.format(self.__avg_rows,\n",
    "                                                                                                        self.__std_rows,\n",
    "                                                                                                        self.__cv_rows,\n",
    "                                                                                                        self.__avg_cost,\n",
    "                                                                                                        self.__std_cost,\n",
    "                                                                                                        self.__cv_cost))\n",
    "        self.__avg_seq_len = np.round(np.mean(self.seq_len_per_cluster),3)\n",
    "        self.__std_seq_len = np.round(np.std(self.seq_len_per_cluster),3)\n",
    "        self.__cv_seq_len = np.round((self.__std_seq_len/self.__avg_seq_len)*100,3)\n",
    "        print('AVG sequece length:{:.2f}\\u00B1{:.2f}[CV:{:.2f}%]'.format(self.__avg_seq_len,\n",
    "                                                                         self.__std_seq_len,self.__cv_seq_len))\n",
    "        print('Number of unique elements grouped: '+str(len(self.unique_elements_grouped)))\n",
    "        \n",
    "        \n",
    "        if VERBOSE:\n",
    "            for key, value in self.perf_cc_clusters.items():\n",
    "                print('Co-cluster-{}, Sequence: {}, Num of trajs: {}, Cost: {}'.format(key,\n",
    "                                                                           self.perf_cc_clusters[key]['cc_atts'],\n",
    "                                                                           len(self.perf_cc_clusters[key]['cc_objs']),\n",
    "                                                                           self.perf_cc_clusters[key]['cc_cost']))\n",
    "#         self.perf_cc_clusters.update({'num_of_objects': len(trajs_data_dict_list)})\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        '''\n",
    "        Method to show the found co-clusters as follows:\n",
    "        1. It shows the current co-cluster K with the absolute number of objects into it and the relative number regarding\n",
    "        the total number of objects in the dataset;\n",
    "        2. It shows the co-cluster sequence of elements and the objects containing it.\n",
    "        '''\n",
    "        self.__it_k = 0\n",
    "        for cluster_k, value in self.perf_cc_clusters.items():\n",
    "            if cluster_k != 'num_of_objects':\n",
    "                self.__it_k += 1\n",
    "                self.relative = len(self.perf_cc_clusters[cluster_k]['cc_objs'])/self.total_num_of_objs_df\n",
    "                print('Cluster #'+str(self.__it_k)+' - Candidate {0} [Absolute:{1} | Relative:{2:2.2f} | Cost: {3:} | Ov_coef: {4:1.2f} | Seq length: {5}]'.format(\n",
    "                                                                                               cluster_k,\n",
    "                                                                                               len(self.perf_cc_clusters[cluster_k]['cc_objs']),\n",
    "                                                                                               self.relative,\n",
    "                                                                                               self.perf_cc_clusters[cluster_k]['cc_cost'],\n",
    "                                                                                               self.perf_cc_clusters[cluster_k]['cc_over_coef'],\n",
    "                                                                                               len(self.perf_cc_clusters[cluster_k]['cc_atts'].split('-'))))\n",
    "                \n",
    "                if len(self.perf_cc_clusters[cluster_k]['cc_objs']) < 10:\n",
    "                    print('Attributes sequence \"{}\" and trajectories \"{}\".'.format(self.perf_cc_clusters[cluster_k]['cc_atts'],\n",
    "                                                                                   str(self.perf_cc_clusters[cluster_k]['cc_objs']).strip('{}')))\n",
    "                else:\n",
    "                    print('Attributes sequence \"{}\" and trajectories \"{},[...]\".'.format(self.perf_cc_clusters[cluster_k]['cc_atts'],\n",
    "                                                                                str(list(self.perf_cc_clusters[cluster_k]['cc_objs'])[0:8]).strip('[]')))\n",
    "                print('')\n",
    "    \n",
    "    def create_alluvial_diagram(self):\n",
    "        # Function to create the CSV file that contains the data to generate the Sankey diagram\n",
    "        \n",
    "        if self.__sdst_dataset == 'All users':\n",
    "            df_traj_user = pd.read_csv('./data/real_application/foursquare_NY/fs_ny_top_users_193.csv', sep=';')\n",
    "            self.__file_alluvial_csv = './coclustering_file_outputs/df_build_alluvial_top193.csv'\n",
    "        elif self.__sdst_dataset == 'Top 10 users':\n",
    "            df_traj_user = pd.read_csv('./data/real_application/foursquare_NY/fs_ny_top_users_10.csv', sep=';')\n",
    "            self.__file_alluvial_csv = './coclustering_file_outputs/df_build_alluvial_top10.csv'\n",
    "        elif self.__sdst_dataset == 'Top 81 users':\n",
    "            df_traj_user = pd.read_csv('../data/real_application/foursquare_NY/fs_ny_top_users_81.csv', sep=';')\n",
    "            self.__file_alluvial_csv = './coclustering_file_outputs/df_build_alluvial_top81.csv'\n",
    "        elif (self.__sdst_dataset == 'SJGS') or (self.__sdst_dataset == 'SJGS2'):\n",
    "            df_traj_user = pd.read_csv('./data/real_application/gene_sequences/SJGS/splice_data.csv', sep=';')\n",
    "            self.__file_alluvial_csv = './coclustering_file_outputs/df_build_alluvial_'+str(self.__sdst_dataset.lower())+'.csv'\n",
    "    \n",
    "    #     df_traj_user.drop(columns=['tid','lat_lon','time','day','type','root_type','rating','weather'],inplace=True)\n",
    "        df_traj_user = df_traj_user[['new_tid','label']]\n",
    "        #     print(df_traj_user.head())\n",
    "\n",
    "        traj_id = 50\n",
    "        user_label = df_traj_user[df_traj_user['new_tid'] == traj_id]['label'].unique()[0]\n",
    "        print('Trajectory \"{}\" belongs to User: \"{}\"'.format(traj_id,user_label))\n",
    "        \n",
    "        self.__max_seq_len = 0\n",
    "        for cluster_k, value in self.perf_cc_clusters.items():\n",
    "            self.__tmp = len(self.perf_cc_clusters[cluster_k]['cc_atts'].split('-'))\n",
    "            if self.__tmp > self.__max_seq_len:\n",
    "                self.__max_seq_len = self.__tmp\n",
    "        \n",
    "        print('Max sequence length: '+str(self.__max_seq_len))\n",
    "        \n",
    "        self.__levels = ['lvl'+str(i) for i in range(1,self.__max_seq_len+3)]\n",
    "        print('Levels: '+str(self.__levels))\n",
    "        self.__columns_df_alluvial = self.__levels\n",
    "        self.__columns_df_alluvial.append('count')\n",
    "        self.__columns_df_alluvial.append('cluster')\n",
    "        print('Columns alluvial df: '+str(self.__columns_df_alluvial))\n",
    "        \n",
    "        self.__df_alluvial = pd.DataFrame(columns = self.__columns_df_alluvial)    \n",
    "        self.__df_alluvial.to_csv(self.__file_alluvial_csv,index=False,sep=';')\n",
    "        \n",
    "        self.__alluvial_field_names = self.__columns_df_alluvial\n",
    "        self.__iter_k = 0\n",
    "        for cluster_k, value in self.perf_cc_clusters.items():       \n",
    "            self.__iter_k += 1\n",
    "            self.__tmp_seq = self.perf_cc_clusters[cluster_k]['cc_atts'].split('-')\n",
    "            self.__alluvial_data = dict.fromkeys(self.__columns_df_alluvial)\n",
    "            for traj_id in self.perf_cc_clusters[cluster_k]['cc_objs']:\n",
    "                user_label = df_traj_user[df_traj_user['new_tid'] == int(traj_id)]['label'].unique()[0]\n",
    "                self.__alluvial_data['lvl1'] = 'U-'+str(user_label)\n",
    "                self.__alluvial_data['lvl2'] = traj_id\n",
    "                \n",
    "                for element_level in range(0,len(self.__tmp_seq)):\n",
    "                    self.__alluvial_data['lvl'+str(element_level+3)] = self.__tmp_seq[element_level]\n",
    "                self.__alluvial_data['count'] = 1\n",
    "                self.__alluvial_data['cluster'] = 'Cluster-'+str(self.__iter_k)\n",
    "                \n",
    "                with open(self.__file_alluvial_csv, 'a+', newline='') as self.__alluvial_object:\n",
    "      \n",
    "                    # Pass the file object and a list \n",
    "                    # of column names to DictWriter()\n",
    "                    # You will get a object of DictWriter\n",
    "                    self.__alluvial_dictwriter_object = DictWriter(self.__alluvial_object, delimiter=\";\",\n",
    "                                                                   fieldnames=self.__alluvial_field_names)\n",
    "\n",
    "                    #Pass the dictionary as an argument to the Writerow()\n",
    "                    self.__alluvial_dictwriter_object.writerow(self.__alluvial_data)\n",
    "\n",
    "                    #Close the file object\n",
    "                    self.__alluvial_object.close()\n",
    "        \n",
    "        ### plot alluvial ###\n",
    "        '''It is ploted out of this process. Check the supplement material.'''\n",
    "#         self.__df_alluvial = pd.read_csv(self.__file_alluvial_csv,sep=';')\n",
    "#         self.__fig = self.__genSankey(self.__df_alluvial,cat_cols=self.__levels,value_cols='count')\n",
    "#         plotly.offline.plot(self.__fig, validate=False)\n",
    "            \n",
    "    def __genSankey(self,df,cat_cols=[],value_cols='',title='Sankey Diagram'):\n",
    "        # old generate sankey. it works well with no repeated variable values\n",
    "        self.__dpi=300\n",
    "        self.__board = plt.figure(figsize=(3, 2),dpi=self.__dpi)\n",
    "        # maximum of 6 value cols -> 6 colors\n",
    "#         colorPalette = ['#4B8BBE','#306998','#FFE873','#FFD43B','#646464']\n",
    "        # maximum of 20 value cols -> 20 colors\n",
    "        colorPalette = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c', '#98df8a', '#d62728', '#ff9896',\n",
    "                        '#9467bd', '#c5b0d5', '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f', '#c7c7c7',\n",
    "                        '#bcbd22', '#dbdb8d','#17becf', '#9edae5']\n",
    "        labelList = []\n",
    "        colorNumList = []\n",
    "        for catCol in cat_cols:\n",
    "            labelListTemp =  list(set(df[catCol].values))\n",
    "            colorNumList.append(len(labelListTemp))\n",
    "            labelList = labelList + labelListTemp\n",
    "\n",
    "        # remove duplicates from labelList\n",
    "        labelList = list(dict.fromkeys(labelList))\n",
    "\n",
    "        # define colors based on number of levels\n",
    "        colorList = []\n",
    "        for idx, colorNum in enumerate(colorNumList):\n",
    "            colorList = colorList + [colorPalette[idx]]*colorNum\n",
    "\n",
    "        # transform df into a source-target pair\n",
    "        for i in range(len(cat_cols)-1):\n",
    "            if i==0:\n",
    "                sourceTargetDf = df[[cat_cols[i],cat_cols[i+1],value_cols]]\n",
    "                sourceTargetDf.columns = ['source','target','count']\n",
    "            else:\n",
    "                tempDf = df[[cat_cols[i],cat_cols[i+1],value_cols]]\n",
    "                tempDf.columns = ['source','target','count']\n",
    "                sourceTargetDf = pd.concat([sourceTargetDf,tempDf])\n",
    "            sourceTargetDf = sourceTargetDf.groupby(['source','target']).agg({'count':'sum'}).reset_index()\n",
    "\n",
    "        # add index for source-target pair\n",
    "        sourceTargetDf['sourceID'] = sourceTargetDf['source'].apply(lambda x: labelList.index(x))\n",
    "        sourceTargetDf['targetID'] = sourceTargetDf['target'].apply(lambda x: labelList.index(x))\n",
    "\n",
    "        # creating the sankey diagram\n",
    "        data = dict(\n",
    "            type='sankey',\n",
    "            node = dict(\n",
    "              pad = 10,\n",
    "              thickness = 15,\n",
    "              line = dict(\n",
    "                color = \"black\",\n",
    "                width = 0.5\n",
    "              ),\n",
    "              label = labelList,\n",
    "              color = colorList\n",
    "            ),\n",
    "            link = dict(\n",
    "              source = sourceTargetDf['sourceID'],\n",
    "              target = sourceTargetDf['targetID'],\n",
    "              value = sourceTargetDf['count']\n",
    "            )\n",
    "          )\n",
    "\n",
    "        layout =  dict(\n",
    "            title = title,\n",
    "#             height = 372,\n",
    "#             width = 550,\n",
    "            font = dict(\n",
    "              size = 10\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #     fig = go.Figure(data = [go.Sankey(data,layout)])\n",
    "#         fig = go.Figure(data = [go.Sankey(data)])\n",
    "        fig = dict(data=[data], layout=layout)\n",
    "        return fig\n",
    "    \n",
    "    def store_dist(self,set_of_candidates):\n",
    "        self.set_of_candidates = set_of_candidates\n",
    "    \n",
    "    def __get_entropy_purity(self):\n",
    "        print('Overall entropy H: '+str(self.overall_entropy))\n",
    "        print('Purity: '+str(self.purity))\n",
    "        self.__gep_avg_relative = np.round(np.mean(self.relative_clusters_value),3)\n",
    "        self.__gep_std_relative = np.round(np.std(self.relative_clusters_value),3)\n",
    "        self.__gep_cv_relative = np.round((self.__gep_std_relative/self.__gep_avg_relative)*100,3)\n",
    "        print('AVG relative co-clusters: {:.2f}\\u00B1{:.2f}[CV:{:.2f}%]'.format(self.__gep_avg_relative,\n",
    "                                                                                self.__gep_std_relative,\n",
    "                                                                                self.__gep_cv_relative))\n",
    "        self.__gep_avg_num_users = np.round(np.mean(self.num_of_users_per_cluster),3)\n",
    "        self.__gep_std_num_users = np.round(np.std(self.num_of_users_per_cluster),3)\n",
    "        self.__gep_cv_num_users = np.round((self.__gep_std_num_users/self.__gep_avg_num_users)*100,3)\n",
    "        print('AVG num. of users: {:.2f}\\u00B1{:.2f}[CV:{:.2f}%]'.format(self.__gep_avg_num_users,\n",
    "                                                                         self.__gep_std_num_users,\n",
    "                                                                         self.__gep_cv_num_users))\n",
    "        print('')\n",
    "    \n",
    "    def calculate_entropy_purity(self, file_dataset):\n",
    "        \n",
    "        self.split = file_dataset.split('.')\n",
    "        if self.split[0] != 'sjgs' or self.split[0] != 'sjgs2' or split[0] != 'splice_data':\n",
    "            if self.split[-1] == 'dat':\n",
    "                self.path = './data/real_application/foursquare_NY/concat_dimensions/'\n",
    "                self.df_trajs_users = create_df_map_traj_user(pd.read_csv(self.path+self.split[0]+'.csv', sep=\";\"))\n",
    "            else:\n",
    "                self.path = './data/real_application/foursquare_NY/concat_dimensions/'\n",
    "                self.df_trajs_users = create_df_map_traj_user(pd.read_csv(self.path+file_dataset, sep=\";\"))\n",
    "        else:\n",
    "            if self.split[-1] == 'dat':\n",
    "                self.path = './data/real_application/gene_sequences/SJGS/'\n",
    "                self.df_trajs_users = create_df_map_traj_user(pd.read_csv(self.path+self.split[0]+'.csv', sep=\";\"))\n",
    "            else:\n",
    "                self.path = './data/real_application/gene_sequences/SJGS/'\n",
    "                self.df_trajs_users = create_df_map_traj_user(pd.read_csv(self.path+file_dataset, sep=\";\"))\n",
    "        \n",
    "        self.relative_clusters_value = []\n",
    "        self.entropy_per_cluster = []\n",
    "        self.num_of_objs_per_cluster = []\n",
    "        self.total_num_of_objs = len(self.df_trajs_users)\n",
    "        self.overall_entropy = 0\n",
    "        self.max_prob_per_cluster = []\n",
    "        self.purity = 0\n",
    "        self.num_of_users_per_cluster = []\n",
    "        \n",
    "        for cluster_k, value in self.perf_cc_clusters.items():\n",
    "            self.users = {}\n",
    "            if cluster_k != 'num_of_objects':\n",
    "                self.num_of_objs_k = len(self.perf_cc_clusters[cluster_k]['cc_objs'])\n",
    "                self.relative_clusters_value.append((self.num_of_objs_k/self.total_num_of_objs)*100)\n",
    "#                 print('Cluster-'+str(cluster_k)+' | # of trajs: '+str(self.num_of_objs_k))\n",
    "                self.trajs = list(map(int,self.perf_cc_clusters[cluster_k]['cc_objs']))\n",
    "                self.df_cluster = self.df_trajs_users[self.df_trajs_users['Tid'].isin(self.trajs)]\n",
    "##                 self.users = self.df_cluster['User'].value_counts().to_dict()\n",
    "                self.users = self.df_cluster['User'].value_counts()\n",
    "                self.num_of_users_per_cluster.append(len(self.users))\n",
    "                self.h_k = entropy(self.users,base=2)\n",
    "##                 self.users = self.users.to_dict()\n",
    "                #print(self.users,end=' | ')\n",
    "                #print(\"Entropy h_k: \"+str(self.h_k))\n",
    "                self.entropy_per_cluster.append(self.h_k)\n",
    "                self.num_of_objs_per_cluster.append(self.num_of_objs_k)\n",
    "##                 self.total_num_of_objs += self.num_of_objs_k\n",
    "##                self.max_prob_per_cluster.append(np.array(list(self.users.values())).max())\n",
    "                self.max_prob_per_cluster.append(list(self.users)[0])\n",
    "        \n",
    "        self.overall_entropy = np.sum((np.array(self.entropy_per_cluster)*\n",
    "                                       (np.array(self.num_of_objs_per_cluster)/self.total_num_of_objs)))\n",
    "        self.purity = np.array(self.max_prob_per_cluster).sum()/self.total_num_of_objs\n",
    "        self.__get_entropy_purity()\n",
    "    \n",
    "    def show_boxplot(self):\n",
    "        '''\n",
    "        Method to show the distribution values of the candidates.\n",
    "        '''\n",
    "        #         array = np.random.uniform(size=20)\n",
    "        self.array = list(self.set_of_candidates.values())\n",
    "        self.ref = ''\n",
    "        if np.mean(self.array) < 0:\n",
    "            self.ref = 'Cost ref'\n",
    "        else:\n",
    "            self.ref = 'Rows ref'\n",
    "        ax = sns.boxplot(data=self.array)\n",
    "        ax = sns.swarmplot(data=self.array, color=\".25\")\n",
    "        plt.xticks([0],[self.ref])\n",
    "#         plt.xlabel(\"Reference\")\n",
    "        plt.ylabel(\"Values\")\n",
    "        plt.title(self.ref+\" distribution\")\n",
    "    #     plt.show(ax)\n",
    "    \n",
    "    def test_norm_dist(self):\n",
    "        import scipy.stats as stats\n",
    "        try:\n",
    "            self.mean = np.mean(list(self.set_of_candidates.values()))\n",
    "            self.std = np.std(list(self.set_of_candidates.values()),ddof=1)\n",
    "            self.dist_values = list(self.set_of_candidates.values())\n",
    "#             self.shapiro_stat, self.shapiro_p_value = stats.shapiro(self.dist_values)\n",
    "#     #         print('O valor da estatística de shapiro-wilk = '+str(self.shapiro_stat))\n",
    "#     #         print('O valor do p-value de shapiro-wilk = '+str(self.shapiro_p_value))\n",
    "#             if self.shapiro_p_value >=0.5:\n",
    "#                 print('Com 95% de confiança, os dados são similares a uma distribuição normal segundo o teste de Shapiro-Wilk.')\n",
    "#             else:\n",
    "#                 print('Com 95% de confiança, os dados NÃO são similares a uma distribuição normal segundo o teste de Shapiro-Wilk.')\n",
    "#             print('')\n",
    "            self.__shapiro_wilk_test()\n",
    "            self.__kolomogorov_smirnov_test()\n",
    "            self.__anderson_darling_test()\n",
    "            print('')\n",
    "        except Exception as inst:\n",
    "            print('Please, test the variable individually.')\n",
    "            print('Error:',inst)\n",
    "#             self.dist_values_rows = []\n",
    "#             self.dist_values_cost = []\n",
    "\n",
    "#             for key,value in self.set_of_candidates.items():\n",
    "#                 self.dist_values_rows.append(self.set_of_candidates[key]['rows'])\n",
    "#                 self.dist_values_cost.append(self.set_of_candidates[key]['cost'])\n",
    "            \n",
    "#             self.shapiro_stat_rows, self.shapiro_p_value_rows = stats.shapiro(self.dist_values_rows)\n",
    "#             self.shapiro_stat_cost, self.shapiro_p_value_cost = stats.shapiro(self.dist_values_cost)\n",
    "    \n",
    "    def __shapiro_wilk_test(self):\n",
    "        self.shapiro_stat, self.shapiro_p_value = stats.shapiro(self.dist_values)\n",
    "#         print('O valor da estatística de shapiro-wilk = '+str(self.shapiro_stat))\n",
    "#         print('O valor do p-value de shapiro-wilk = '+str(self.shapiro_p_value))\n",
    "        if self.shapiro_p_value >=0.5:\n",
    "            print('Segundo o teste de Shapiro-Wilk, com 95% de confiança, os dados são similares a uma distribuição normal.')\n",
    "        else:\n",
    "            print('Segundo o teste de Shapiro-Wilk, com 95% de confiança, os dados NÃO são similares a uma distribuição normal.')\n",
    "    \n",
    "    def __kolomogorov_smirnov_test(self):\n",
    "        self.ks_stat, self.ks_p_value = stats.kstest(self.dist_values,cdf='norm', args=(self.mean,self.std), N=len(self.dist_values))\n",
    "        self.ks_critico = self.__kolmogorov_smirnov_critico(len(self.dist_values))\n",
    "        if self.ks_critico >= self.ks_stat:\n",
    "            print('Segundo o teste Kolomogorov-Smirnov, com 95% de confiança, os dados são similares a uma distribuição normal.')\n",
    "        else:\n",
    "            print('Segundo o teste Kolomogorov-Smirnov, com 95% de confiança, os dados NÃO são similares a uma distribuição normal.')\n",
    "    # Checking the critical value of the Kolmogorov-Smirnov test\n",
    "    def __kolmogorov_smirnov_critico(self,n):\n",
    "        # table of critical values for the kolmogorov-smirnov test - 95% confidence\n",
    "        # Source: https://www.soest.hawaii.edu/GG/FACULTY/ITO/GG413/K_S_Table_one_Sample.pdf\n",
    "        # Source: http://www.real-statistics.com/statistics-tables/kolmogorov-smirnov-table/\n",
    "        # alpha = 0.05 (95% confidential level)\n",
    "\n",
    "        if n <= 40:\n",
    "            # valores entre 1 e 40\n",
    "            self.kolmogorov_critico = [0.97500, 0.84189, 0.70760, 0.62394, 0.56328, 0.51926, 0.48342, 0.45427, 0.43001, 0.40925, \n",
    "                          0.39122, 0.37543, 0.36143, 0.34890, 0.33760, 0.32733, 0.31796, 0.30936, 0.30143, 0.29408, \n",
    "                          0.28724, 0.28087, 0.27490, 0.26931, 0.26404, 0.25907, 0.25438, 0.24993, 0.24571, 0.24170, \n",
    "                          0.23788, 0.23424, 0.23076, 0.22743, 0.22425, 0.22119, 0.21826, 0.21544, 0.21273, 0.21012]\n",
    "            self.ks_critico = self.kolmogorov_critico[n - 1]\n",
    "        elif n > 40:\n",
    "            # valores acima de 40:\n",
    "            self.kolmogorov_critico = 1.36/(np.sqrt(n))\n",
    "            self.ks_critico = self.kolmogorov_critico\n",
    "        else:\n",
    "            pass            \n",
    "\n",
    "        return self.ks_critico\n",
    "    \n",
    "    def __anderson_darling_test(self):\n",
    "        self.ad_stat, self.ad_critico, self.ad_teorico = stats.anderson(self.dist_values,'norm')\n",
    "        if self.ad_stat < self.ad_critico[2]:\n",
    "            print('Segundo o teste de Anderson-Darling, com 95% de confiança, os dados são similares a uma distribuição normal.')\n",
    "        else:\n",
    "            print('Segundo o teste de Anderson-Darling, com 95% de confiança, os dados NÃO são similares a uma distribuição normal.')\n",
    "\n",
    "    def test_skewness(self):\n",
    "        self.dist_values = list(self.set_of_candidates.values())\n",
    "        self.mean = np.mean(self.dist_values)\n",
    "        self.median = np.median(self.dist_values)\n",
    "        vals,counts = np.unique(self.dist_values, return_counts=True)\n",
    "        index = np.argmax(counts)\n",
    "        self.mode = vals[index]\n",
    "        \n",
    "        \n",
    "        if (self.mean == self.median) and (self.mean == self.mode):\n",
    "            print('Distribuição normal | Mean:{} = Median: {} = Mode: {}.'.format(self.mean,self.median,self.mode))\n",
    "        #positive values\n",
    "        if (self.mean < self.median) and (self.median < self.mode):\n",
    "            print('Assimetria à Esquerda (negativa) | Mean:{} < Median: {} < Mode: {}.'.format(self.mean,self.median,self.mode))\n",
    "        if (self.mode < self.median) and (self.median < self.mean):\n",
    "            print('Assimetria à Direita (positiva) | Mode:{} < Median: {} < Mean: {}.'.format(self.mode,self.median,self.mean))\n",
    "        #negativa values\n",
    "        if self.mean < 0:\n",
    "            print('mean:',self.mean,' median:',self.median,' mode:',self.mode)\n",
    "            if (self.mean > self.median) and (self.median > self.mode):\n",
    "                print('Assimetria à Esquerda (negativa) | Mean:{} < Median: {} < Mode: {}.'.format(self.mean,self.median,self.mode))\n",
    "            if (self.mode > self.median) and (self.median > self.mean):\n",
    "                print('Assimetria à Direita (positiva) | Mode:{} < Median: {} < Mean: {}.'.format(self.mode,self.median,self.mean))\n",
    "        print('')\n",
    "\n",
    "\n",
    "\n",
    "#### Modificação do candidate deviation método. Esta modificação é para fazer os cálculos das medidas sem ter que\n",
    "#### recalcular os candidatos no mesmo dataset. Os candidatos não mudam no modo de descoberta automática.\n",
    "# def short_path_metrics(ref,value_ref,set_of_clusters,cc_type_process='incremental'):\n",
    "def short_path_metrics(ref,cc_type_analysis,value_ref,set_of_clusters,cc_type_process='incremental',cc_z_threshold=None):\n",
    "    '''\n",
    "    Method to return the avg number of the reference in the set of co-clusters.\n",
    "    If the set is bigger than 1 it calculates the avg, otherwise it is 0.\n",
    "    Parameters:\n",
    "        ref_analysis: 1. index_rows_set -> considers the rows; 2. cost_function -> considers the cost.\n",
    "        test_value: The value to test.\n",
    "        set_of_clusters: The current set of co-clusters containing its values for the ref_analysis    \n",
    "    '''\n",
    "   \n",
    "    if len(set_of_clusters) >= 2:\n",
    "        try:# single ref\n",
    "            mean = np.mean(list(set_of_clusters.values()))\n",
    "            std = np.std(list(set_of_clusters.values()))\n",
    "        except:# double ref\n",
    "            sum_rows = []\n",
    "            sum_cost = []\n",
    "            for key,value in set_of_clusters.items():\n",
    "                sum_rows.append(set_of_clusters[key]['rows'])\n",
    "                sum_cost.append(set_of_clusters[key]['cost'])\n",
    "            mean_rows = np.mean(sum_rows)\n",
    "            mean_cost = np.mean(sum_cost)\n",
    "            std_rows = np.std(sum_rows)\n",
    "            std_cost = np.std(sum_cost)\n",
    "        \n",
    "        if cc_type_process == 'incremental':\n",
    "            if ref == \"rows\":\n",
    "                ### normal mean\n",
    "                if cc_type_analysis == 'mean':\n",
    "                    return len(value_ref['index_rows_set']) >= np.floor(mean)\n",
    "                else:\n",
    "                ### z-score: we consider values greater than -1 once it is a positive distribution\n",
    "                    try:\n",
    "                        z = (len(value_ref['index_rows_set'])-mean)/std\n",
    "                    except:\n",
    "                        z = (value_ref-mean)/std\n",
    "                    print('Z-score(rows): ',z)\n",
    "                    return z >= cc_z_threshold\n",
    "            elif ref == \"cost\":\n",
    "                ### normal mean\n",
    "                if cc_type_analysis == 'mean':\n",
    "                    return value_ref['cost_function'] <= np.ceil(mean)\n",
    "                else:\n",
    "                ### z-score: we consider values smaller than 1 once it is a negative distribution\n",
    "                    try:\n",
    "                        z = (value_ref['cost_function']-mean)/std\n",
    "                    except:\n",
    "                        z = (value_ref-mean)/std\n",
    "                    print('Z-score(cost): ',z)\n",
    "                    return z <= -cc_z_threshold\n",
    "            else:#combine\n",
    "                ### normal mean\n",
    "                if cc_type_analysis == 'mean':\n",
    "#                     print('Mean(rows):',mean_rows,' Mean(cost):',mean_cost)\n",
    "                    return ((len(value_ref['index_rows_set']) >= np.floor(mean_rows)) or \n",
    "                            (value_ref['cost_function'] <= np.ceil(mean_cost)))\n",
    "                else:\n",
    "                ### z-score\n",
    "                    z_rows = (len(value_ref['index_rows_set'])-mean_rows)/std_rows\n",
    "                    z_cost = (value_ref['cost_function']-mean_cost)/std_cost\n",
    "#                     print('Z-score(rows): ',z_rows,' Z-score(cost): ',z_cost)\n",
    "                    return ((z_rows >= -cc_z_threshold) or (z_cost <= cc_z_threshold))\n",
    "                \n",
    "        elif cc_type_process == 'sample':\n",
    "            candidates_to_remove = []\n",
    "            \n",
    "            if ref != 'combine':\n",
    "#             try:# single ref: rows OR cost\n",
    "#                 mean\n",
    "                if cc_type_analysis == 'mean':\n",
    "                    for key,value in set_of_clusters.items():\n",
    "#                         print('Candidate-'+key+' Mean:',mean,' Value ref:',value,end='')\n",
    "                        if ref == 'rows' and set_of_clusters[key]['rows'] < mean_rows:\n",
    "#                             print(' -> Remove')\n",
    "                            candidates_to_remove.append(key)\n",
    "                        elif ref == 'cost' and set_of_clusters[key]['cost'] > mean_cost:\n",
    "#                             print(' -> Remove')\n",
    "                            candidates_to_remove.append(key)\n",
    "                        else:\n",
    "#                             print(' -> Keep')\n",
    "                            pass\n",
    "                else:#z-score\n",
    "                    for key,value in set_of_clusters.items():\n",
    "#                         z = (value-mean)/std\n",
    "#                         print('Candidate-'+key+' Z-score:',z,end='')\n",
    "                        if ref == 'rows':\n",
    "                            z = (set_of_clusters[key]['rows']-mean_rows)/std_rows\n",
    "                            if z < cc_z_threshold:\n",
    "                                candidates_to_remove.append(key)\n",
    "#                             print(' -> Remove')\n",
    "                        elif ref == 'cost':\n",
    "                            z = (set_of_clusters[key]['cost']-mean_cost)/std_cost\n",
    "                            if z > -cc_z_threshold:\n",
    "                                candidates_to_remove.append(key)\n",
    "#                             print(' -> Remove')\n",
    "                        else:\n",
    "#                             print(' -> Keep')\n",
    "                            pass\n",
    "\n",
    "            else:\n",
    "#             except:#double ref combine: rows AND cost\n",
    "                \n",
    "                if cc_type_analysis == 'mean':\n",
    "                    for key,value in set_of_clusters.items():\n",
    "#                         print('Candidate-'+key+' Mean(rows):',mean_rows,' Mean(cost):',mean_cost,end='')\n",
    "                        if (set_of_clusters[key]['rows'] < mean_rows) and (set_of_clusters[key]['cost'] > mean_cost):\n",
    "#                             print(' -> Remove')\n",
    "                            candidates_to_remove.append(key)\n",
    "                        else:\n",
    "#                             print(' -> Keep')\n",
    "                            pass\n",
    "\n",
    "                else:#z-score\n",
    "                    for key,value in set_of_clusters.items():\n",
    "                        z_rows = (set_of_clusters[key]['rows']-mean_rows)/std_rows\n",
    "                        z_cost = (set_of_clusters[key]['cost']-mean_cost)/std_cost\n",
    "#                         print('Candidate-'+key+' Z-score(row):',z_rows,' Z-score(cost):',z_cost,end='')\n",
    "                        if (z_rows < cc_z_threshold) and (z_cost > -cc_z_threshold):\n",
    "#                             print(' -> Remove')\n",
    "                            candidates_to_remove.append(key)\n",
    "                        else:\n",
    "#                             print(' -> Keep')\n",
    "                            pass\n",
    "            \n",
    "#             return candidates_to_remove  \n",
    "#             print(\"Remove candidates: \",candidates_to_remove)\n",
    "#             print(\"Number of candidates to remove: \",len(candidates_to_remove))\n",
    "            tmp_set_of_candidates = value_ref.copy()\n",
    "            for candidate in candidates_to_remove:#at this point, value_ref is the set of candidates\n",
    "#                 del value_ref[candidate]\n",
    "                del tmp_set_of_candidates[candidate]\n",
    "            return tmp_set_of_candidates\n",
    "\n",
    "        else: # just pass step. Storing the candidate co-clusters to analyze them with sample analysis if desirable\n",
    "            return True               \n",
    "                \n",
    "    else:# pass step to reach a minimum number of elements to perform computation\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show_boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_boxplot():\n",
    "    array = np.random.uniform(size=20)\n",
    "    ax = sns.boxplot(data = array)\n",
    "    ax = sns.swarmplot(data=array, color=\".25\")\n",
    "    plt.xticks([0],['SDF'])\n",
    "    plt.xlabel(\"Reference\")\n",
    "#     plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {}\n",
    "f['1'] = {}\n",
    "f['1'].update({'cc_objs':[]})\n",
    "f['1'].update({'cc_atts':[]})\n",
    "f['1'].update({'cc_elements':[]})\n",
    "print(f)\n",
    "print(f['1']['cc_objs'])\n",
    "f.update({'2':{}})\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_queue_s(cocluster_sequence_str, tested_sequence_str, s_poi_freq_queue_list, poi_node_queue):\n",
    "def update_queue_s(cocluster_sequence_str, s_poi_freq_queue_list, poi_node_queue):\n",
    "    '''\n",
    "    Method to update the nodes in queue s. It decrements the value of a given node in s.\n",
    "    The input are:\n",
    "        1. The current string sequence of a cocluster;\n",
    "        2. The tested string sequence to improve a cocluster;\n",
    "        3. The queue s;\n",
    "        4. A single node of queue s.\n",
    "    '''\n",
    "    # update list s when the first sequence is identified\n",
    "#     if cocluster_sequence_str == '':\n",
    "    tmp_split = cocluster_sequence_str.split('-')\n",
    "    if len(tmp_split) == 2:\n",
    "#         tmp_split = tested_sequence_str.split('-')\n",
    "        s_poi_freq_queue_list.append(poi_node_queue)\n",
    "        for attribute in tmp_split:\n",
    "            for node_s in s_poi_freq_queue_list:\n",
    "                if attribute == node_s[0]:\n",
    "                    node_s[1] -= 1\n",
    "                    if node_s[1] <= 0: # all occurences were used, then remove the element from the queue\n",
    "                        print('Element with 0 removed.')\n",
    "                        s_poi_freq_queue_list.remove(node_s)\n",
    "                    break\n",
    "    else: # update a single node in case a sequence is already discovered\n",
    "        poi_node_queue[1] -= 1\n",
    "        if poi_node_queue[1] <= 0: # all occurences were used, then remove the element from the queue\n",
    "            s_poi_freq_queue_list.remove(node_s)\n",
    "        else:\n",
    "            s_poi_freq_queue_list.append(poi_node_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_uppercase_S(cc_atts, cc_objs, S_dict):\n",
    "    '''\n",
    "    Method to update the dictonary S. It decrements the frequency of the given attributes in S.\n",
    "    S is updated regarding the frequency of an attribute times the number of objects that it appears in a\n",
    "    co-cluster.\n",
    "    E.g., Given a co-clsuter with sequence Home-Work-Home with 5 trajectories. Then, in S, Home is \n",
    "    decremented with value 10 (2*5) and Work with value 5 (1*5).\n",
    "    \n",
    "    The input are:\n",
    "        1. Co-cluster attributes;\n",
    "        2. Co-cluster objects;\n",
    "        3. The dictionary S of attributes and its frequency.\n",
    "    '''\n",
    "    tmp_dict = {}\n",
    "    for attribute in cc_atts: # groups repeation\n",
    "#         S_poi_freq_dict[attribute] -= 1\n",
    "        try:\n",
    "            tmp_dict[attribute] += 1\n",
    "        except:\n",
    "            tmp_dict.update({attribute:1})\n",
    "    for attribute, value in tmp_dict.items():\n",
    "        S_dict[attribute] -= (tmp_dict[attribute]*len(cc_objs))\n",
    "        if S_dict[attribute] <= 0:\n",
    "            S_dict.pop(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = ['a','b','c','d']\n",
    "a2 = [1,2,3,4]\n",
    "[str(i)+str(j) for i in a1 for j in a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,1,1,0,0,0,0])\n",
    "b = np.array([1,1,3,0,0,0,0])\n",
    "c = np.outer(a,b)\n",
    "print(c)\n",
    "d = (c*0)+1\n",
    "print(d)\n",
    "print(c+d)\n",
    "e = d*4\n",
    "print(e)\n",
    "f = d-e\n",
    "print(f)\n",
    "print(sum(sum(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(e):\n",
    "    return e[:][1]\n",
    "\n",
    "er = [['f',2],['h',5],['t',1]]\n",
    "print(er)\n",
    "er.sort()\n",
    "print(er)\n",
    "er.sort(reverse=True, key=myFunc)\n",
    "print(er)\n",
    "er[0][1] -= 1\n",
    "print(er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= [1,2,3,4,5,6,7,8]\n",
    "print(t)\n",
    "push_to_end = 3\n",
    "complete_cicle = False\n",
    "reload = True\n",
    "print(t.pop(2))\n",
    "print(t)\n",
    "t= [1,2,3,4,5,6,7,8]\n",
    "print(t)\n",
    "# while(reload and complete_cicle != True):\n",
    "#     for i in range(len(t)):\n",
    "#         if t[i] == push_to_end:\n",
    "#             tmp = t.pop(i)\n",
    "#             t.append(tmp)\n",
    "#             complete_cicle == True\n",
    "#         else:\n",
    "#             print(t[i])\n",
    "#         if push_to_end == t[i] and complete_cicle == True:\n",
    "#             reload = False\n",
    "#             break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_data):\n",
    "    '''\n",
    "    This method will assign the variables used by the algorithm.\n",
    "    \n",
    "    INPUT\n",
    "        input_data: A panda dataframe of the input data file.\n",
    "    \n",
    "    OUTPUT\n",
    "        D: A binary matrix from the input data.\n",
    "        N: A noise binary matrix with the same size of D.\n",
    "        data_dict: A dictionary to store D as a vertical representation.\n",
    "        data_res_dict: A copy of data_dict used to sort the attributes of D and find unconvered elements.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    data_pd = input_data #txt file with sequence of check-ins (POI)\n",
    "    frequence_per_poi_dict = {} # store the frequence of a POI as \"POI\": num_of_occurrences\n",
    "    poi_at_trajs_dict_set = {}  # store a set with each index line (tid trajectory) that contains a given POI.\n",
    "                            # \"POI\": set(0,1,4,...); It is the S variable\n",
    "#     global data_res_dict\n",
    "    uncover_poi_dict = {} # It is the s* variable\n",
    "#     global D # input data as a binary matrix\n",
    "#     global N # noise matrix with the same size of D\n",
    "    num_of_objects = 0\n",
    "    num_of_attributes = 0\n",
    "    map_id_to_attribute = {} # map the \n",
    "    map_attribute_to_id = {} # map the\n",
    "    trajectory_dict = {} # it stores the trajectories with its check-ins. \"TID\": [POI1,POI2,...]\n",
    "#     max_val_att = 0 \n",
    "    att_id = 0 # assign an ID to each attribute\n",
    "    \n",
    "    # read each line\n",
    "    for index, row in data_pd.iterrows():\n",
    "        num_of_objects+=1\n",
    "        object_data = row[0].split(\" \")\n",
    "#         trajectory_dict[str(index)] = {}\n",
    "#         trajectory_dict[str(index)] = object_data\n",
    "        trajectory_dict[str(index)] = []\n",
    "        \n",
    "#         for attribute in object_data: # we look at each item of the given transaction\n",
    "        for att_j in range(len(object_data)): # we look at each item of the given transaction\n",
    "            attribute = object_data[att_j]\n",
    "            \n",
    "            if attribute != \"\":\n",
    "#                 if int(attribute) > max_val_att:\n",
    "#                     max_val_att = int(attribute)\n",
    "#                 if attribute not in map_unique_attributes_dataset:\n",
    "#                 if attribute not in map_attribute_to_id.keys():\n",
    "    \n",
    "                if attribute not in map_attribute_to_id: # mapping\n",
    "#                     unique_attributes_dataset.append(attribute)\n",
    "                    map_attribute_to_id[attribute] = str(att_id)\n",
    "                    map_id_to_attribute[str(att_id)] = attribute\n",
    "                    att_id += 1\n",
    "                \n",
    "                # substitute the check-in by its ID\n",
    "                trajectory_dict[str(index)].append(map_attribute_to_id[attribute])\n",
    "                \n",
    "                # store the indeces containing a given POI\n",
    "                if map_attribute_to_id[attribute] in poi_at_trajs_dict_set:\n",
    "#                     data_dict[map_attribute_to_id[attribute]].append(index)\n",
    "                    poi_at_trajs_dict_set[map_attribute_to_id[attribute]].add(str(index))\n",
    "                else:\n",
    "#                     data_dict[map_attribute_to_id[attribute]] = [index]\n",
    "                    poi_at_trajs_dict_set[map_attribute_to_id[attribute]] = set([str(index)])\n",
    "                \n",
    "                # store the frequence for each POI\n",
    "                if map_attribute_to_id[attribute] in frequence_per_poi_dict:\n",
    "                    current_value = frequence_per_poi_dict[map_attribute_to_id[attribute]]\n",
    "                    frequence_per_poi_dict[map_attribute_to_id[attribute]] = current_value + 1\n",
    "                else:\n",
    "                    frequence_per_poi_dict[map_attribute_to_id[attribute]] = 1\n",
    "            \n",
    "                    \n",
    "    uncover_poi_dict = poi_at_trajs_dict_set.copy()\n",
    "#     num_of_attributes = len(data_dict)\n",
    "#     num_of_attributes = max_val_att+1\n",
    "#     num_of_attributes = len(map_attribute_to_id)\n",
    "    print(\"######################################\")\n",
    "    print(\"Number of trajectories: \"+str(index+1))\n",
    "    print(\"Number of unique check-ins: \"+str(len(map_attribute_to_id)))\n",
    "    print(\"########################################\")\n",
    "    if VERBOSE:\n",
    "        print(\"Map_attribute_to_id:\"+str(map_attribute_to_id))\n",
    "        print(\"\")\n",
    "        print(\"Map_id_to_attribute:\"+str(map_id_to_attribute))\n",
    "        print(\"\")\n",
    "        print(\"Frequence_per_poi:\"+str(frequence_per_poi_dict))\n",
    "        print(\"\")\n",
    "        print(\"Trajectories: \"+str(trajectory_dict))\n",
    "        print(\"\")\n",
    "        print(\"POI occurring at trajectories: \"+str(poi_at_trajs_dict_set))\n",
    "        print(\"Get data is DONE!\")\n",
    "        \n",
    "    \n",
    "#     D = np.zeros((num_of_objects,num_of_attributes),dtype=int)\n",
    "#     for key, values in poi_at_trajs_dict.items():\n",
    "#         print(\"key:\"+str(key)+\" Values:\"+str(values))\n",
    "#         for line in values:\n",
    "# #             D[line][int(key)] = 1\n",
    "# #             D[line][map_unique_attributes_dataset[key]] = 1\n",
    "# #             print(line,key)\n",
    "# #             print(type(line),type(key))\n",
    "#             D[line][int(key)] = 1\n",
    "#     N = np.zeros((num_of_objects,num_of_attributes),dtype=int)\n",
    "    \n",
    "#     return D, N, poi_at_trajs_dict, data_res_dict, map_id_to_attribute\n",
    "    return map_id_to_attribute, frequence_per_poi_dict, poi_at_trajs_dict_set, trajectory_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tid  Traj_length  User\n",
      "1    1           81   185\n",
      "3    3           77   185\n",
      "6    6           71   185\n",
      "{185: 3}\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/real_application/foursquare_NY/fs_ny_top_users_10.csv', sep=\";\")\n",
    "df_tmp = create_df_map_traj_user(df)\n",
    "print(df_tmp[df_tmp['Tid'].isin([1,3,6])])\n",
    "e = df_tmp[df_tmp['Tid'].isin([1,3,6])]['User'].value_counts()\n",
    "print(e.to_dict())\n",
    "print(df_tmp[df_tmp['Tid']==1]['User'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_df_map_traj_user(df):\n",
    "def create_df_map_traj_user(df=pd.DataFrame):\n",
    "    '''\n",
    "    Method to support the calculation of the quality result.\n",
    "    It returns a dataframe with the users and their trajectories with its respective length.\n",
    "    '''\n",
    "    try:\n",
    "        #     df = pd.read_csv('data/real_application/foursquare_NY/fs_ny_top_users_10.csv', sep=\";\")\n",
    "        df_map_traj_user = pd.DataFrame(columns=['Tid','Traj_length','User'])\n",
    "        tids = []\n",
    "        user = ''\n",
    "        traj_length = 0\n",
    "\n",
    "        sequence = []\n",
    "        past_tid = None\n",
    "        curr_tid = None\n",
    "        num_of_seqs = 0\n",
    "        map_element_id = 0\n",
    "        unique_elements = {}\n",
    "        map_id_to_element = {}\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            curr_tid = df.loc[i,\"new_tid\"]\n",
    "            if curr_tid not in tids:\n",
    "                tids.append(int(curr_tid))\n",
    "                user = int(df.loc[i,\"label\"])\n",
    "                traj_length = len(df[df['new_tid'] == curr_tid])\n",
    "                # append rows to an empty DataFrame\n",
    "                df_map_traj_user = df_map_traj_user.append({'Tid' : curr_tid, 'Traj_length' : traj_length, 'User' : user},ignore_index = True)\n",
    "        df_map_traj_user['Tid'] = df_map_traj_user['Tid'].astype(int, errors='ignore')\n",
    "        df_map_traj_user['Traj_length'] = df_map_traj_user['Traj_length'].astype(int, errors='ignore')\n",
    "        df_map_traj_user['User'] = df_map_traj_user['User'].astype(int, errors='ignore')\n",
    "\n",
    "    #     print(df_map_traj_user.shape)\n",
    "    #     print(df_map_traj_user.head())\n",
    "    #     print('Todo DataFrame (traj_length):',' mean=',df_map_traj_user['Traj_length'].mean(),\n",
    "    #           ' std=',df_map_traj_user['Traj_length'].std())\n",
    "    #     u_185 = df_map_traj_user[df_map_traj_user['User']==185]\n",
    "    #     print(u_185)\n",
    "    #     print('User 185:',' mean=',u_185['Traj_length'].mean(),' std=',u_185['Traj_length'].std())\n",
    "    #     df_map_traj_user.groupby(['label']).nunique()['new_tid'].mean()\n",
    "        r = (df_map_traj_user.groupby(['User'])['Traj_length']\n",
    "             .agg([np.count_nonzero,np.mean,np.std])\n",
    "             .rename(columns={'count_nonzero':'Count_trajs',\n",
    "                              'mean':'AVG_traj_length',\n",
    "                              'std':'STD_traj_length'}))\n",
    "    #     print('Número médio de trajs por usuário = {:.2f} com DP = {:.2f}'.format(r['Count_trajs'].mean(),\n",
    "    #                                                                        r['Count_trajs'].std()))\n",
    "    #     print('Tamanho médio das trajs por usuário = {:.2f} com DP médio = {:.2f}'.format(r['AVG_traj_length'].mean(),\n",
    "    #                                                                        r['STD_traj_length'].mean()))\n",
    "        return df_map_traj_user\n",
    "    except:\n",
    "        raise('Please, check the input data format.')\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "my_fila = deque([{'hotel':4},{'casa':7},{'trabalho':9},{'padaria':2}])\n",
    "my_fila2 = deque()\n",
    "my_fila2.append({'hotel':4})\n",
    "my_fila2.append({'padaria':2})\n",
    "print('Fila 1: ',type(my_fila))\n",
    "print('Fila 1: ',my_fila)\n",
    "print('Fila 2: ',my_fila2)\n",
    "print(my_fila)\n",
    "my_fila.append({'festa':1})\n",
    "print(my_fila)\n",
    "my_fila.appendleft({'aeroporto':1})\n",
    "print(my_fila)\n",
    "print(my_fila[1])\n",
    "my_fila.insert(1,{'padaria':3})\n",
    "print(my_fila)\n",
    "print(len(my_fila))\n",
    "print(my_fila.pop())\n",
    "print(type(my_fila[2]))\n",
    "r = my_fila[4]\n",
    "print(r)\n",
    "print(list(r.keys())[0])\n",
    "print(my_fila.index(my_fila[4],2,len(my_fila)))\n",
    "print('Fila 1: ',my_fila)\n",
    "print('Fila 2: ',my_fila2)\n",
    "my_fila.pop()#delete from the right end\n",
    "my_fila.popleft()#delete from the left end\n",
    "print('Fila 1: ',my_fila)\n",
    "f = my_fila.popleft()\n",
    "print('Fila 1: ',my_fila)\n",
    "print('Element poped: ',f)\n",
    "my_fila.append(f)\n",
    "print(my_fila)\n",
    "# my_fila.popleft()\n",
    "# print(my_fila)\n",
    "# my_fila.popleft()\n",
    "# print(my_fila)\n",
    "# my_fila.popleft()\n",
    "# print(my_fila)\n",
    "# my_fila.popleft()\n",
    "# print(my_fila)\n",
    "# print(my_fila)\n",
    "# while my_fila:\n",
    "#     fx = my_fila.popleft()\n",
    "#     print(fx)\n",
    "#     print(my_fila)\n",
    "# for p in my_fila:\n",
    "#     print(p)\n",
    "print(num_elements_to_test('log2',len(my_fila)))\n",
    "print(num_elements_to_test('log10',len(my_fila)))\n",
    "print(num_elements_to_test('length',len(my_fila)))\n",
    "print(my_fila)\n",
    "my_fila.remove({'casa':7})\n",
    "print(my_fila)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_elements_to_test(option,number):\n",
    "    \n",
    "    if option == 'log2':\n",
    "        return int(round(np.log2(number)))\n",
    "    elif option == 'log10':\n",
    "        return int(round(np.log10(number)))\n",
    "    elif option == 'length':# Attention! The number for length must to be at most length of structure. e.g. array, dic.\n",
    "        return int(round(number))\n",
    "    else:\n",
    "        return print('Choose a valid option!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too noisy (line,col)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_too_noisy(count_presence, C, e_obj, e_att, att_data_dict, E, dimension):\n",
    "    num_of_atts = len(C[0])\n",
    "    num_of_objs = len(C[1])\n",
    "    if dimension == \"obj\":\n",
    "        # obj must be present in at least (1-e_obj).||C_a||\n",
    "        return count_presence >= ((1-e_obj) * num_of_atts) # return true if the obj is not too noisy\n",
    "    else:\n",
    "        # col must be present in at least (1-e_tt).||C_o||\n",
    "        return count_presence >= ((1-e_att) * num_of_objs) # return true if the att is not too noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(numOfObj, numOfAtt, cov=0, noise=0):\n",
    "    if VERBOSE:\n",
    "        print('Num. objs: {0:2d}, Num. att: {1:2d}, Num. covered: {2:2d}, Num. noise: {3:2d}'.format(numOfObj,numOfAtt,cov,noise))\n",
    "#     return ((numOfObj+numOfAtt) - (numOfObj*numOfAtt)) + cov + (2*noise)\n",
    "    return ((numOfObj+numOfAtt) - (numOfObj*numOfAtt)) + cov + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort attributes in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_freq = {'10':10,'45':45,'65':9,'87':2,'0':100}\n",
    "sorted_attributes = sort_attributes(test_dict_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_attributes(data_res):\n",
    "    \n",
    "    try:\n",
    "        ##usar este for caso o value seja uma lista\n",
    "        freq_res_dict = {}\n",
    "        for key,value in data_res.items():\n",
    "            freq_res_dict[key] = len(value)\n",
    "\n",
    "        # Create a list of tuples sorted by index 1 i.e. value field     \n",
    "        listofTuples = sorted(freq_res_dict.items() , reverse=True, key=lambda x: x[1])# usar se value for lista\n",
    "        # Iterate over the sorted sequence\n",
    "        # for elem in listofTuples :\n",
    "        #     print(elem[0] , \" ::\" , elem[1] )\n",
    "    #     print(listofTuples)\n",
    "        sorted_attributes = [elem[0] for elem in listofTuples]\n",
    "    except:\n",
    "        ## este é usado caso value seja um número\n",
    "        sorted_attributes = {k: v for k, v in sorted(data_res.items(), reverse=True, key=lambda item: item[1])}\n",
    "    \n",
    "#     if VERBOSE:\n",
    "#         print(\"Sorted att: \",sorted_attributes)\n",
    "    return sorted_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update residual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_residual_dataset(res_data, attributes_cocluster, objects_cocluster):\n",
    "    for key, value in res_data.items():\n",
    "        if key in attributes_cocluster:\n",
    "            diff_objs = set(res_data[key]).difference(set(objects_cocluster))\n",
    "            res_data[key] = list(diff_objs)\n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1h:0m:0s'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_time_output(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time_output(time_in_sec):\n",
    "    ''' \n",
    "    This function converts the seconds for the format Hours:Minutes:Seconds.\n",
    "    '''\n",
    "    hours = np.floor((time_in_sec/3600))\n",
    "    mins = np.floor((time_in_sec - (hours*3600))/60)\n",
    "    secs = np.floor(time_in_sec%60)\n",
    "#     print(str(int(hours))+'h:'+str(int(mins))+'m:'+str(int(secs))+'s')\n",
    "    \n",
    "    return str(int(hours))+'h:'+str(int(mins))+'m:'+str(int(secs))+'s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054.01"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_time_minutes(123240.456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts the seconds for the format in minutes\n",
    "def format_time_minutes(time_in_sec):\n",
    "    return np.round(time_in_sec/60,2)\n",
    "#     return time_in_sec/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results - check path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path_method):\n",
    "    current_dir = os.getcwd()\n",
    "    print(current_dir)\n",
    "    res = os.path.exists(path_method)\n",
    "    # clean the folder to save new data\n",
    "    if res:\n",
    "        #check if it is empty\n",
    "        dir_empty = os.listdir(path_method)\n",
    "        if len(dir_empty) != 0:\n",
    "    #         shutil.rmtree(\"OutputAnalysis/kmeans/\")\n",
    "            rm = !rm -r --preserve-root './OutputAnalysis/ococlus/'*\n",
    "            if not rm:\n",
    "                print(\"OCoClus' folder was cleaned.\")\n",
    "    #             os.chdir(path_method)\n",
    "            else:\n",
    "                print(\"sad\")\n",
    "                print(rm)\n",
    "        else:\n",
    "    #         print(\"Empty!\")\n",
    "            pass\n",
    "    #         os.chdir(path_method)\n",
    "    else: # nothing exist so create it\n",
    "        # trying to insert to flase directory \n",
    "        try: \n",
    "    #         os.chdir(fd) \n",
    "            os.mkdir(path_method)\n",
    "            print(\"The path was created: \"+path_method)\n",
    "\n",
    "        # Caching the exception     \n",
    "        except: \n",
    "            print(\"Something wrong with specified directory. Exception- \", sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save clustering result into a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def writeFileOutput(cols, rows, dataset, method='OCoClus', fileName='OCoClusResult'):\n",
    "def writeFileOutput(co_clusters, dataset, method='OCoClus', fileName='OCoClusResult'):\n",
    "    text = \"\"\n",
    "#    for c in range(len(data.rows_)):\n",
    "#        res = [i for i, val in enumerate(data.columns_[c]) if val]\n",
    "#        for j in res:\n",
    "#            text += str(j)+\" \"\n",
    "\n",
    "#        res = [i for i, val in enumerate(data.rows_[c]) if val]\n",
    "#        text += \"[\"\n",
    "#        for j in res:\n",
    "#            text += str(j)+\" \"\n",
    "#        text += \"]\\n\"\n",
    "    \n",
    "    num_of_clusters = len(co_clusters)\n",
    "    \n",
    "#     for c in range(len(cols)):\n",
    "    for c in range(num_of_clusters):\n",
    "#         for i in cols[c]:\n",
    "        for i in co_clusters[c][0]: # get the attributes in cluster c\n",
    "            text += str(i)+\" \"\n",
    "        \n",
    "        text += \"(\"+str(len(co_clusters[c][1]))+\") [\" # get the number of objects in clusters c\n",
    "        for j in range(len(co_clusters[c][1])): # save in the file each obj\n",
    "            if j+1 != len(co_clusters[c][1]):\n",
    "                text += str(co_clusters[c][1][j])+\" \"\n",
    "            else:\n",
    "                text += str(co_clusters[c][1][j])\n",
    "        text += \"]\\n\"\n",
    "    \n",
    "    #print(text)\n",
    "    if method == 'Dhillon':\n",
    "        f = open('./datasets/outputs/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./datasets/outputs/\"+fileName+\".txt\")\n",
    "    elif method == 'Kluger':\n",
    "        f = open('./datasets/outputs/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./datasets/outputs/\"+fileName+\".txt\")\n",
    "    elif method == 'OCoClus':\n",
    "        f = open('./OutputAnalysis/ococlus/'+dataset+'/'+fileName+'.txt', 'w+')#saving at dataset folder\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        print(\"Output file saved in: \"+\"./OutputAnalysis/ococlus/\"+dataset+\"/\"+fileName+\".txt\")\n",
    "    else:\n",
    "        print(\"The output file was not generated. Method option not recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rec_error(data,clusters):\n",
    "    '''\n",
    "    This evaluation measure is computed during the algorithm life time.\n",
    "    '''\n",
    "    reconstructed_ococlus = np.zeros(data.shape,dtype=int)\n",
    "    for nc in range(len(clusters)):\n",
    "        for i in clusters[nc][1]: # object cluster\n",
    "            for j in clusters[nc][0]: # attribute cluster\n",
    "                reconstructed_ococlus[int(i)][int(j)] = 1\n",
    "    print(\"Reconstruction error: \",np.sum(np.bitwise_xor(data,reconstructed_ococlus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omega format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clustering_output_omega(co_clusters):\n",
    "# def build_clustering_output_omega(rowClusters,columnClusters):\n",
    "    '''\n",
    "    Build the clustering output format to use in the omega index evaluation from Remy Cazabet version.\n",
    "    It is optional and we just present this version as a complementary information. If you are interested,\n",
    "    check it out on his team work group at https://github.com/isaranto/omega_index.\n",
    "    '''\n",
    "    \n",
    "    num_of_clusters = len(co_clusters)    \n",
    "    clustering = {}\n",
    "    \n",
    "    for nc in range(num_of_clusters):\n",
    "        rowCluster = co_clusters[nc][1]\n",
    "        columnCluster = co_clusters[nc][0]\n",
    "        clustering[\"c\"+str(nc)] = []\n",
    "        \n",
    "        for i in rowCluster:\n",
    "            for j in columnCluster:\n",
    "                clustering[\"c\"+str(nc)].append((\"01\"+str(i)+\"02\"+str(j)))\n",
    "        \n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eXascale Infolab \n",
    "We used the xmeasure and OvpNMI project that pushished evaluation measures for overlapping task. We can check it on https://github.com/eXascaleInfolab/xmeasures or https://exascale.info/. Look their project on github to know how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmeasures_format(dict_gt):\n",
    "    '''\n",
    "    This function build the xmeasure format to use it on their evaluation measure.\n",
    "    '''\n",
    "    newData = []\n",
    "    for i in range(len(dict_gt)):\n",
    "#         print(dict_gt['c'+str(i)])\n",
    "        stringLine = dict_gt['c'+str(i)][0]\n",
    "        for j in range(1,len(dict_gt['c'+str(i)])):\n",
    "#             stringLine = stringLine+\" \"+dict_gt['c'+str(i)][j]\n",
    "            stringLine += \" \"+dict_gt['c'+str(i)][j]\n",
    "        newData.append(stringLine)\n",
    "    \n",
    "    return newData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
